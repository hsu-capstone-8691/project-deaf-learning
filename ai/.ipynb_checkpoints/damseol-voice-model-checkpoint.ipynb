{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabf2SLImPAG"
      },
      "source": [
        "### **1. 환경 설정 및 함수 정의**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUlInrZ4eH7y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchcrepe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from glob import glob\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Or54w3upKxs"
      },
      "outputs": [],
      "source": [
        "# 데이터 경로 설정\n",
        "pcm_files = sorted(glob(\"E:/KsponSpeech/original/KsponSpeech_01/KsponSpeech_0001/*.pcm\"))\n",
        "txt_files = sorted(glob(\"E:/KsponSpeech/original/KsponSpeech_01/KsponSpeech_0001/*.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Leks_hVIeUWl"
      },
      "outputs": [],
      "source": [
        "# PCM 파일 로드 함수\n",
        "def load_pcm(file_path, sr=16000):\n",
        "    #raw_audio = np.fromfile(file_path, dtype=np.int16).astype(np.float32)  # PCM int16 > float32 변환\n",
        "    raw_audio = torch.from_numpy(np.fromfile(file_path, dtype=np.int16).astype(np.float32))\n",
        "    audio = raw_audio / torch.iinfo(torch.int16).max  # 정규화 (-1 ~ 1)\n",
        "\n",
        "    # 샘플링 레이트 16000으로 고정\n",
        "    orig_sr = 16000  # PCM 파일의 기본 SR이 16000이라고 가정\n",
        "    if orig_sr != sr:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=sr)\n",
        "        audio = resampler(audio)\n",
        "\n",
        "    return audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlE-w2Msd1qL"
      },
      "outputs": [],
      "source": [
        "# 텍스트 전처리\n",
        "import re\n",
        "\n",
        "def clean_transcript(text):\n",
        "    #  이중 전사 중 발음 스크립트 선택 (\"(컴퓨터)/(컴퓨타)\" → \"컴퓨터\")\n",
        "    text = re.sub(r'\\(([^)]+)\\)/\\(([^)]+)\\)', r'(\\2)', text)\n",
        "\n",
        "    # 식별 불가 스크립트 제거 (\"unk/나는\" → \"나는\")\n",
        "    text = re.sub(r'unk/\\S+', '', text)\n",
        "\n",
        "    # 노이즈 스크립트 제거 (\"n/ o/ b/ u/ l/\" 제거)\n",
        "    text = re.sub(r'[nobul]/', '', text)\n",
        "\n",
        "    # 특수문자 제거\n",
        "    text = re.sub(r'[.,/!?;:\\-()*+]', '', text)\n",
        "    text = re.sub(r'\\S+/', '', text)\n",
        "\n",
        "    # 문장의 처음에 공백이 있거나 공백이 두 개 이상 이어져 있는 경우 공백을 하나로 정리\n",
        "    text = re.sub(r'^\\s+', '', text)  # 문장 처음의 공백 제거\n",
        "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백을 하나로 축소\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R1fWnhgetlK"
      },
      "outputs": [],
      "source": [
        "def extract_features(audio, sr=16000):\n",
        "\n",
        "    # MFCC 추출\n",
        "    mfcc_transform = torchaudio.transforms.MFCC(\n",
        "        sample_rate=sr,\n",
        "        n_mfcc=13,\n",
        "        melkwargs={\n",
        "            \"n_fft\": 400,\n",
        "            \"hop_length\": 160,\n",
        "            \"n_mels\": 80\n",
        "        }\n",
        "    )\n",
        "    mfcc = mfcc_transform(audio)\n",
        "\n",
        "    # MFCC 정규화\n",
        "    mfcc = normalize_mfcc(mfcc)\n",
        "\n",
        "    # 스펙트로그램 변환\n",
        "    spectorgram_transform = torchaudio.transforms.Spectrogram(\n",
        "        n_fft=400,\n",
        "        hop_length=160,\n",
        "    )\n",
        "    spectrogram = spectorgram_transform(audio)\n",
        "    spectrogram_db = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n",
        "\n",
        "    # Pitch (음높이) 및 Energy (에너지)\n",
        "    #pitch_transform = torchaudio.transforms.PitchShift(sr, n_steps=2)\n",
        "    #pitch_values = pitch_transform(audio)\n",
        "    pitch_values = 0  # 나중에 변경\n",
        "\n",
        "    energy = torch.sqrt(torch.mean(audio**2, dim=-1))\n",
        "\n",
        "    return mfcc, spectrogram_db, pitch_values, energy\n",
        "\n",
        "def normalize_mfcc(mfcc):\n",
        "  mean = torch.mean(mfcc, dim=1, keepdim=True)\n",
        "  std = torch.std(mfcc, dim=1, keepdim=True)\n",
        "  return (mfcc - mean) / (std + 1e-8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5hYaLYqpYAk"
      },
      "outputs": [],
      "source": [
        "# 데이터 로드 예시\n",
        "\n",
        "audio_data = [load_pcm(file) for file in pcm_files]\n",
        "texts = [open(txt, \"r\", encoding=\"cp949\").read().strip() for txt in txt_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPoaRcBzrNxp"
      },
      "outputs": [],
      "source": [
        "# 텍스트 전처리 출력 테스트\n",
        "for text in texts:\n",
        "  print(text)\n",
        "  print(clean_transcript(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elqE68LpcBOb",
        "outputId": "377cff22-2b6f-4269-8e11-2a08fa09d848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text:  아/ 몬 소리야, 그건 또. b/\n",
            "MFCC Raw: tensor([[-1.3372e+00, -1.2596e+00, -1.2233e+00,  ..., -1.0976e+00,\n",
            "         -1.0822e+00, -1.2013e+00],\n",
            "        [-1.3993e+00, -8.7625e-01, -8.3103e-01,  ..., -4.9156e-01,\n",
            "         -5.2939e-01, -4.6778e-01],\n",
            "        [ 5.7234e-01,  9.0763e-01,  5.2310e-01,  ...,  5.3884e-01,\n",
            "          7.1024e-01,  1.2474e+00],\n",
            "        ...,\n",
            "        [ 1.5656e+00, -4.1436e-01, -2.1543e-01,  ...,  8.1051e-01,\n",
            "          1.9598e-01,  5.7674e-04],\n",
            "        [ 9.5650e-02, -1.0997e+00, -2.7613e-01,  ...,  4.5922e-01,\n",
            "          1.1356e+00,  1.1126e+00],\n",
            "        [ 1.3364e+00,  1.5560e+00,  1.7766e+00,  ...,  1.4341e+00,\n",
            "          1.4761e+00,  2.3825e+00]])\n",
            "MFCC Shape: torch.Size([13, 315])\n",
            "Spectrogram Shape: torch.Size([201, 315])\n",
            "Pitch Mean: 0\n",
            "Energy: tensor(0.0379)\n"
          ]
        }
      ],
      "source": [
        "# 예제 데이터로 특징 추출\n",
        "mfcc, spectrogram_db, pitch_mean, energy = extract_features(audio_data[0])\n",
        "print(\"Text: \", texts[0])\n",
        "print(\"MFCC Raw:\", mfcc)\n",
        "print(\"MFCC Shape:\", mfcc.shape)\n",
        "print(\"Spectrogram Shape:\", spectrogram_db.shape)\n",
        "print(\"Pitch Mean:\", pitch_mean)\n",
        "print(\"Energy:\", energy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvvTVqMWfDNF"
      },
      "source": [
        "### **2. 딥러닝 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KGt5ZE-fSl8"
      },
      "source": [
        "#### **2.1 데이터셋 클래스 정의**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSINL6FafAul",
        "outputId": "ba6daeb1-37db-437f-8fec-c46a0298a12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization Dictionary:\n",
            "{'<PAD>': -1, '<BLANK>': 0, '\\n': 1, ' ': 2, '!': 3, '%': 4, '(': 5, ')': 6, '*': 7, '+': 8, ',': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'F': 27, 'G': 28, 'L': 29, 'M': 30, 'N': 31, 'O': 32, 'P': 33, 'R': 34, 'S': 35, 'T': 36, 'V': 37, 'X': 38, 'b': 39, 'c': 40, 'g': 41, 'k': 42, 'l': 43, 'n': 44, 'o': 45, 'u': 46, '가': 47, '각': 48, '간': 49, '갈': 50, '감': 51, '갑': 52, '값': 53, '갔': 54, '강': 55, '갖': 56, '같': 57, '갚': 58, '개': 59, '객': 60, '걍': 61, '걔': 62, '걘': 63, '거': 64, '걱': 65, '건': 66, '걷': 67, '걸': 68, '검': 69, '것': 70, '겄': 71, '게': 72, '겠': 73, '겨': 74, '격': 75, '겪': 76, '견': 77, '결': 78, '겸': 79, '겹': 80, '겼': 81, '경': 82, '계': 83, '고': 84, '곡': 85, '곤': 86, '골': 87, '곰': 88, '곱': 89, '곳': 90, '공': 91, '과': 92, '곽': 93, '관': 94, '광': 95, '괜': 96, '괴': 97, '굉': 98, '교': 99, '구': 100, '국': 101, '군': 102, '굳': 103, '굴': 104, '굿': 105, '궁': 106, '궈': 107, '권': 108, '귀': 109, '귓': 110, '규': 111, '균': 112, '귤': 113, '그': 114, '극': 115, '근': 116, '글': 117, '긁': 118, '금': 119, '급': 120, '긋': 121, '긍': 122, '기': 123, '긴': 124, '길': 125, '김': 126, '깃': 127, '깊': 128, '까': 129, '깎': 130, '깐': 131, '깔': 132, '깜': 133, '깝': 134, '깡': 135, '깨': 136, '꺼': 137, '껀': 138, '껌': 139, '께': 140, '껴': 141, '꼈': 142, '꼬': 143, '꼭': 144, '꼰': 145, '꼽': 146, '꽃': 147, '꽤': 148, '꾸': 149, '꾹': 150, '꿀': 151, '꿈': 152, '꿋': 153, '꿔': 154, '뀌': 155, '끄': 156, '끈': 157, '끊': 158, '끌': 159, '끔': 160, '끝': 161, '끼': 162, '낄': 163, '낌': 164, '나': 165, '낙': 166, '난': 167, '날': 168, '남': 169, '납': 170, '낫': 171, '났': 172, '낮': 173, '낯': 174, '낳': 175, '내': 176, '낸': 177, '낼': 178, '냄': 179, '냅': 180, '냈': 181, '냉': 182, '냐': 183, '냑': 184, '냥': 185, '너': 186, '넌': 187, '널': 188, '넓': 189, '넘': 190, '넜': 191, '넣': 192, '네': 193, '넷': 194, '녀': 195, '녁': 196, '년': 197, '념': 198, '녔': 199, '노': 200, '녹': 201, '논': 202, '놀': 203, '놈': 204, '농': 205, '높': 206, '놓': 207, '놔': 208, '놨': 209, '뇨': 210, '누': 211, '눈': 212, '눌': 213, '눠': 214, '뉴': 215, '느': 216, '는': 217, '늘': 218, '능': 219, '늦': 220, '늬': 221, '니': 222, '닌': 223, '닐': 224, '님': 225, '다': 226, '닥': 227, '단': 228, '닫': 229, '달': 230, '닭': 231, '담': 232, '답': 233, '당': 234, '대': 235, '댄': 236, '댈': 237, '댐': 238, '댓': 239, '댔': 240, '더': 241, '덕': 242, '던': 243, '덜': 244, '덟': 245, '덤': 246, '덥': 247, '덩': 248, '덮': 249, '데': 250, '덴': 251, '도': 252, '독': 253, '돈': 254, '돌': 255, '돔': 256, '돗': 257, '동': 258, '돼': 259, '됐': 260, '되': 261, '된': 262, '될': 263, '됬': 264, '두': 265, '둔': 266, '둘': 267, '둬': 268, '뒤': 269, '뒷': 270, '듀': 271, '드': 272, '득': 273, '든': 274, '듣': 275, '들': 276, '듯': 277, '등': 278, '디': 279, '딘': 280, '딜': 281, '딥': 282, '딨': 283, '딩': 284, '따': 285, '딱': 286, '딲': 287, '딴': 288, '딸': 289, '땀': 290, '땅': 291, '때': 292, '땐': 293, '땜': 294, '땠': 295, '땡': 296, '떄': 297, '떠': 298, '떡': 299, '떤': 300, '떨': 301, '떰': 302, '떴': 303, '떻': 304, '떼': 305, '또': 306, '똑': 307, '똘': 308, '똥': 309, '뚝': 310, '뚫': 311, '뚱': 312, '뛰': 313, '뛴': 314, '뛸': 315, '뜨': 316, '뜰': 317, '뜻': 318, '띠': 319, '띡': 320, '라': 321, '락': 322, '란': 323, '랄': 324, '람': 325, '랐': 326, '랑': 327, '랗': 328, '래': 329, '랜': 330, '램': 331, '랩': 332, '랬': 333, '량': 334, '러': 335, '럭': 336, '런': 337, '럴': 338, '럼': 339, '럽': 340, '렀': 341, '렁': 342, '렇': 343, '레': 344, '렌': 345, '려': 346, '력': 347, '련': 348, '렴': 349, '렵': 350, '렸': 351, '례': 352, '로': 353, '록': 354, '론': 355, '롤': 356, '롭': 357, '롯': 358, '롱': 359, '료': 360, '루': 361, '룸': 362, '룹': 363, '룻': 364, '류': 365, '륙': 366, '률': 367, '르': 368, '른': 369, '를': 370, '름': 371, '릇': 372, '릉': 373, '릏': 374, '리': 375, '릭': 376, '린': 377, '릴': 378, '림': 379, '립': 380, '링': 381, '마': 382, '막': 383, '만': 384, '많': 385, '말': 386, '맘': 387, '맛': 388, '망': 389, '맞': 390, '맡': 391, '매': 392, '맥': 393, '맨': 394, '맹': 395, '맺': 396, '머': 397, '먹': 398, '먼': 399, '멀': 400, '멈': 401, '멋': 402, '멍': 403, '메': 404, '멘': 405, '멜': 406, '멤': 407, '며': 408, '면': 409, '명': 410, '몇': 411, '모': 412, '목': 413, '몬': 414, '몰': 415, '몸': 416, '못': 417, '몽': 418, '무': 419, '묵': 420, '문': 421, '물': 422, '뭉': 423, '뭐': 424, '뭔': 425, '뭘': 426, '뮤': 427, '므': 428, '믄': 429, '미': 430, '민': 431, '밀': 432, '밌': 433, '밍': 434, '밑': 435, '바': 436, '박': 437, '밖': 438, '반': 439, '받': 440, '발': 441, '밝': 442, '밤': 443, '밥': 444, '방': 445, '배': 446, '백': 447, '밲': 448, '밸': 449, '버': 450, '벅': 451, '번': 452, '벌': 453, '범': 454, '법': 455, '벗': 456, '베': 457, '벤': 458, '벼': 459, '벽': 460, '변': 461, '별': 462, '병': 463, '보': 464, '복': 465, '볶': 466, '본': 467, '볼': 468, '봄': 469, '봅': 470, '봉': 471, '봐': 472, '봤': 473, '부': 474, '북': 475, '분': 476, '불': 477, '붐': 478, '붓': 479, '붙': 480, '뷔': 481, '브': 482, '블': 483, '비': 484, '빈': 485, '빌': 486, '빔': 487, '빙': 488, '빠': 489, '빡': 490, '빨': 491, '빴': 492, '빵': 493, '빼': 494, '뺄': 495, '뺑': 496, '뻐': 497, '뻔': 498, '뻘': 499, '뻥': 500, '뽑': 501, '뽕': 502, '뿌': 503, '쁘': 504, '쁜': 505, '삘': 506, '사': 507, '삭': 508, '산': 509, '살': 510, '삶': 511, '삼': 512, '샀': 513, '상': 514, '새': 515, '색': 516, '샌': 517, '생': 518, '샤': 519, '샾': 520, '서': 521, '석': 522, '섞': 523, '선': 524, '설': 525, '섬': 526, '섭': 527, '섯': 528, '성': 529, '세': 530, '섹': 531, '센': 532, '셀': 533, '셈': 534, '셉': 535, '셋': 536, '셔': 537, '션': 538, '셜': 539, '셤': 540, '셨': 541, '소': 542, '속': 543, '손': 544, '솔': 545, '솜': 546, '솝': 547, '송': 548, '쇼': 549, '수': 550, '숙': 551, '순': 552, '술': 553, '숨': 554, '숭': 555, '숱': 556, '쉐': 557, '쉬': 558, '쉰': 559, '쉴': 560, '쉽': 561, '슐': 562, '스': 563, '슥': 564, '슨': 565, '슬': 566, '습': 567, '슷': 568, '승': 569, '시': 570, '식': 571, '신': 572, '싣': 573, '실': 574, '싫': 575, '심': 576, '십': 577, '싱': 578, '싶': 579, '싸': 580, '싹': 581, '싼': 582, '쌀': 583, '쌈': 584, '쌌': 585, '쌓': 586, '쌔': 587, '쌤': 588, '써': 589, '썬': 590, '썰': 591, '썼': 592, '쎄': 593, '쎌': 594, '쏘': 595, '쐈': 596, '쓰': 597, '쓴': 598, '쓸': 599, '씀': 600, '씨': 601, '씩': 602, '씬': 603, '씻': 604, '아': 605, '악': 606, '안': 607, '앉': 608, '않': 609, '알': 610, '암': 611, '압': 612, '앗': 613, '았': 614, '앙': 615, '앚': 616, '앞': 617, '애': 618, '액': 619, '앤': 620, '앨': 621, '앱': 622, '야': 623, '약': 624, '얀': 625, '얇': 626, '얌': 627, '양': 628, '얘': 629, '어': 630, '억': 631, '언': 632, '얹': 633, '얻': 634, '얼': 635, '엄': 636, '업': 637, '없': 638, '엇': 639, '었': 640, '엉': 641, '엊': 642, '엎': 643, '에': 644, '엑': 645, '엔': 646, '엠': 647, '엥': 648, '여': 649, '역': 650, '연': 651, '열': 652, '엽': 653, '였': 654, '영': 655, '옆': 656, '예': 657, '옛': 658, '오': 659, '옥': 660, '온': 661, '올': 662, '옮': 663, '옷': 664, '옹': 665, '와': 666, '완': 667, '왈': 668, '왓': 669, '왔': 670, '왕': 671, '왜': 672, '왠': 673, '왤': 674, '외': 675, '왼': 676, '요': 677, '욕': 678, '욜': 679, '용': 680, '우': 681, '욱': 682, '운': 683, '울': 684, '움': 685, '웃': 686, '웅': 687, '워': 688, '원': 689, '월': 690, '웜': 691, '웠': 692, '웨': 693, '웬': 694, '위': 695, '윗': 696, '윙': 697, '유': 698, '육': 699, '윤': 700, '윷': 701, '으': 702, '은': 703, '을': 704, '음': 705, '응': 706, '의': 707, '이': 708, '익': 709, '인': 710, '일': 711, '읽': 712, '임': 713, '입': 714, '있': 715, '잉': 716, '자': 717, '작': 718, '잔': 719, '잖': 720, '잘': 721, '잠': 722, '잡': 723, '잤': 724, '장': 725, '재': 726, '잰': 727, '쟁': 728, '쟤': 729, '저': 730, '적': 731, '전': 732, '절': 733, '젊': 734, '점': 735, '접': 736, '젓': 737, '정': 738, '제': 739, '젠': 740, '젤': 741, '젬': 742, '져': 743, '졌': 744, '조': 745, '족': 746, '존': 747, '졸': 748, '좀': 749, '종': 750, '좋': 751, '죄': 752, '죠': 753, '주': 754, '죽': 755, '준': 756, '줄': 757, '줏': 758, '중': 759, '줘': 760, '줬': 761, '쥐': 762, '즈': 763, '즌': 764, '즐': 765, '즘': 766, '증': 767, '지': 768, '직': 769, '진': 770, '질': 771, '짐': 772, '집': 773, '짓': 774, '짜': 775, '짝': 776, '짠': 777, '짧': 778, '짬': 779, '째': 780, '쨌': 781, '쩌': 782, '쩍': 783, '쩔': 784, '쩰': 785, '쪼': 786, '쪽': 787, '쫄': 788, '쫌': 789, '쭉': 790, '쭐': 791, '쭤': 792, '쯔': 793, '쯤': 794, '찌': 795, '찍': 796, '찜': 797, '차': 798, '착': 799, '찬': 800, '찮': 801, '찰': 802, '참': 803, '찻': 804, '찼': 805, '창': 806, '찾': 807, '채': 808, '책': 809, '챙': 810, '처': 811, '척': 812, '천': 813, '철': 814, '첨': 815, '첫': 816, '청': 817, '체': 818, '첸': 819, '쳐': 820, '쳤': 821, '초': 822, '촉': 823, '촌': 824, '총': 825, '촬': 826, '최': 827, '추': 828, '축': 829, '춘': 830, '출': 831, '춤': 832, '춥': 833, '충': 834, '춰': 835, '췄': 836, '췌': 837, '취': 838, '츠': 839, '측': 840, '층': 841, '치': 842, '친': 843, '칠': 844, '침': 845, '칫': 846, '칭': 847, '카': 848, '칵': 849, '캐': 850, '캔': 851, '캠': 852, '캡': 853, '커': 854, '컨': 855, '컬': 856, '컴': 857, '컵': 858, '컷': 859, '컸': 860, '케': 861, '켓': 862, '켜': 863, '코': 864, '콕': 865, '콘': 866, '콜': 867, '콩': 868, '쿄': 869, '쿠': 870, '쿡': 871, '쿨': 872, '쿼': 873, '퀴': 874, '퀸': 875, '크': 876, '큰': 877, '클': 878, '큼': 879, '키': 880, '킨': 881, '킬': 882, '킹': 883, '타': 884, '탁': 885, '탄': 886, '탈': 887, '탑': 888, '탓': 889, '탔': 890, '탕': 891, '태': 892, '택': 893, '탱': 894, '터': 895, '턱': 896, '턴': 897, '털': 898, '텀': 899, '텅': 900, '테': 901, '텍': 902, '텐': 903, '텔': 904, '템': 905, '텨': 906, '토': 907, '톡': 908, '톤': 909, '톱': 910, '통': 911, '퇴': 912, '투': 913, '툭': 914, '툴': 915, '튀': 916, '튜': 917, '트': 918, '특': 919, '튼': 920, '틀': 921, '티': 922, '틱': 923, '틸': 924, '팀': 925, '팅': 926, '파': 927, '팍': 928, '판': 929, '팔': 930, '팝': 931, '팟': 932, '팡': 933, '패': 934, '팩': 935, '팬': 936, '퍼': 937, '펀': 938, '페': 939, '펜': 940, '편': 941, '평': 942, '폐': 943, '포': 944, '폭': 945, '폰': 946, '폴': 947, '표': 948, '푠': 949, '푸': 950, '푼': 951, '풀': 952, '품': 953, '풋': 954, '풍': 955, '퓨': 956, '프': 957, '픈': 958, '플': 959, '피': 960, '필': 961, '핍': 962, '핑': 963, '하': 964, '학': 965, '한': 966, '할': 967, '핥': 968, '함': 969, '합': 970, '핫': 971, '항': 972, '해': 973, '핵': 974, '핸': 975, '했': 976, '행': 977, '햐': 978, '향': 979, '허': 980, '헌': 981, '헐': 982, '험': 983, '헛': 984, '헤': 985, '헬': 986, '헷': 987, '혀': 988, '혁': 989, '현': 990, '혈': 991, '혐': 992, '혔': 993, '형': 994, '혜': 995, '호': 996, '혹': 997, '혼': 998, '홀': 999, '홈': 1000, '홉': 1001, '홍': 1002, '화': 1003, '확': 1004, '환': 1005, '활': 1006, '황': 1007, '회': 1008, '획': 1009, '횟': 1010, '효': 1011, '후': 1012, '훈': 1013, '훙': 1014, '훨': 1015, '휴': 1016, '흐': 1017, '흠': 1018, '흥': 1019, '희': 1020, '히': 1021, '힌': 1022, '힐': 1023, '힘': 1024, '힙': 1025}\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, pcm_files, txt_files, sr=16000):\n",
        "        self.pcm_files = pcm_files\n",
        "        self.txt_files = txt_files\n",
        "        self.sr = sr\n",
        "        self.tokenizer = self.create_tokenizer() # 토크나이저 추가\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pcm_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            audio = load_pcm(self.pcm_files[idx], self.sr)\n",
        "            text = open(self.txt_files[idx], \"r\", encoding=\"cp949\").read().strip()\n",
        "            text = clean_transcript(text)\n",
        "            mfcc, spec, pitch, energy = extract_features(audio, self.sr)\n",
        "            tokenized_text = self.tokenizer.encode(text) # 텍스트 토큰화\n",
        "            return mfcc, torch.tensor(tokenized_text, dtype=torch.long) # 토큰화된 텍스트 반환\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {self.pcm_files[idx]}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_tokenizer(self): # 간단한 토크나이저 생성\n",
        "        chars = set()\n",
        "        for txt_file in self.txt_files:\n",
        "            with open(txt_file, \"r\", encoding=\"cp949\") as f:\n",
        "                text = f.read()\n",
        "                for char in text:\n",
        "                    chars.add(char)\n",
        "        char_to_int = {char: i + 1 for i, char in enumerate(sorted(list(chars)))} # 0은 padding에 사용\n",
        "        int_to_char = {i + 1: char for i, char in enumerate(sorted(list(chars)))}\n",
        "        return SimpleTokenizer(char_to_int, int_to_char)\n",
        "\n",
        "class SimpleTokenizer: # 심플 토크나이저 클래스\n",
        "    def __init__(self, char_to_int, int_to_char):\n",
        "      self.char_to_int = {'<PAD>': -1, '<BLANK>': 0, **char_to_int}\n",
        "      self.int_to_char = {-1: '<PAD>', 0: '<BLANK>', **int_to_char}\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.char_to_int.get(char, 0) for char in text]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \"\".join([self.int_to_char[token] for token in tokens if token != 0])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if not batch:\n",
        "        return torch.empty(0), torch.empty(0), torch.empty(0)\n",
        "    mfccs = [item[0] for item in batch]\n",
        "    texts = [item[1] for item in batch]\n",
        "\n",
        "    max_len = max([mfcc.shape[1] for mfcc in mfccs])\n",
        "    mfccs_padded = [torch.nn.functional.pad(mfcc, (0, max_len - mfcc.shape[1]), value=0) for mfcc in mfccs]\n",
        "    mfccs_padded = torch.stack(mfccs_padded, dim=0)\n",
        "\n",
        "    text_lengths = torch.tensor([len(t) for t in texts if len(t) > 0], dtype=torch.long) # text 길이 tensor 추가\n",
        "    texts_padded = pad_sequence([t for t in texts if len(t) > 0], batch_first=True, padding_value=-1) # text padding 추가\n",
        "\n",
        "    return mfccs_padded, texts_padded, text_lengths # text 길이 반환\n",
        "\n",
        "# 데이터셋 생성\n",
        "dataset = SpeechDataset(pcm_files, txt_files)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "# 토큰화 사전 출력\n",
        "print(\"Tokenization Dictionary:\")\n",
        "print(dataset.tokenizer.char_to_int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZEoDy5ffWUz"
      },
      "source": [
        "#### **2.2 모델 구축 및 학습**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z8bh3Aq748Y"
      },
      "outputs": [],
      "source": [
        "# Set Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HTVvmzMfRNO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, input_dim=13, hidden_dim=256, output_dim=100):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(2, 0, 1)  # (batch, feature, time) > (time, batch, feature)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = torch.nn.functional.log_softmax(x, dim=2)\n",
        "\n",
        "        if torch.isnan(x).any():\n",
        "            print(\"Warning: NaN detected in model forward!\")\n",
        "        elif torch.isinf(x).any():\n",
        "            print(\"Warning: Inf detected in model forward!\")\n",
        "\n",
        "        return x\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for layer in self.children():\n",
        "        if hasattr(layer, 'reset_parameters'):\n",
        "            layer.reset_parameters()\n",
        "\n",
        "\n",
        "# 모델 생성\n",
        "model = SpeechRecognitionModel().to(device)\n",
        "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=1e-5\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-6, eps=1e-4)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "# scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-5, steps_per_epoch=len(dataloader), epochs=10)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "28ceRwjnfghy",
        "outputId": "7254e5ac-7798-4066-e208-5bb6146666f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [1/250], Loss: 202.7964\n",
            "Epoch [1/10], Step [2/250], Loss: 328.9559\n",
            "Epoch [1/10], Step [3/250], Loss: 272.9131\n",
            "Epoch [1/10], Step [4/250], Loss: 125.5943\n",
            "Epoch [1/10], Step [5/250], Loss: 136.9655\n",
            "Epoch [1/10], Step [6/250], Loss: 174.7416\n",
            "Epoch [1/10], Step [7/250], Loss: 682.9363\n",
            "Epoch [1/10], Step [8/250], Loss: 280.3847\n",
            "Epoch [1/10], Step [9/250], Loss: 762.5361\n",
            "Epoch [1/10], Step [10/250], Loss: 319.1257\n",
            "Epoch [1/10], Step [11/250], Loss: 1344.4042\n",
            "Epoch [1/10], Step [12/250], Loss: 198.5763\n",
            "Epoch [1/10], Step [13/250], Loss: 1000.0983\n",
            "Epoch [1/10], Step [14/250], Loss: 293.5516\n",
            "Epoch [1/10], Step [15/250], Loss: 229.5682\n",
            "Epoch [1/10], Step [16/250], Loss: 411.0673\n",
            "Epoch [1/10], Step [17/250], Loss: 161.9754\n",
            "Epoch [1/10], Step [18/250], Loss: 58.0655\n",
            "Epoch [1/10], Step [19/250], Loss: 15.3333\n",
            "Epoch [1/10], Step [20/250], Loss: 10.8804\n",
            "Epoch [1/10], Step [21/250], Loss: 8.9048\n",
            "Epoch [1/10], Step [22/250], Loss: 4.8591\n",
            "Epoch [1/10], Step [23/250], Loss: 4.8716\n",
            "Epoch [1/10], Step [24/250], Loss: 4.5061\n",
            "Epoch [1/10], Step [25/250], Loss: 5.3502\n",
            "Epoch [1/10], Step [26/250], Loss: 5.4878\n",
            "Epoch [1/10], Step [27/250], Loss: 4.7905\n",
            "Epoch [1/10], Step [28/250], Loss: 5.0314\n",
            "Epoch [1/10], Step [29/250], Loss: 5.5115\n",
            "Epoch [1/10], Step [30/250], Loss: 4.7297\n",
            "Epoch [1/10], Step [31/250], Loss: 5.2669\n",
            "Epoch [1/10], Step [32/250], Loss: 4.6281\n",
            "Epoch [1/10], Step [33/250], Loss: 4.5245\n",
            "Epoch [1/10], Step [34/250], Loss: 4.3948\n",
            "Epoch [1/10], Step [35/250], Loss: 4.1858\n",
            "Epoch [1/10], Step [36/250], Loss: 4.3308\n",
            "Epoch [1/10], Step [37/250], Loss: 5.3201\n",
            "Epoch [1/10], Step [38/250], Loss: 4.0456\n",
            "Epoch [1/10], Step [39/250], Loss: 4.5288\n",
            "Epoch [1/10], Step [40/250], Loss: 4.3709\n",
            "Epoch [1/10], Step [41/250], Loss: 5.3337\n",
            "Epoch [1/10], Step [42/250], Loss: 4.9899\n",
            "Epoch [1/10], Step [43/250], Loss: 4.0742\n",
            "Epoch [1/10], Step [44/250], Loss: 5.6725\n",
            "Epoch [1/10], Step [45/250], Loss: 9.9120\n",
            "Epoch [1/10], Step [46/250], Loss: 4.1692\n",
            "Epoch [1/10], Step [47/250], Loss: 4.3566\n",
            "Epoch [1/10], Step [48/250], Loss: 4.2936\n",
            "Epoch [1/10], Step [49/250], Loss: 4.2013\n",
            "Epoch [1/10], Step [50/250], Loss: 4.1341\n",
            "Epoch [1/10], Step [51/250], Loss: 3.0629\n",
            "Epoch [1/10], Step [52/250], Loss: 4.1063\n",
            "Epoch [1/10], Step [53/250], Loss: 3.9339\n",
            "Epoch [1/10], Step [54/250], Loss: 5.4657\n",
            "Epoch [1/10], Step [55/250], Loss: 4.0602\n",
            "Epoch [1/10], Step [56/250], Loss: 3.9333\n",
            "Epoch [1/10], Step [57/250], Loss: 3.7209\n",
            "Epoch [1/10], Step [58/250], Loss: 7.6673\n",
            "Epoch [1/10], Step [59/250], Loss: 4.6889\n",
            "Epoch [1/10], Step [60/250], Loss: 4.0948\n",
            "Epoch [1/10], Step [61/250], Loss: 4.2029\n",
            "Epoch [1/10], Step [62/250], Loss: 4.9049\n",
            "Epoch [1/10], Step [63/250], Loss: 4.2002\n",
            "Epoch [1/10], Step [64/250], Loss: 3.7128\n",
            "Epoch [1/10], Step [65/250], Loss: 4.1843\n",
            "Epoch [1/10], Step [66/250], Loss: 4.5590\n",
            "Epoch [1/10], Step [67/250], Loss: 4.7952\n",
            "Epoch [1/10], Step [68/250], Loss: 4.0820\n",
            "Epoch [1/10], Step [69/250], Loss: 4.0857\n",
            "Epoch [1/10], Step [70/250], Loss: 4.2728\n",
            "Epoch [1/10], Step [71/250], Loss: 3.6961\n",
            "Epoch [1/10], Step [72/250], Loss: 4.3583\n",
            "Epoch [1/10], Step [73/250], Loss: 3.9254\n",
            "Epoch [1/10], Step [74/250], Loss: 3.6976\n",
            "Epoch [1/10], Step [75/250], Loss: 3.8582\n",
            "Epoch [1/10], Step [76/250], Loss: 4.2130\n",
            "Epoch [1/10], Step [77/250], Loss: 4.7522\n",
            "Epoch [1/10], Step [78/250], Loss: 3.7902\n",
            "Epoch [1/10], Step [79/250], Loss: 3.6752\n",
            "Epoch [1/10], Step [80/250], Loss: 3.9943\n",
            "Epoch [1/10], Step [81/250], Loss: 4.0276\n",
            "Epoch [1/10], Step [82/250], Loss: 4.2128\n",
            "Epoch [1/10], Step [83/250], Loss: 4.1978\n",
            "Epoch [1/10], Step [84/250], Loss: 5.9094\n",
            "Epoch [1/10], Step [85/250], Loss: 4.7567\n",
            "Epoch [1/10], Step [86/250], Loss: 4.8659\n",
            "Epoch [1/10], Step [87/250], Loss: 4.5578\n",
            "Epoch [1/10], Step [88/250], Loss: 4.4566\n",
            "Epoch [1/10], Step [89/250], Loss: 4.2332\n",
            "Epoch [1/10], Step [90/250], Loss: 4.4410\n",
            "Epoch [1/10], Step [91/250], Loss: 4.0051\n",
            "Epoch [1/10], Step [92/250], Loss: 4.4987\n",
            "Epoch [1/10], Step [93/250], Loss: 4.2149\n",
            "Epoch [1/10], Step [94/250], Loss: 5.4423\n",
            "Epoch [1/10], Step [95/250], Loss: 5.1461\n",
            "Epoch [1/10], Step [96/250], Loss: 3.7688\n",
            "Epoch [1/10], Step [97/250], Loss: 4.2376\n",
            "Epoch [1/10], Step [98/250], Loss: 4.2203\n",
            "Epoch [1/10], Step [99/250], Loss: 3.9588\n",
            "Epoch [1/10], Step [100/250], Loss: 4.1401\n",
            "Epoch [1/10], Step [101/250], Loss: 4.1285\n",
            "Epoch [1/10], Step [102/250], Loss: 4.1775\n",
            "Epoch [1/10], Step [103/250], Loss: 3.9423\n",
            "Epoch [1/10], Step [104/250], Loss: 4.4078\n",
            "Epoch [1/10], Step [105/250], Loss: 4.0313\n",
            "Epoch [1/10], Step [106/250], Loss: 4.7299\n",
            "Epoch [1/10], Step [107/250], Loss: 4.4430\n",
            "Epoch [1/10], Step [108/250], Loss: 4.3422\n",
            "Epoch [1/10], Step [109/250], Loss: 3.5611\n",
            "Epoch [1/10], Step [110/250], Loss: 4.9494\n",
            "Epoch [1/10], Step [111/250], Loss: 3.9983\n",
            "Epoch [1/10], Step [112/250], Loss: 4.6507\n",
            "Epoch [1/10], Step [113/250], Loss: 4.1787\n",
            "Epoch [1/10], Step [114/250], Loss: 4.1290\n",
            "Epoch [1/10], Step [115/250], Loss: 4.4099\n",
            "Epoch [1/10], Step [116/250], Loss: 4.1164\n",
            "Epoch [1/10], Step [117/250], Loss: 4.1578\n",
            "Epoch [1/10], Step [118/250], Loss: 4.4085\n",
            "Epoch [1/10], Step [119/250], Loss: 4.5985\n",
            "Epoch [1/10], Step [120/250], Loss: 3.8799\n",
            "Epoch [1/10], Step [121/250], Loss: 4.3412\n",
            "Epoch [1/10], Step [122/250], Loss: 4.0137\n",
            "Epoch [1/10], Step [123/250], Loss: 3.9985\n",
            "Epoch [1/10], Step [124/250], Loss: 4.2227\n",
            "Epoch [1/10], Step [125/250], Loss: 7.9537\n",
            "Epoch [1/10], Step [126/250], Loss: 4.0316\n",
            "Epoch [1/10], Step [127/250], Loss: 3.8971\n",
            "Epoch [1/10], Step [128/250], Loss: 6.0069\n",
            "Epoch [1/10], Step [129/250], Loss: 3.9702\n",
            "Epoch [1/10], Step [130/250], Loss: 4.3106\n",
            "Epoch [1/10], Step [131/250], Loss: 3.8518\n",
            "Epoch [1/10], Step [132/250], Loss: 3.9030\n",
            "Epoch [1/10], Step [133/250], Loss: 4.2553\n",
            "Epoch [1/10], Step [134/250], Loss: 4.9939\n",
            "Epoch [1/10], Step [135/250], Loss: 6.9317\n",
            "Epoch [1/10], Step [136/250], Loss: 4.4112\n",
            "Epoch [1/10], Step [137/250], Loss: 4.0867\n",
            "Epoch [1/10], Step [138/250], Loss: 4.2109\n",
            "Epoch [1/10], Step [139/250], Loss: 3.9063\n",
            "Epoch [1/10], Step [140/250], Loss: 4.5000\n",
            "Epoch [1/10], Step [141/250], Loss: 4.4889\n",
            "Epoch [1/10], Step [142/250], Loss: 4.3768\n",
            "Epoch [1/10], Step [143/250], Loss: 4.1672\n",
            "Epoch [1/10], Step [144/250], Loss: 4.1992\n",
            "Epoch [1/10], Step [145/250], Loss: 4.1871\n",
            "Epoch [1/10], Step [146/250], Loss: 3.8604\n",
            "Epoch [1/10], Step [147/250], Loss: 4.1300\n",
            "Epoch [1/10], Step [148/250], Loss: 4.9865\n",
            "Epoch [1/10], Step [149/250], Loss: 4.4437\n",
            "Epoch [1/10], Step [150/250], Loss: 3.6746\n",
            "Epoch [1/10], Step [151/250], Loss: 4.0583\n",
            "Epoch [1/10], Step [152/250], Loss: 4.0027\n",
            "Epoch [1/10], Step [153/250], Loss: 5.8773\n",
            "Epoch [1/10], Step [154/250], Loss: 4.4477\n",
            "Epoch [1/10], Step [155/250], Loss: 4.2248\n",
            "Epoch [1/10], Step [156/250], Loss: 4.0397\n",
            "Epoch [1/10], Step [157/250], Loss: 4.4826\n",
            "Epoch [1/10], Step [158/250], Loss: 4.0549\n",
            "Epoch [1/10], Step [159/250], Loss: 3.9821\n",
            "Epoch [1/10], Step [160/250], Loss: 3.7667\n",
            "Epoch [1/10], Step [161/250], Loss: 4.0321\n",
            "Epoch [1/10], Step [162/250], Loss: 4.6973\n",
            "Epoch [1/10], Step [163/250], Loss: 6.0899\n",
            "Epoch [1/10], Step [164/250], Loss: 3.9370\n",
            "Epoch [1/10], Step [165/250], Loss: 3.9910\n",
            "Epoch [1/10], Step [166/250], Loss: 4.4095\n",
            "Epoch [1/10], Step [167/250], Loss: 4.3545\n",
            "Epoch [1/10], Step [168/250], Loss: 4.1838\n",
            "Epoch [1/10], Step [169/250], Loss: 6.4436\n",
            "Epoch [1/10], Step [170/250], Loss: 5.6474\n",
            "Epoch [1/10], Step [171/250], Loss: 4.4298\n",
            "Epoch [1/10], Step [172/250], Loss: 4.2188\n",
            "Epoch [1/10], Step [173/250], Loss: 4.1030\n",
            "Epoch [1/10], Step [174/250], Loss: 4.0876\n",
            "Epoch [1/10], Step [175/250], Loss: 4.1343\n",
            "Epoch [1/10], Step [176/250], Loss: 3.8892\n",
            "Epoch [1/10], Step [177/250], Loss: 4.9211\n",
            "Epoch [1/10], Step [178/250], Loss: 4.8320\n",
            "Epoch [1/10], Step [179/250], Loss: 4.3369\n",
            "Epoch [1/10], Step [180/250], Loss: 4.1065\n",
            "Epoch [1/10], Step [181/250], Loss: 3.8363\n",
            "Epoch [1/10], Step [182/250], Loss: 3.8059\n",
            "Epoch [1/10], Step [183/250], Loss: 4.7457\n",
            "Epoch [1/10], Step [184/250], Loss: 4.2657\n",
            "Epoch [1/10], Step [185/250], Loss: 3.7762\n",
            "Epoch [1/10], Step [186/250], Loss: 3.6824\n",
            "Epoch [1/10], Step [187/250], Loss: 3.8480\n",
            "Epoch [1/10], Step [188/250], Loss: 3.8062\n",
            "Epoch [1/10], Step [189/250], Loss: 5.6528\n",
            "Epoch [1/10], Step [190/250], Loss: 3.8952\n",
            "Epoch [1/10], Step [191/250], Loss: 4.3111\n",
            "Epoch [1/10], Step [192/250], Loss: 6.7193\n",
            "Epoch [1/10], Step [193/250], Loss: 4.1387\n",
            "Epoch [1/10], Step [194/250], Loss: 13.3481\n",
            "Epoch [1/10], Step [195/250], Loss: 4.4238\n",
            "Epoch [1/10], Step [196/250], Loss: 3.9392\n",
            "Epoch [1/10], Step [197/250], Loss: 4.4878\n",
            "Epoch [1/10], Step [198/250], Loss: 4.5487\n",
            "Epoch [1/10], Step [199/250], Loss: 4.1944\n",
            "Epoch [1/10], Step [200/250], Loss: 3.9301\n",
            "Epoch [1/10], Step [201/250], Loss: 4.4963\n",
            "Epoch [1/10], Step [202/250], Loss: 4.4223\n",
            "Epoch [1/10], Step [203/250], Loss: 4.2674\n",
            "Epoch [1/10], Step [204/250], Loss: 4.5650\n",
            "Epoch [1/10], Step [205/250], Loss: 4.6796\n",
            "Epoch [1/10], Step [206/250], Loss: 6.2118\n",
            "Epoch [1/10], Step [207/250], Loss: 3.9865\n",
            "Epoch [1/10], Step [208/250], Loss: 3.1597\n",
            "Epoch [1/10], Step [209/250], Loss: 3.9218\n",
            "Epoch [1/10], Step [210/250], Loss: 5.1416\n",
            "Epoch [1/10], Step [211/250], Loss: 4.2464\n",
            "Epoch [1/10], Step [212/250], Loss: 4.1282\n",
            "Epoch [1/10], Step [213/250], Loss: 4.1659\n",
            "Epoch [1/10], Step [214/250], Loss: 4.1938\n",
            "Epoch [1/10], Step [215/250], Loss: 4.2825\n",
            "Epoch [1/10], Step [216/250], Loss: 3.9859\n",
            "Epoch [1/10], Step [217/250], Loss: 4.3215\n",
            "Epoch [1/10], Step [218/250], Loss: 4.1924\n",
            "Epoch [1/10], Step [219/250], Loss: 4.1143\n",
            "Epoch [1/10], Step [220/250], Loss: 4.4588\n",
            "Epoch [1/10], Step [221/250], Loss: 4.6918\n",
            "Epoch [1/10], Step [222/250], Loss: 3.7610\n",
            "Epoch [1/10], Step [223/250], Loss: 4.6985\n",
            "Epoch [1/10], Step [224/250], Loss: 3.8577\n",
            "Epoch [1/10], Step [225/250], Loss: 4.3146\n",
            "Epoch [1/10], Step [226/250], Loss: 3.9592\n",
            "Epoch [1/10], Step [227/250], Loss: 4.4306\n",
            "Epoch [1/10], Step [228/250], Loss: 4.0544\n",
            "Epoch [1/10], Step [229/250], Loss: 3.8283\n",
            "Epoch [1/10], Step [230/250], Loss: 4.0566\n",
            "Epoch [1/10], Step [231/250], Loss: 3.9323\n",
            "Epoch [1/10], Step [232/250], Loss: 6.2771\n",
            "Epoch [1/10], Step [233/250], Loss: 4.1077\n",
            "Epoch [1/10], Step [234/250], Loss: 4.0247\n",
            "Epoch [1/10], Step [235/250], Loss: 3.7431\n",
            "Epoch [1/10], Step [236/250], Loss: 4.1052\n",
            "Epoch [1/10], Step [237/250], Loss: 4.3621\n",
            "Epoch [1/10], Step [238/250], Loss: 4.1572\n",
            "Epoch [1/10], Step [239/250], Loss: 6.2631\n",
            "Epoch [1/10], Step [240/250], Loss: 4.6081\n",
            "Epoch [1/10], Step [241/250], Loss: 3.9405\n",
            "Epoch [1/10], Step [242/250], Loss: 4.2460\n",
            "Epoch [1/10], Step [243/250], Loss: 4.5897\n",
            "Epoch [1/10], Step [244/250], Loss: 4.7093\n",
            "Epoch [1/10], Step [245/250], Loss: 4.5020\n",
            "Epoch [1/10], Step [246/250], Loss: 3.0724\n",
            "Epoch [1/10], Step [247/250], Loss: 4.1848\n",
            "Epoch [1/10], Step [248/250], Loss: 4.4184\n",
            "Epoch [1/10], Step [249/250], Loss: 3.9162\n",
            "Epoch [1/10], Step [250/250], Loss: 3.9172\n",
            "Epoch 1: Avg Loss = 32.1704 Current Loss = 3.9172\n",
            "Epoch [2/10], Step [1/250], Loss: 3.9242\n",
            "Epoch [2/10], Step [2/250], Loss: 4.0731\n",
            "Epoch [2/10], Step [3/250], Loss: 4.0262\n",
            "Epoch [2/10], Step [4/250], Loss: 8.0901\n",
            "Epoch [2/10], Step [5/250], Loss: 4.4332\n",
            "Epoch [2/10], Step [6/250], Loss: 3.7636\n",
            "Epoch [2/10], Step [7/250], Loss: 5.7136\n",
            "Epoch [2/10], Step [8/250], Loss: 3.8559\n",
            "Epoch [2/10], Step [9/250], Loss: 4.2505\n",
            "Epoch [2/10], Step [10/250], Loss: 3.7585\n",
            "Epoch [2/10], Step [11/250], Loss: 3.9079\n",
            "Epoch [2/10], Step [12/250], Loss: 9.5598\n",
            "Epoch [2/10], Step [13/250], Loss: 3.9415\n",
            "Epoch [2/10], Step [14/250], Loss: 6.9723\n",
            "Epoch [2/10], Step [15/250], Loss: 4.3442\n",
            "Epoch [2/10], Step [16/250], Loss: 5.2529\n",
            "Epoch [2/10], Step [17/250], Loss: 3.8256\n",
            "Epoch [2/10], Step [18/250], Loss: 4.1137\n",
            "Epoch [2/10], Step [19/250], Loss: 4.1941\n",
            "Epoch [2/10], Step [20/250], Loss: 4.9388\n",
            "Epoch [2/10], Step [21/250], Loss: 4.1193\n",
            "Epoch [2/10], Step [22/250], Loss: 4.2522\n",
            "Epoch [2/10], Step [23/250], Loss: 3.9626\n",
            "Epoch [2/10], Step [24/250], Loss: 4.3354\n",
            "Epoch [2/10], Step [25/250], Loss: 4.1134\n",
            "Epoch [2/10], Step [26/250], Loss: 4.2859\n",
            "Epoch [2/10], Step [27/250], Loss: 4.6055\n",
            "Epoch [2/10], Step [28/250], Loss: 4.2560\n",
            "Epoch [2/10], Step [29/250], Loss: 4.1445\n",
            "Epoch [2/10], Step [30/250], Loss: 4.1955\n",
            "Epoch [2/10], Step [31/250], Loss: 4.0364\n",
            "Epoch [2/10], Step [32/250], Loss: 3.9787\n",
            "Epoch [2/10], Step [33/250], Loss: 3.8628\n",
            "Epoch [2/10], Step [34/250], Loss: 4.1126\n",
            "Epoch [2/10], Step [35/250], Loss: 7.7730\n",
            "Epoch [2/10], Step [36/250], Loss: 5.1637\n",
            "Epoch [2/10], Step [37/250], Loss: 4.1005\n",
            "Epoch [2/10], Step [38/250], Loss: 4.1580\n",
            "Epoch [2/10], Step [39/250], Loss: 3.8981\n",
            "Epoch [2/10], Step [40/250], Loss: 3.9640\n",
            "Epoch [2/10], Step [41/250], Loss: 3.9623\n",
            "Epoch [2/10], Step [42/250], Loss: 4.5878\n",
            "Epoch [2/10], Step [43/250], Loss: 4.1187\n",
            "Epoch [2/10], Step [44/250], Loss: 4.0534\n",
            "Epoch [2/10], Step [45/250], Loss: 4.6017\n",
            "Epoch [2/10], Step [46/250], Loss: 3.9018\n",
            "Epoch [2/10], Step [47/250], Loss: 4.3477\n",
            "Epoch [2/10], Step [48/250], Loss: 3.5327\n",
            "Epoch [2/10], Step [49/250], Loss: 4.0431\n",
            "Epoch [2/10], Step [50/250], Loss: 4.1110\n",
            "Epoch [2/10], Step [51/250], Loss: 3.9311\n",
            "Epoch [2/10], Step [52/250], Loss: 4.0721\n",
            "Epoch [2/10], Step [53/250], Loss: 3.8279\n",
            "Epoch [2/10], Step [54/250], Loss: 3.9476\n",
            "Epoch [2/10], Step [55/250], Loss: 4.0340\n",
            "Epoch [2/10], Step [56/250], Loss: 4.2686\n",
            "Epoch [2/10], Step [57/250], Loss: 3.7822\n",
            "Epoch [2/10], Step [58/250], Loss: 4.0016\n",
            "Epoch [2/10], Step [59/250], Loss: 4.0705\n",
            "Epoch [2/10], Step [60/250], Loss: 5.3230\n",
            "Epoch [2/10], Step [61/250], Loss: 4.3803\n",
            "Epoch [2/10], Step [62/250], Loss: 4.3494\n",
            "Epoch [2/10], Step [63/250], Loss: 4.1922\n",
            "Epoch [2/10], Step [64/250], Loss: 4.5120\n",
            "Epoch [2/10], Step [65/250], Loss: 3.7354\n",
            "Epoch [2/10], Step [66/250], Loss: 4.3728\n",
            "Epoch [2/10], Step [67/250], Loss: 4.1883\n",
            "Epoch [2/10], Step [68/250], Loss: 4.6304\n",
            "Epoch [2/10], Step [69/250], Loss: 4.1811\n",
            "Epoch [2/10], Step [70/250], Loss: 4.0439\n",
            "Epoch [2/10], Step [71/250], Loss: 4.3948\n",
            "Epoch [2/10], Step [72/250], Loss: 4.6484\n",
            "Epoch [2/10], Step [73/250], Loss: 3.8861\n",
            "Epoch [2/10], Step [74/250], Loss: 4.6831\n",
            "Epoch [2/10], Step [75/250], Loss: 3.8725\n",
            "Epoch [2/10], Step [76/250], Loss: 4.0005\n",
            "Epoch [2/10], Step [77/250], Loss: 4.0514\n",
            "Epoch [2/10], Step [78/250], Loss: 4.5410\n",
            "Epoch [2/10], Step [79/250], Loss: 4.3464\n",
            "Epoch [2/10], Step [80/250], Loss: 4.0054\n",
            "Epoch [2/10], Step [81/250], Loss: 4.0913\n",
            "Epoch [2/10], Step [82/250], Loss: 4.2209\n",
            "Epoch [2/10], Step [83/250], Loss: 4.0233\n",
            "Epoch [2/10], Step [84/250], Loss: 4.3475\n",
            "Epoch [2/10], Step [85/250], Loss: 4.0141\n",
            "Epoch [2/10], Step [86/250], Loss: 4.1245\n",
            "Epoch [2/10], Step [87/250], Loss: 3.4846\n",
            "Epoch [2/10], Step [88/250], Loss: 4.0147\n",
            "Epoch [2/10], Step [89/250], Loss: 4.2101\n",
            "Epoch [2/10], Step [90/250], Loss: 3.8467\n",
            "Epoch [2/10], Step [91/250], Loss: 3.7923\n",
            "Epoch [2/10], Step [92/250], Loss: 3.9020\n",
            "Epoch [2/10], Step [93/250], Loss: 3.1886\n",
            "Epoch [2/10], Step [94/250], Loss: 4.1225\n",
            "Epoch [2/10], Step [95/250], Loss: 4.2795\n",
            "Epoch [2/10], Step [96/250], Loss: 5.3846\n",
            "Epoch [2/10], Step [97/250], Loss: 3.9836\n",
            "Epoch [2/10], Step [98/250], Loss: 4.2161\n",
            "Epoch [2/10], Step [99/250], Loss: 4.3598\n",
            "Epoch [2/10], Step [100/250], Loss: 3.9694\n",
            "Epoch [2/10], Step [101/250], Loss: 4.1403\n",
            "Epoch [2/10], Step [102/250], Loss: 5.2825\n",
            "Epoch [2/10], Step [103/250], Loss: 4.1281\n",
            "Epoch [2/10], Step [104/250], Loss: 3.8519\n",
            "Epoch [2/10], Step [105/250], Loss: 4.2766\n",
            "Epoch [2/10], Step [106/250], Loss: 5.8899\n",
            "Epoch [2/10], Step [107/250], Loss: 5.0532\n",
            "Epoch [2/10], Step [108/250], Loss: 5.0303\n",
            "Epoch [2/10], Step [109/250], Loss: 5.3295\n",
            "Epoch [2/10], Step [110/250], Loss: 3.7463\n",
            "Epoch [2/10], Step [111/250], Loss: 4.2084\n",
            "Epoch [2/10], Step [112/250], Loss: 6.2297\n",
            "Epoch [2/10], Step [113/250], Loss: 3.8796\n",
            "Epoch [2/10], Step [114/250], Loss: 5.9069\n",
            "Epoch [2/10], Step [115/250], Loss: 4.0962\n",
            "Epoch [2/10], Step [116/250], Loss: 3.6521\n",
            "Epoch [2/10], Step [117/250], Loss: 4.0374\n",
            "Epoch [2/10], Step [118/250], Loss: 3.9667\n",
            "Epoch [2/10], Step [119/250], Loss: 3.8391\n",
            "Epoch [2/10], Step [120/250], Loss: 3.8721\n",
            "Epoch [2/10], Step [121/250], Loss: 3.8029\n",
            "Epoch [2/10], Step [122/250], Loss: 4.0526\n",
            "Epoch [2/10], Step [123/250], Loss: 4.4129\n",
            "Epoch [2/10], Step [124/250], Loss: 3.6321\n",
            "Epoch [2/10], Step [125/250], Loss: 3.9152\n",
            "Epoch [2/10], Step [126/250], Loss: 5.4416\n",
            "Epoch [2/10], Step [127/250], Loss: 3.6569\n",
            "Epoch [2/10], Step [128/250], Loss: 3.8831\n",
            "Epoch [2/10], Step [129/250], Loss: 4.1658\n",
            "Epoch [2/10], Step [130/250], Loss: 4.0014\n",
            "Epoch [2/10], Step [131/250], Loss: 4.0807\n",
            "Epoch [2/10], Step [132/250], Loss: 4.4408\n",
            "Epoch [2/10], Step [133/250], Loss: 3.8839\n",
            "Epoch [2/10], Step [134/250], Loss: 3.7108\n",
            "Epoch [2/10], Step [135/250], Loss: 4.1382\n",
            "Epoch [2/10], Step [136/250], Loss: 4.2520\n",
            "Epoch [2/10], Step [137/250], Loss: 3.9368\n",
            "Epoch [2/10], Step [138/250], Loss: 5.4631\n",
            "Epoch [2/10], Step [139/250], Loss: 7.4334\n",
            "Epoch [2/10], Step [140/250], Loss: 3.9261\n",
            "Epoch [2/10], Step [141/250], Loss: 3.9925\n",
            "Epoch [2/10], Step [142/250], Loss: 3.9134\n",
            "Epoch [2/10], Step [143/250], Loss: 4.6051\n",
            "Epoch [2/10], Step [144/250], Loss: 5.2042\n",
            "Epoch [2/10], Step [145/250], Loss: 4.0992\n",
            "Epoch [2/10], Step [146/250], Loss: 3.8355\n",
            "Epoch [2/10], Step [147/250], Loss: 4.3050\n",
            "Epoch [2/10], Step [148/250], Loss: 4.0594\n",
            "Epoch [2/10], Step [149/250], Loss: 4.1211\n",
            "Epoch [2/10], Step [150/250], Loss: 3.8208\n",
            "Epoch [2/10], Step [151/250], Loss: 3.9268\n",
            "Epoch [2/10], Step [152/250], Loss: 4.0768\n",
            "Epoch [2/10], Step [153/250], Loss: 3.9303\n",
            "Epoch [2/10], Step [154/250], Loss: 3.9242\n",
            "Epoch [2/10], Step [155/250], Loss: 4.2710\n",
            "Epoch [2/10], Step [156/250], Loss: 4.4929\n",
            "Epoch [2/10], Step [157/250], Loss: 6.2405\n",
            "Epoch [2/10], Step [158/250], Loss: 4.2088\n",
            "Epoch [2/10], Step [159/250], Loss: 4.0105\n",
            "Epoch [2/10], Step [160/250], Loss: 4.6334\n",
            "Epoch [2/10], Step [161/250], Loss: 3.8279\n",
            "Epoch [2/10], Step [162/250], Loss: 3.9350\n",
            "Epoch [2/10], Step [163/250], Loss: 3.8003\n",
            "Epoch [2/10], Step [164/250], Loss: 3.9154\n",
            "Epoch [2/10], Step [165/250], Loss: 4.0114\n",
            "Epoch [2/10], Step [166/250], Loss: 3.7854\n",
            "Epoch [2/10], Step [167/250], Loss: 3.9596\n",
            "Epoch [2/10], Step [168/250], Loss: 3.8275\n",
            "Epoch [2/10], Step [169/250], Loss: 4.1723\n",
            "Epoch [2/10], Step [170/250], Loss: 4.3064\n",
            "Epoch [2/10], Step [171/250], Loss: 3.9355\n",
            "Epoch [2/10], Step [172/250], Loss: 4.3944\n",
            "Epoch [2/10], Step [173/250], Loss: 3.8432\n",
            "Epoch [2/10], Step [174/250], Loss: 3.3235\n",
            "Epoch [2/10], Step [175/250], Loss: 3.6544\n",
            "Epoch [2/10], Step [176/250], Loss: 4.0948\n",
            "Epoch [2/10], Step [177/250], Loss: 4.7755\n",
            "Epoch [2/10], Step [178/250], Loss: 3.8244\n",
            "Epoch [2/10], Step [179/250], Loss: 4.0935\n",
            "Epoch [2/10], Step [180/250], Loss: 3.8801\n",
            "Epoch [2/10], Step [181/250], Loss: 3.8695\n",
            "Epoch [2/10], Step [182/250], Loss: 3.8392\n",
            "Epoch [2/10], Step [183/250], Loss: 3.8204\n",
            "Epoch [2/10], Step [184/250], Loss: 4.5090\n",
            "Epoch [2/10], Step [185/250], Loss: 3.8882\n",
            "Epoch [2/10], Step [186/250], Loss: 3.8690\n",
            "Epoch [2/10], Step [187/250], Loss: 5.1720\n",
            "Epoch [2/10], Step [188/250], Loss: 4.7115\n",
            "Epoch [2/10], Step [189/250], Loss: 4.1771\n",
            "Epoch [2/10], Step [190/250], Loss: 4.2811\n",
            "Epoch [2/10], Step [191/250], Loss: 4.2697\n",
            "Epoch [2/10], Step [192/250], Loss: 4.6208\n",
            "Epoch [2/10], Step [193/250], Loss: 4.0679\n",
            "Epoch [2/10], Step [194/250], Loss: 4.0085\n",
            "Epoch [2/10], Step [195/250], Loss: 4.3749\n",
            "Epoch [2/10], Step [196/250], Loss: 3.9465\n",
            "Epoch [2/10], Step [197/250], Loss: 4.0285\n",
            "Epoch [2/10], Step [198/250], Loss: 3.9560\n",
            "Epoch [2/10], Step [199/250], Loss: 4.2814\n",
            "Epoch [2/10], Step [200/250], Loss: 4.0862\n",
            "Epoch [2/10], Step [201/250], Loss: 3.9990\n",
            "Epoch [2/10], Step [202/250], Loss: 4.5917\n",
            "Epoch [2/10], Step [203/250], Loss: 4.1859\n",
            "Epoch [2/10], Step [204/250], Loss: 3.9101\n",
            "Epoch [2/10], Step [205/250], Loss: 3.9728\n",
            "Epoch [2/10], Step [206/250], Loss: 4.2920\n",
            "Epoch [2/10], Step [207/250], Loss: 4.0821\n",
            "Epoch [2/10], Step [208/250], Loss: 4.1002\n",
            "Epoch [2/10], Step [209/250], Loss: 5.0408\n",
            "Epoch [2/10], Step [210/250], Loss: 4.0171\n",
            "Epoch [2/10], Step [211/250], Loss: 3.8480\n",
            "Epoch [2/10], Step [212/250], Loss: 3.7536\n",
            "Epoch [2/10], Step [213/250], Loss: 4.4477\n",
            "Epoch [2/10], Step [214/250], Loss: 3.9377\n",
            "Epoch [2/10], Step [215/250], Loss: 3.9855\n",
            "Epoch [2/10], Step [216/250], Loss: 3.7485\n",
            "Epoch [2/10], Step [217/250], Loss: 4.9327\n",
            "Epoch [2/10], Step [218/250], Loss: 4.9707\n",
            "Epoch [2/10], Step [219/250], Loss: 4.3166\n",
            "Epoch [2/10], Step [220/250], Loss: 3.8367\n",
            "Epoch [2/10], Step [221/250], Loss: 3.9213\n",
            "Epoch [2/10], Step [222/250], Loss: 4.2291\n",
            "Epoch [2/10], Step [223/250], Loss: 3.4671\n",
            "Epoch [2/10], Step [224/250], Loss: 4.0423\n",
            "Epoch [2/10], Step [225/250], Loss: 4.0622\n",
            "Epoch [2/10], Step [226/250], Loss: 4.4852\n",
            "Epoch [2/10], Step [227/250], Loss: 3.9347\n",
            "Epoch [2/10], Step [228/250], Loss: 3.8001\n",
            "Epoch [2/10], Step [229/250], Loss: 3.9271\n",
            "Epoch [2/10], Step [230/250], Loss: 3.7378\n",
            "Epoch [2/10], Step [231/250], Loss: 7.7518\n",
            "Epoch [2/10], Step [232/250], Loss: 4.5421\n",
            "Epoch [2/10], Step [233/250], Loss: 3.8229\n",
            "Epoch [2/10], Step [234/250], Loss: 4.2066\n",
            "Epoch [2/10], Step [235/250], Loss: 3.9186\n",
            "Epoch [2/10], Step [236/250], Loss: 3.9547\n",
            "Epoch [2/10], Step [237/250], Loss: 4.1871\n",
            "Epoch [2/10], Step [238/250], Loss: 4.0554\n",
            "Epoch [2/10], Step [239/250], Loss: 5.1512\n",
            "Epoch [2/10], Step [240/250], Loss: 3.9791\n",
            "Epoch [2/10], Step [241/250], Loss: 4.1164\n",
            "Epoch [2/10], Step [242/250], Loss: 5.8887\n",
            "Epoch [2/10], Step [243/250], Loss: 4.3386\n",
            "Epoch [2/10], Step [244/250], Loss: 4.0051\n",
            "Epoch [2/10], Step [245/250], Loss: 4.0485\n",
            "Epoch [2/10], Step [246/250], Loss: 3.5464\n",
            "Epoch [2/10], Step [247/250], Loss: 3.9507\n",
            "Epoch [2/10], Step [248/250], Loss: 4.1216\n",
            "Epoch [2/10], Step [249/250], Loss: 4.1030\n",
            "Epoch [2/10], Step [250/250], Loss: 3.8553\n",
            "Epoch 2: Avg Loss = 4.2798 Current Loss = 3.8553\n",
            "Epoch [3/10], Step [1/250], Loss: 3.7592\n",
            "Epoch [3/10], Step [2/250], Loss: 4.0400\n",
            "Epoch [3/10], Step [3/250], Loss: 4.0298\n",
            "Epoch [3/10], Step [4/250], Loss: 3.7879\n",
            "Epoch [3/10], Step [5/250], Loss: 4.3802\n",
            "Epoch [3/10], Step [6/250], Loss: 4.3118\n",
            "Epoch [3/10], Step [7/250], Loss: 3.9972\n",
            "Epoch [3/10], Step [8/250], Loss: 3.8336\n",
            "Epoch [3/10], Step [9/250], Loss: 3.7662\n",
            "Epoch [3/10], Step [10/250], Loss: 3.7055\n",
            "Epoch [3/10], Step [11/250], Loss: 5.1860\n",
            "Epoch [3/10], Step [12/250], Loss: 3.8665\n",
            "Epoch [3/10], Step [13/250], Loss: 4.2423\n",
            "Epoch [3/10], Step [14/250], Loss: 4.5597\n",
            "Epoch [3/10], Step [15/250], Loss: 4.0445\n",
            "Epoch [3/10], Step [16/250], Loss: 4.3121\n",
            "Epoch [3/10], Step [17/250], Loss: 3.9961\n",
            "Epoch [3/10], Step [18/250], Loss: 3.8256\n",
            "Epoch [3/10], Step [19/250], Loss: 3.8035\n",
            "Epoch [3/10], Step [20/250], Loss: 3.7370\n",
            "Epoch [3/10], Step [21/250], Loss: 4.0802\n",
            "Epoch [3/10], Step [22/250], Loss: 4.0705\n",
            "Epoch [3/10], Step [23/250], Loss: 3.8083\n",
            "Epoch [3/10], Step [24/250], Loss: 3.9316\n",
            "Epoch [3/10], Step [25/250], Loss: 4.2465\n",
            "Epoch [3/10], Step [26/250], Loss: 3.8501\n",
            "Epoch [3/10], Step [27/250], Loss: 3.7671\n",
            "Epoch [3/10], Step [28/250], Loss: 3.3736\n",
            "Epoch [3/10], Step [29/250], Loss: 4.2100\n",
            "Epoch [3/10], Step [30/250], Loss: 4.0538\n",
            "Epoch [3/10], Step [31/250], Loss: 3.9682\n",
            "Epoch [3/10], Step [32/250], Loss: 4.5733\n",
            "Epoch [3/10], Step [33/250], Loss: 4.3157\n",
            "Epoch [3/10], Step [34/250], Loss: 4.1478\n",
            "Epoch [3/10], Step [35/250], Loss: 4.1219\n",
            "Epoch [3/10], Step [36/250], Loss: 4.3357\n",
            "Epoch [3/10], Step [37/250], Loss: 4.1001\n",
            "Epoch [3/10], Step [38/250], Loss: 5.3641\n",
            "Epoch [3/10], Step [39/250], Loss: 4.4076\n",
            "Epoch [3/10], Step [40/250], Loss: 4.6513\n",
            "Epoch [3/10], Step [41/250], Loss: 6.3671\n",
            "Epoch [3/10], Step [42/250], Loss: 3.9509\n",
            "Epoch [3/10], Step [43/250], Loss: 4.4304\n",
            "Epoch [3/10], Step [44/250], Loss: 4.9758\n",
            "Epoch [3/10], Step [45/250], Loss: 3.9450\n",
            "Epoch [3/10], Step [46/250], Loss: 3.8004\n",
            "Epoch [3/10], Step [47/250], Loss: 3.7893\n",
            "Epoch [3/10], Step [48/250], Loss: 4.3181\n",
            "Epoch [3/10], Step [49/250], Loss: 3.7329\n",
            "Epoch [3/10], Step [50/250], Loss: 3.9326\n",
            "Epoch [3/10], Step [51/250], Loss: 4.2048\n",
            "Epoch [3/10], Step [52/250], Loss: 4.0123\n",
            "Epoch [3/10], Step [53/250], Loss: 4.0569\n",
            "Epoch [3/10], Step [54/250], Loss: 4.3487\n",
            "Epoch [3/10], Step [55/250], Loss: 4.2664\n",
            "Epoch [3/10], Step [56/250], Loss: 3.9377\n",
            "Epoch [3/10], Step [57/250], Loss: 3.9826\n",
            "Epoch [3/10], Step [58/250], Loss: 4.5216\n",
            "Epoch [3/10], Step [59/250], Loss: 3.7844\n",
            "Epoch [3/10], Step [60/250], Loss: 4.0972\n",
            "Epoch [3/10], Step [61/250], Loss: 4.0884\n",
            "Epoch [3/10], Step [62/250], Loss: 4.1630\n",
            "Epoch [3/10], Step [63/250], Loss: 3.8714\n",
            "Epoch [3/10], Step [64/250], Loss: 3.9839\n",
            "Epoch [3/10], Step [65/250], Loss: 3.9623\n",
            "Epoch [3/10], Step [66/250], Loss: 4.2645\n",
            "Epoch [3/10], Step [67/250], Loss: 3.6123\n",
            "Epoch [3/10], Step [68/250], Loss: 3.9348\n",
            "Epoch [3/10], Step [69/250], Loss: 4.5642\n",
            "Epoch [3/10], Step [70/250], Loss: 3.8783\n",
            "Epoch [3/10], Step [71/250], Loss: 3.9440\n",
            "Epoch [3/10], Step [72/250], Loss: 4.2493\n",
            "Epoch [3/10], Step [73/250], Loss: 4.4205\n",
            "Epoch [3/10], Step [74/250], Loss: 3.8212\n",
            "Epoch [3/10], Step [75/250], Loss: 4.8688\n",
            "Epoch [3/10], Step [76/250], Loss: 4.4762\n",
            "Epoch [3/10], Step [77/250], Loss: 4.0818\n",
            "Epoch [3/10], Step [78/250], Loss: 3.8686\n",
            "Epoch [3/10], Step [79/250], Loss: 4.3408\n",
            "Epoch [3/10], Step [80/250], Loss: 4.1052\n",
            "Epoch [3/10], Step [81/250], Loss: 3.7255\n",
            "Epoch [3/10], Step [82/250], Loss: 3.8246\n",
            "Epoch [3/10], Step [83/250], Loss: 4.0480\n",
            "Epoch [3/10], Step [84/250], Loss: 3.9311\n",
            "Epoch [3/10], Step [85/250], Loss: 3.9296\n",
            "Epoch [3/10], Step [86/250], Loss: 3.8609\n",
            "Epoch [3/10], Step [87/250], Loss: 3.7919\n",
            "Epoch [3/10], Step [88/250], Loss: 4.5714\n",
            "Epoch [3/10], Step [89/250], Loss: 3.7999\n",
            "Epoch [3/10], Step [90/250], Loss: 3.7710\n",
            "Epoch [3/10], Step [91/250], Loss: 4.0805\n",
            "Epoch [3/10], Step [92/250], Loss: 4.1372\n",
            "Epoch [3/10], Step [93/250], Loss: 3.9462\n",
            "Epoch [3/10], Step [94/250], Loss: 4.3764\n",
            "Epoch [3/10], Step [95/250], Loss: 4.1224\n",
            "Epoch [3/10], Step [96/250], Loss: 3.6696\n",
            "Epoch [3/10], Step [97/250], Loss: 4.1478\n",
            "Epoch [3/10], Step [98/250], Loss: 4.3310\n",
            "Epoch [3/10], Step [99/250], Loss: 3.9040\n",
            "Epoch [3/10], Step [100/250], Loss: 6.3644\n",
            "Epoch [3/10], Step [101/250], Loss: 4.2963\n",
            "Epoch [3/10], Step [102/250], Loss: 3.8873\n",
            "Epoch [3/10], Step [103/250], Loss: 5.4274\n",
            "Epoch [3/10], Step [104/250], Loss: 3.4791\n",
            "Epoch [3/10], Step [105/250], Loss: 6.2927\n",
            "Epoch [3/10], Step [106/250], Loss: 4.0020\n",
            "Epoch [3/10], Step [107/250], Loss: 8.8244\n",
            "Epoch [3/10], Step [108/250], Loss: 4.2932\n",
            "Epoch [3/10], Step [109/250], Loss: 3.8912\n",
            "Epoch [3/10], Step [110/250], Loss: 3.9775\n",
            "Epoch [3/10], Step [111/250], Loss: 4.0139\n",
            "Epoch [3/10], Step [112/250], Loss: 4.2027\n",
            "Epoch [3/10], Step [113/250], Loss: 3.9846\n",
            "Epoch [3/10], Step [114/250], Loss: 3.7551\n",
            "Epoch [3/10], Step [115/250], Loss: 3.8041\n",
            "Epoch [3/10], Step [116/250], Loss: 3.8837\n",
            "Epoch [3/10], Step [117/250], Loss: 3.6609\n",
            "Epoch [3/10], Step [118/250], Loss: 3.8081\n",
            "Epoch [3/10], Step [119/250], Loss: 3.9549\n",
            "Epoch [3/10], Step [120/250], Loss: 4.3707\n",
            "Epoch [3/10], Step [121/250], Loss: 5.2408\n",
            "Epoch [3/10], Step [122/250], Loss: 4.3306\n",
            "Epoch [3/10], Step [123/250], Loss: 4.0256\n",
            "Epoch [3/10], Step [124/250], Loss: 4.2895\n",
            "Epoch [3/10], Step [125/250], Loss: 3.6895\n",
            "Epoch [3/10], Step [126/250], Loss: 4.3691\n",
            "Epoch [3/10], Step [127/250], Loss: 3.9753\n",
            "Epoch [3/10], Step [128/250], Loss: 3.8607\n",
            "Epoch [3/10], Step [129/250], Loss: 4.4864\n",
            "Epoch [3/10], Step [130/250], Loss: 3.9410\n",
            "Epoch [3/10], Step [131/250], Loss: 3.8263\n",
            "Epoch [3/10], Step [132/250], Loss: 3.9571\n",
            "Epoch [3/10], Step [133/250], Loss: 4.1855\n",
            "Epoch [3/10], Step [134/250], Loss: 3.9934\n",
            "Epoch [3/10], Step [135/250], Loss: 4.7946\n",
            "Epoch [3/10], Step [136/250], Loss: 3.6098\n",
            "Epoch [3/10], Step [137/250], Loss: 3.8566\n",
            "Epoch [3/10], Step [138/250], Loss: 3.8308\n",
            "Epoch [3/10], Step [139/250], Loss: 3.7713\n",
            "Epoch [3/10], Step [140/250], Loss: 4.1174\n",
            "Epoch [3/10], Step [141/250], Loss: 4.6789\n",
            "Epoch [3/10], Step [142/250], Loss: 4.1275\n",
            "Epoch [3/10], Step [143/250], Loss: 4.1983\n",
            "Epoch [3/10], Step [144/250], Loss: 6.9487\n",
            "Epoch [3/10], Step [145/250], Loss: 3.9520\n",
            "Epoch [3/10], Step [146/250], Loss: 6.3999\n",
            "Epoch [3/10], Step [147/250], Loss: 4.2813\n",
            "Epoch [3/10], Step [148/250], Loss: 4.3930\n",
            "Epoch [3/10], Step [149/250], Loss: 4.2336\n",
            "Epoch [3/10], Step [150/250], Loss: 4.5850\n",
            "Epoch [3/10], Step [151/250], Loss: 4.2674\n",
            "Epoch [3/10], Step [152/250], Loss: 4.2316\n",
            "Epoch [3/10], Step [153/250], Loss: 4.2445\n",
            "Epoch [3/10], Step [154/250], Loss: 5.8372\n",
            "Epoch [3/10], Step [155/250], Loss: 3.9873\n",
            "Epoch [3/10], Step [156/250], Loss: 4.0433\n",
            "Epoch [3/10], Step [157/250], Loss: 3.8337\n",
            "Epoch [3/10], Step [158/250], Loss: 4.0757\n",
            "Epoch [3/10], Step [159/250], Loss: 4.0359\n",
            "Epoch [3/10], Step [160/250], Loss: 3.7062\n",
            "Epoch [3/10], Step [161/250], Loss: 4.1166\n",
            "Epoch [3/10], Step [162/250], Loss: 4.0259\n",
            "Epoch [3/10], Step [163/250], Loss: 3.8506\n",
            "Epoch [3/10], Step [164/250], Loss: 3.9839\n",
            "Epoch [3/10], Step [165/250], Loss: 3.8520\n",
            "Epoch [3/10], Step [166/250], Loss: 3.8101\n",
            "Epoch [3/10], Step [167/250], Loss: 4.0425\n",
            "Epoch [3/10], Step [168/250], Loss: 4.2140\n",
            "Epoch [3/10], Step [169/250], Loss: 3.8476\n",
            "Epoch [3/10], Step [170/250], Loss: 4.1437\n",
            "Epoch [3/10], Step [171/250], Loss: 4.0328\n",
            "Epoch [3/10], Step [172/250], Loss: 4.2702\n",
            "Epoch [3/10], Step [173/250], Loss: 3.8546\n",
            "Epoch [3/10], Step [174/250], Loss: 3.8881\n",
            "Epoch [3/10], Step [175/250], Loss: 4.2527\n",
            "Epoch [3/10], Step [176/250], Loss: 3.8096\n",
            "Epoch [3/10], Step [177/250], Loss: 4.0389\n",
            "Epoch [3/10], Step [178/250], Loss: 4.1181\n",
            "Epoch [3/10], Step [179/250], Loss: 4.2601\n",
            "Epoch [3/10], Step [180/250], Loss: 3.6650\n",
            "Epoch [3/10], Step [181/250], Loss: 4.0354\n",
            "Epoch [3/10], Step [182/250], Loss: 4.0642\n",
            "Epoch [3/10], Step [183/250], Loss: 4.7031\n",
            "Epoch [3/10], Step [184/250], Loss: 3.7549\n",
            "Epoch [3/10], Step [185/250], Loss: 3.6773\n",
            "Epoch [3/10], Step [186/250], Loss: 5.4353\n",
            "Epoch [3/10], Step [187/250], Loss: 3.8064\n",
            "Epoch [3/10], Step [188/250], Loss: 3.8740\n",
            "Epoch [3/10], Step [189/250], Loss: 3.8978\n",
            "Epoch [3/10], Step [190/250], Loss: 7.7270\n",
            "Epoch [3/10], Step [191/250], Loss: 3.9116\n",
            "Epoch [3/10], Step [192/250], Loss: 3.8872\n",
            "Epoch [3/10], Step [193/250], Loss: 4.3034\n",
            "Epoch [3/10], Step [194/250], Loss: 3.7485\n",
            "Epoch [3/10], Step [195/250], Loss: 4.1063\n",
            "Epoch [3/10], Step [196/250], Loss: 4.0868\n",
            "Epoch [3/10], Step [197/250], Loss: 4.5952\n",
            "Epoch [3/10], Step [198/250], Loss: 3.6915\n",
            "Epoch [3/10], Step [199/250], Loss: 4.2080\n",
            "Epoch [3/10], Step [200/250], Loss: 3.8998\n",
            "Epoch [3/10], Step [201/250], Loss: 3.9269\n",
            "Epoch [3/10], Step [202/250], Loss: 4.4223\n",
            "Epoch [3/10], Step [203/250], Loss: 4.0708\n",
            "Epoch [3/10], Step [204/250], Loss: 4.1644\n",
            "Epoch [3/10], Step [205/250], Loss: 3.9192\n",
            "Epoch [3/10], Step [206/250], Loss: 4.0318\n",
            "Epoch [3/10], Step [207/250], Loss: 4.0334\n",
            "Epoch [3/10], Step [208/250], Loss: 3.7473\n",
            "Epoch [3/10], Step [209/250], Loss: 5.7586\n",
            "Epoch [3/10], Step [210/250], Loss: 4.4179\n",
            "Epoch [3/10], Step [211/250], Loss: 4.2091\n",
            "Epoch [3/10], Step [212/250], Loss: 3.7216\n",
            "Epoch [3/10], Step [213/250], Loss: 4.3883\n",
            "Epoch [3/10], Step [214/250], Loss: 4.3039\n",
            "Epoch [3/10], Step [215/250], Loss: 4.2677\n",
            "Epoch [3/10], Step [216/250], Loss: 5.9518\n",
            "Epoch [3/10], Step [217/250], Loss: 4.0510\n",
            "Epoch [3/10], Step [218/250], Loss: 3.9746\n",
            "Epoch [3/10], Step [219/250], Loss: 3.8391\n",
            "Epoch [3/10], Step [220/250], Loss: 3.9014\n",
            "Epoch [3/10], Step [221/250], Loss: 3.5969\n",
            "Epoch [3/10], Step [222/250], Loss: 4.2262\n",
            "Epoch [3/10], Step [223/250], Loss: 4.0392\n",
            "Epoch [3/10], Step [224/250], Loss: 6.2871\n",
            "Epoch [3/10], Step [225/250], Loss: 4.0764\n",
            "Epoch [3/10], Step [226/250], Loss: 3.7440\n",
            "Epoch [3/10], Step [227/250], Loss: 4.0498\n",
            "Epoch [3/10], Step [228/250], Loss: 4.0788\n",
            "Epoch [3/10], Step [229/250], Loss: 3.8085\n",
            "Epoch [3/10], Step [230/250], Loss: 4.5557\n",
            "Epoch [3/10], Step [231/250], Loss: 4.1350\n",
            "Epoch [3/10], Step [232/250], Loss: 4.0188\n",
            "Epoch [3/10], Step [233/250], Loss: 4.1236\n",
            "Epoch [3/10], Step [234/250], Loss: 3.9977\n",
            "Epoch [3/10], Step [235/250], Loss: 3.9772\n",
            "Epoch [3/10], Step [236/250], Loss: 3.9217\n",
            "Epoch [3/10], Step [237/250], Loss: 4.6183\n",
            "Epoch [3/10], Step [238/250], Loss: 4.1625\n",
            "Epoch [3/10], Step [239/250], Loss: 4.4583\n",
            "Epoch [3/10], Step [240/250], Loss: 5.0328\n",
            "Epoch [3/10], Step [241/250], Loss: 3.8577\n",
            "Epoch [3/10], Step [242/250], Loss: 4.1939\n",
            "Epoch [3/10], Step [243/250], Loss: 4.1293\n",
            "Epoch [3/10], Step [244/250], Loss: 3.9718\n",
            "Epoch [3/10], Step [245/250], Loss: 4.4897\n",
            "Epoch [3/10], Step [246/250], Loss: 4.0388\n",
            "Epoch [3/10], Step [247/250], Loss: 4.1578\n",
            "Epoch [3/10], Step [248/250], Loss: 4.1042\n",
            "Epoch [3/10], Step [249/250], Loss: 3.7947\n",
            "Epoch [3/10], Step [250/250], Loss: 4.0764\n",
            "Epoch 3: Avg Loss = 4.2007 Current Loss = 4.0764\n",
            "Epoch [4/10], Step [1/250], Loss: 4.0956\n",
            "Epoch [4/10], Step [2/250], Loss: 4.1635\n",
            "Epoch [4/10], Step [3/250], Loss: 5.4566\n",
            "Epoch [4/10], Step [4/250], Loss: 3.4254\n",
            "Epoch [4/10], Step [5/250], Loss: 3.9950\n",
            "Epoch [4/10], Step [6/250], Loss: 4.1018\n",
            "Epoch [4/10], Step [7/250], Loss: 3.4766\n",
            "Epoch [4/10], Step [8/250], Loss: 3.5004\n",
            "Epoch [4/10], Step [9/250], Loss: 4.0631\n",
            "Epoch [4/10], Step [10/250], Loss: 3.6121\n",
            "Epoch [4/10], Step [11/250], Loss: 4.2363\n",
            "Epoch [4/10], Step [12/250], Loss: 4.0392\n",
            "Epoch [4/10], Step [13/250], Loss: 4.0570\n",
            "Epoch [4/10], Step [14/250], Loss: 4.2118\n",
            "Epoch [4/10], Step [15/250], Loss: 5.3798\n",
            "Epoch [4/10], Step [16/250], Loss: 4.0681\n",
            "Epoch [4/10], Step [17/250], Loss: 3.9657\n",
            "Epoch [4/10], Step [18/250], Loss: 3.9064\n",
            "Epoch [4/10], Step [19/250], Loss: 3.8936\n",
            "Epoch [4/10], Step [20/250], Loss: 3.9768\n",
            "Epoch [4/10], Step [21/250], Loss: 3.8618\n",
            "Epoch [4/10], Step [22/250], Loss: 3.8338\n",
            "Epoch [4/10], Step [23/250], Loss: 4.1655\n",
            "Epoch [4/10], Step [24/250], Loss: 5.3459\n",
            "Epoch [4/10], Step [25/250], Loss: 3.6481\n",
            "Epoch [4/10], Step [26/250], Loss: 5.5070\n",
            "Epoch [4/10], Step [27/250], Loss: 4.1612\n",
            "Epoch [4/10], Step [28/250], Loss: 3.5265\n",
            "Epoch [4/10], Step [29/250], Loss: 3.8423\n",
            "Epoch [4/10], Step [30/250], Loss: 3.9177\n",
            "Epoch [4/10], Step [31/250], Loss: 3.7838\n",
            "Epoch [4/10], Step [32/250], Loss: 3.8861\n",
            "Epoch [4/10], Step [33/250], Loss: 4.3721\n",
            "Epoch [4/10], Step [34/250], Loss: 3.9075\n",
            "Epoch [4/10], Step [35/250], Loss: 3.5587\n",
            "Epoch [4/10], Step [36/250], Loss: 4.1675\n",
            "Epoch [4/10], Step [37/250], Loss: 4.0305\n",
            "Epoch [4/10], Step [38/250], Loss: 3.9494\n",
            "Epoch [4/10], Step [39/250], Loss: 4.8047\n",
            "Epoch [4/10], Step [40/250], Loss: 4.8912\n",
            "Epoch [4/10], Step [41/250], Loss: 4.0103\n",
            "Epoch [4/10], Step [42/250], Loss: 4.9850\n",
            "Epoch [4/10], Step [43/250], Loss: 3.9635\n",
            "Epoch [4/10], Step [44/250], Loss: 4.0606\n",
            "Epoch [4/10], Step [45/250], Loss: 4.2608\n",
            "Epoch [4/10], Step [46/250], Loss: 3.8170\n",
            "Epoch [4/10], Step [47/250], Loss: 3.9598\n",
            "Epoch [4/10], Step [48/250], Loss: 4.0260\n",
            "Epoch [4/10], Step [49/250], Loss: 5.5318\n",
            "Epoch [4/10], Step [50/250], Loss: 4.2439\n",
            "Epoch [4/10], Step [51/250], Loss: 4.0507\n",
            "Epoch [4/10], Step [52/250], Loss: 4.0220\n",
            "Epoch [4/10], Step [53/250], Loss: 3.8833\n",
            "Epoch [4/10], Step [54/250], Loss: 3.8227\n",
            "Epoch [4/10], Step [55/250], Loss: 3.8056\n",
            "Epoch [4/10], Step [56/250], Loss: 3.7064\n",
            "Epoch [4/10], Step [57/250], Loss: 3.9975\n",
            "Epoch [4/10], Step [58/250], Loss: 4.1858\n",
            "Epoch [4/10], Step [59/250], Loss: 3.9753\n",
            "Epoch [4/10], Step [60/250], Loss: 5.5210\n",
            "Epoch [4/10], Step [61/250], Loss: 3.8340\n",
            "Epoch [4/10], Step [62/250], Loss: 4.4610\n",
            "Epoch [4/10], Step [63/250], Loss: 3.7751\n",
            "Epoch [4/10], Step [64/250], Loss: 4.1182\n",
            "Epoch [4/10], Step [65/250], Loss: 4.0417\n",
            "Epoch [4/10], Step [66/250], Loss: 4.0313\n",
            "Epoch [4/10], Step [67/250], Loss: 4.0940\n",
            "Epoch [4/10], Step [68/250], Loss: 3.8717\n",
            "Epoch [4/10], Step [69/250], Loss: 3.8059\n",
            "Epoch [4/10], Step [70/250], Loss: 3.8951\n",
            "Epoch [4/10], Step [71/250], Loss: 3.9085\n",
            "Epoch [4/10], Step [72/250], Loss: 4.9565\n",
            "Epoch [4/10], Step [73/250], Loss: 4.1923\n",
            "Epoch [4/10], Step [74/250], Loss: 4.3003\n",
            "Epoch [4/10], Step [75/250], Loss: 4.6782\n",
            "Epoch [4/10], Step [76/250], Loss: 4.3214\n",
            "Epoch [4/10], Step [77/250], Loss: 3.8334\n",
            "Epoch [4/10], Step [78/250], Loss: 4.4705\n",
            "Epoch [4/10], Step [79/250], Loss: 3.8658\n",
            "Epoch [4/10], Step [80/250], Loss: 3.9658\n",
            "Epoch [4/10], Step [81/250], Loss: 3.9984\n",
            "Epoch [4/10], Step [82/250], Loss: 3.8472\n",
            "Epoch [4/10], Step [83/250], Loss: 3.9854\n",
            "Epoch [4/10], Step [84/250], Loss: 4.3672\n",
            "Epoch [4/10], Step [85/250], Loss: 3.9366\n",
            "Epoch [4/10], Step [86/250], Loss: 3.8273\n",
            "Epoch [4/10], Step [87/250], Loss: 4.0104\n",
            "Epoch [4/10], Step [88/250], Loss: 3.8317\n",
            "Epoch [4/10], Step [89/250], Loss: 3.9306\n",
            "Epoch [4/10], Step [90/250], Loss: 3.7832\n",
            "Epoch [4/10], Step [91/250], Loss: 4.0271\n",
            "Epoch [4/10], Step [92/250], Loss: 4.2013\n",
            "Epoch [4/10], Step [93/250], Loss: 3.9718\n",
            "Epoch [4/10], Step [94/250], Loss: 4.1526\n",
            "Epoch [4/10], Step [95/250], Loss: 3.8264\n",
            "Epoch [4/10], Step [96/250], Loss: 3.8956\n",
            "Epoch [4/10], Step [97/250], Loss: 4.1571\n",
            "Epoch [4/10], Step [98/250], Loss: 4.2296\n",
            "Epoch [4/10], Step [99/250], Loss: 3.7971\n",
            "Epoch [4/10], Step [100/250], Loss: 3.6905\n",
            "Epoch [4/10], Step [101/250], Loss: 4.0302\n",
            "Epoch [4/10], Step [102/250], Loss: 3.9369\n",
            "Epoch [4/10], Step [103/250], Loss: 4.3898\n",
            "Epoch [4/10], Step [104/250], Loss: 3.6318\n",
            "Epoch [4/10], Step [105/250], Loss: 4.5987\n",
            "Epoch [4/10], Step [106/250], Loss: 3.9256\n",
            "Epoch [4/10], Step [107/250], Loss: 3.8432\n",
            "Epoch [4/10], Step [108/250], Loss: 4.0376\n",
            "Epoch [4/10], Step [109/250], Loss: 3.8878\n",
            "Epoch [4/10], Step [110/250], Loss: 3.9220\n",
            "Epoch [4/10], Step [111/250], Loss: 3.9225\n",
            "Epoch [4/10], Step [112/250], Loss: 5.7461\n",
            "Epoch [4/10], Step [113/250], Loss: 4.2683\n",
            "Epoch [4/10], Step [114/250], Loss: 3.8717\n",
            "Epoch [4/10], Step [115/250], Loss: 3.6784\n",
            "Epoch [4/10], Step [116/250], Loss: 6.5199\n",
            "Epoch [4/10], Step [117/250], Loss: 5.7911\n",
            "Epoch [4/10], Step [118/250], Loss: 4.2328\n",
            "Epoch [4/10], Step [119/250], Loss: 4.5899\n",
            "Epoch [4/10], Step [120/250], Loss: 3.9710\n",
            "Epoch [4/10], Step [121/250], Loss: 3.9823\n",
            "Epoch [4/10], Step [122/250], Loss: 3.4783\n",
            "Epoch [4/10], Step [123/250], Loss: 5.1239\n",
            "Epoch [4/10], Step [124/250], Loss: 3.6999\n",
            "Epoch [4/10], Step [125/250], Loss: 3.8021\n",
            "Epoch [4/10], Step [126/250], Loss: 4.3640\n",
            "Epoch [4/10], Step [127/250], Loss: 4.1219\n",
            "Epoch [4/10], Step [128/250], Loss: 4.6710\n",
            "Epoch [4/10], Step [129/250], Loss: 3.6948\n",
            "Epoch [4/10], Step [130/250], Loss: 5.5363\n",
            "Epoch [4/10], Step [131/250], Loss: 4.0668\n",
            "Epoch [4/10], Step [132/250], Loss: 5.1352\n",
            "Epoch [4/10], Step [133/250], Loss: 3.5861\n",
            "Epoch [4/10], Step [134/250], Loss: 3.9733\n",
            "Epoch [4/10], Step [135/250], Loss: 4.2069\n",
            "Epoch [4/10], Step [136/250], Loss: 3.7627\n",
            "Epoch [4/10], Step [137/250], Loss: 3.7887\n",
            "Epoch [4/10], Step [138/250], Loss: 4.0358\n",
            "Epoch [4/10], Step [139/250], Loss: 3.8097\n",
            "Epoch [4/10], Step [140/250], Loss: 4.1022\n",
            "Epoch [4/10], Step [141/250], Loss: 3.9033\n",
            "Epoch [4/10], Step [142/250], Loss: 3.8464\n",
            "Epoch [4/10], Step [143/250], Loss: 3.9386\n",
            "Epoch [4/10], Step [144/250], Loss: 5.0852\n",
            "Epoch [4/10], Step [145/250], Loss: 3.8156\n",
            "Epoch [4/10], Step [146/250], Loss: 3.5770\n",
            "Epoch [4/10], Step [147/250], Loss: 4.3021\n",
            "Epoch [4/10], Step [148/250], Loss: 4.0679\n",
            "Epoch [4/10], Step [149/250], Loss: 3.9138\n",
            "Epoch [4/10], Step [150/250], Loss: 3.9725\n",
            "Epoch [4/10], Step [151/250], Loss: 4.0297\n",
            "Epoch [4/10], Step [152/250], Loss: 3.7680\n",
            "Epoch [4/10], Step [153/250], Loss: 7.1029\n",
            "Epoch [4/10], Step [154/250], Loss: 3.9170\n",
            "Epoch [4/10], Step [155/250], Loss: 3.8868\n",
            "Epoch [4/10], Step [156/250], Loss: 3.7863\n",
            "Epoch [4/10], Step [157/250], Loss: 3.9269\n",
            "Epoch [4/10], Step [158/250], Loss: 3.8345\n",
            "Epoch [4/10], Step [159/250], Loss: 5.0867\n",
            "Epoch [4/10], Step [160/250], Loss: 3.8376\n",
            "Epoch [4/10], Step [161/250], Loss: 3.9623\n",
            "Epoch [4/10], Step [162/250], Loss: 4.0475\n",
            "Epoch [4/10], Step [163/250], Loss: 3.6937\n",
            "Epoch [4/10], Step [164/250], Loss: 4.0536\n",
            "Epoch [4/10], Step [165/250], Loss: 4.0687\n",
            "Epoch [4/10], Step [166/250], Loss: 5.1351\n",
            "Epoch [4/10], Step [167/250], Loss: 3.8421\n",
            "Epoch [4/10], Step [168/250], Loss: 3.9442\n",
            "Epoch [4/10], Step [169/250], Loss: 7.3764\n",
            "Epoch [4/10], Step [170/250], Loss: 4.9820\n",
            "Epoch [4/10], Step [171/250], Loss: 4.1904\n",
            "Epoch [4/10], Step [172/250], Loss: 3.9980\n",
            "Epoch [4/10], Step [173/250], Loss: 4.1993\n",
            "Epoch [4/10], Step [174/250], Loss: 4.0118\n",
            "Epoch [4/10], Step [175/250], Loss: 3.9839\n",
            "Epoch [4/10], Step [176/250], Loss: 3.9477\n",
            "Epoch [4/10], Step [177/250], Loss: 3.9238\n",
            "Epoch [4/10], Step [178/250], Loss: 4.0771\n",
            "Epoch [4/10], Step [179/250], Loss: 4.1565\n",
            "Epoch [4/10], Step [180/250], Loss: 4.0533\n",
            "Epoch [4/10], Step [181/250], Loss: 4.2906\n",
            "Epoch [4/10], Step [182/250], Loss: 4.3427\n",
            "Epoch [4/10], Step [183/250], Loss: 3.8455\n",
            "Epoch [4/10], Step [184/250], Loss: 4.0351\n",
            "Epoch [4/10], Step [185/250], Loss: 3.9112\n",
            "Epoch [4/10], Step [186/250], Loss: 3.3343\n",
            "Epoch [4/10], Step [187/250], Loss: 3.8810\n",
            "Epoch [4/10], Step [188/250], Loss: 5.2861\n",
            "Epoch [4/10], Step [189/250], Loss: 4.1322\n",
            "Epoch [4/10], Step [190/250], Loss: 3.9607\n",
            "Epoch [4/10], Step [191/250], Loss: 3.7335\n",
            "Epoch [4/10], Step [192/250], Loss: 4.5985\n",
            "Epoch [4/10], Step [193/250], Loss: 3.9390\n",
            "Epoch [4/10], Step [194/250], Loss: 3.9009\n",
            "Epoch [4/10], Step [195/250], Loss: 3.7458\n",
            "Epoch [4/10], Step [196/250], Loss: 3.8007\n",
            "Epoch [4/10], Step [197/250], Loss: 4.1983\n",
            "Epoch [4/10], Step [198/250], Loss: 3.7536\n",
            "Epoch [4/10], Step [199/250], Loss: 4.1134\n",
            "Epoch [4/10], Step [200/250], Loss: 3.7919\n",
            "Epoch [4/10], Step [201/250], Loss: 3.8255\n",
            "Epoch [4/10], Step [202/250], Loss: 4.0417\n",
            "Epoch [4/10], Step [203/250], Loss: 4.8119\n",
            "Epoch [4/10], Step [204/250], Loss: 3.7489\n",
            "Epoch [4/10], Step [205/250], Loss: 3.8631\n",
            "Epoch [4/10], Step [206/250], Loss: 3.9564\n",
            "Epoch [4/10], Step [207/250], Loss: 4.0998\n",
            "Epoch [4/10], Step [208/250], Loss: 3.9256\n",
            "Epoch [4/10], Step [209/250], Loss: 3.8793\n",
            "Epoch [4/10], Step [210/250], Loss: 4.3129\n",
            "Epoch [4/10], Step [211/250], Loss: 3.9778\n",
            "Epoch [4/10], Step [212/250], Loss: 3.9174\n",
            "Epoch [4/10], Step [213/250], Loss: 4.2157\n",
            "Epoch [4/10], Step [214/250], Loss: 3.9802\n",
            "Epoch [4/10], Step [215/250], Loss: 3.7427\n",
            "Epoch [4/10], Step [216/250], Loss: 3.7407\n",
            "Epoch [4/10], Step [217/250], Loss: 4.4760\n",
            "Epoch [4/10], Step [218/250], Loss: 3.6845\n",
            "Epoch [4/10], Step [219/250], Loss: 3.8589\n",
            "Epoch [4/10], Step [220/250], Loss: 4.5711\n",
            "Epoch [4/10], Step [221/250], Loss: 3.8160\n",
            "Epoch [4/10], Step [222/250], Loss: 3.9720\n",
            "Epoch [4/10], Step [223/250], Loss: 4.7472\n",
            "Epoch [4/10], Step [224/250], Loss: 3.7965\n",
            "Epoch [4/10], Step [225/250], Loss: 6.3704\n",
            "Epoch [4/10], Step [226/250], Loss: 4.2652\n",
            "Epoch [4/10], Step [227/250], Loss: 3.8502\n",
            "Epoch [4/10], Step [228/250], Loss: 4.4744\n",
            "Epoch [4/10], Step [229/250], Loss: 3.9113\n",
            "Epoch [4/10], Step [230/250], Loss: 4.4563\n",
            "Epoch [4/10], Step [231/250], Loss: 3.8409\n",
            "Epoch [4/10], Step [232/250], Loss: 4.0012\n",
            "Epoch [4/10], Step [233/250], Loss: 3.7852\n",
            "Epoch [4/10], Step [234/250], Loss: 3.8013\n",
            "Epoch [4/10], Step [235/250], Loss: 3.8743\n",
            "Epoch [4/10], Step [236/250], Loss: 6.2058\n",
            "Epoch [4/10], Step [237/250], Loss: 4.1242\n",
            "Epoch [4/10], Step [238/250], Loss: 3.9070\n",
            "Epoch [4/10], Step [239/250], Loss: 3.7283\n",
            "Epoch [4/10], Step [240/250], Loss: 4.4528\n",
            "Epoch [4/10], Step [241/250], Loss: 3.8263\n",
            "Epoch [4/10], Step [242/250], Loss: 3.7243\n",
            "Epoch [4/10], Step [243/250], Loss: 4.3230\n",
            "Epoch [4/10], Step [244/250], Loss: 3.5294\n",
            "Epoch [4/10], Step [245/250], Loss: 3.9158\n",
            "Epoch [4/10], Step [246/250], Loss: 4.0481\n",
            "Epoch [4/10], Step [247/250], Loss: 3.9311\n",
            "Epoch [4/10], Step [248/250], Loss: 4.0488\n",
            "Epoch [4/10], Step [249/250], Loss: 5.0229\n",
            "Epoch [4/10], Step [250/250], Loss: 3.8949\n",
            "Epoch 4: Avg Loss = 4.1454 Current Loss = 3.8949\n",
            "Epoch [5/10], Step [1/250], Loss: 4.5248\n",
            "Epoch [5/10], Step [2/250], Loss: 3.8125\n",
            "Epoch [5/10], Step [3/250], Loss: 3.9170\n",
            "Epoch [5/10], Step [4/250], Loss: 3.8261\n",
            "Epoch [5/10], Step [5/250], Loss: 4.1198\n",
            "Epoch [5/10], Step [6/250], Loss: 3.8107\n",
            "Epoch [5/10], Step [7/250], Loss: 3.5710\n",
            "Epoch [5/10], Step [8/250], Loss: 3.7556\n",
            "Epoch [5/10], Step [9/250], Loss: 3.8716\n",
            "Epoch [5/10], Step [10/250], Loss: 3.6784\n",
            "Epoch [5/10], Step [11/250], Loss: 3.9348\n",
            "Epoch [5/10], Step [12/250], Loss: 3.8148\n",
            "Epoch [5/10], Step [13/250], Loss: 3.7991\n",
            "Epoch [5/10], Step [14/250], Loss: 4.3701\n",
            "Epoch [5/10], Step [15/250], Loss: 4.6711\n",
            "Epoch [5/10], Step [16/250], Loss: 3.8058\n",
            "Epoch [5/10], Step [17/250], Loss: 4.4942\n",
            "Epoch [5/10], Step [18/250], Loss: 4.1933\n",
            "Epoch [5/10], Step [19/250], Loss: 3.9895\n",
            "Epoch [5/10], Step [20/250], Loss: 4.3343\n",
            "Epoch [5/10], Step [21/250], Loss: 4.1082\n",
            "Epoch [5/10], Step [22/250], Loss: 3.8603\n",
            "Epoch [5/10], Step [23/250], Loss: 4.0377\n",
            "Epoch [5/10], Step [24/250], Loss: 3.9493\n",
            "Epoch [5/10], Step [25/250], Loss: 4.0929\n",
            "Epoch [5/10], Step [26/250], Loss: 3.9899\n",
            "Epoch [5/10], Step [27/250], Loss: 3.8418\n",
            "Epoch [5/10], Step [28/250], Loss: 3.8518\n",
            "Epoch [5/10], Step [29/250], Loss: 4.7175\n",
            "Epoch [5/10], Step [30/250], Loss: 3.7498\n",
            "Epoch [5/10], Step [31/250], Loss: 3.9077\n",
            "Epoch [5/10], Step [32/250], Loss: 3.9646\n",
            "Epoch [5/10], Step [33/250], Loss: 4.0570\n",
            "Epoch [5/10], Step [34/250], Loss: 3.6161\n",
            "Epoch [5/10], Step [35/250], Loss: 4.0751\n",
            "Epoch [5/10], Step [36/250], Loss: 4.0217\n",
            "Epoch [5/10], Step [37/250], Loss: 4.1815\n",
            "Epoch [5/10], Step [38/250], Loss: 3.7069\n",
            "Epoch [5/10], Step [39/250], Loss: 4.2069\n",
            "Epoch [5/10], Step [40/250], Loss: 4.1681\n",
            "Epoch [5/10], Step [41/250], Loss: 3.9427\n",
            "Epoch [5/10], Step [42/250], Loss: 3.7630\n",
            "Epoch [5/10], Step [43/250], Loss: 5.9719\n",
            "Epoch [5/10], Step [44/250], Loss: 4.9893\n",
            "Epoch [5/10], Step [45/250], Loss: 4.0617\n",
            "Epoch [5/10], Step [46/250], Loss: 4.4106\n",
            "Epoch [5/10], Step [47/250], Loss: 4.6953\n",
            "Epoch [5/10], Step [48/250], Loss: 3.8475\n",
            "Epoch [5/10], Step [49/250], Loss: 4.0929\n",
            "Epoch [5/10], Step [50/250], Loss: 4.5403\n",
            "Epoch [5/10], Step [51/250], Loss: 3.8227\n",
            "Epoch [5/10], Step [52/250], Loss: 4.0655\n",
            "Epoch [5/10], Step [53/250], Loss: 4.5443\n",
            "Epoch [5/10], Step [54/250], Loss: 4.3424\n",
            "Epoch [5/10], Step [55/250], Loss: 4.2388\n",
            "Epoch [5/10], Step [56/250], Loss: 4.2822\n",
            "Epoch [5/10], Step [57/250], Loss: 3.7262\n",
            "Epoch [5/10], Step [58/250], Loss: 4.1607\n",
            "Epoch [5/10], Step [59/250], Loss: 5.0870\n",
            "Epoch [5/10], Step [60/250], Loss: 3.8645\n",
            "Epoch [5/10], Step [61/250], Loss: 3.8100\n",
            "Epoch [5/10], Step [62/250], Loss: 4.0374\n",
            "Epoch [5/10], Step [63/250], Loss: 3.8529\n",
            "Epoch [5/10], Step [64/250], Loss: 5.5283\n",
            "Epoch [5/10], Step [65/250], Loss: 4.4220\n",
            "Epoch [5/10], Step [66/250], Loss: 3.9653\n",
            "Epoch [5/10], Step [67/250], Loss: 3.8308\n",
            "Epoch [5/10], Step [68/250], Loss: 4.2049\n",
            "Epoch [5/10], Step [69/250], Loss: 4.4666\n",
            "Epoch [5/10], Step [70/250], Loss: 4.3683\n",
            "Epoch [5/10], Step [71/250], Loss: 3.8970\n",
            "Epoch [5/10], Step [72/250], Loss: 4.4387\n",
            "Epoch [5/10], Step [73/250], Loss: 4.1045\n",
            "Epoch [5/10], Step [74/250], Loss: 4.2299\n",
            "Epoch [5/10], Step [75/250], Loss: 4.2133\n",
            "Epoch [5/10], Step [76/250], Loss: 4.0326\n",
            "Epoch [5/10], Step [77/250], Loss: 3.7310\n",
            "Epoch [5/10], Step [78/250], Loss: 3.9402\n",
            "Epoch [5/10], Step [79/250], Loss: 3.6107\n",
            "Epoch [5/10], Step [80/250], Loss: 3.9414\n",
            "Epoch [5/10], Step [81/250], Loss: 3.6050\n",
            "Epoch [5/10], Step [82/250], Loss: 3.8213\n",
            "Epoch [5/10], Step [83/250], Loss: 4.4810\n",
            "Epoch [5/10], Step [84/250], Loss: 4.5826\n",
            "Epoch [5/10], Step [85/250], Loss: 4.3525\n",
            "Epoch [5/10], Step [86/250], Loss: 4.7895\n",
            "Epoch [5/10], Step [87/250], Loss: 4.7108\n",
            "Epoch [5/10], Step [88/250], Loss: 4.2780\n",
            "Epoch [5/10], Step [89/250], Loss: 4.3631\n",
            "Epoch [5/10], Step [90/250], Loss: 3.9009\n",
            "Epoch [5/10], Step [91/250], Loss: 3.9900\n",
            "Epoch [5/10], Step [92/250], Loss: 3.9575\n",
            "Epoch [5/10], Step [93/250], Loss: 4.1955\n",
            "Epoch [5/10], Step [94/250], Loss: 4.1746\n",
            "Epoch [5/10], Step [95/250], Loss: 4.0058\n",
            "Epoch [5/10], Step [96/250], Loss: 4.8624\n",
            "Epoch [5/10], Step [97/250], Loss: 3.9320\n",
            "Epoch [5/10], Step [98/250], Loss: 4.1629\n",
            "Epoch [5/10], Step [99/250], Loss: 3.9865\n",
            "Epoch [5/10], Step [100/250], Loss: 3.8721\n",
            "Epoch [5/10], Step [101/250], Loss: 3.8012\n",
            "Epoch [5/10], Step [102/250], Loss: 3.8125\n",
            "Epoch [5/10], Step [103/250], Loss: 4.0756\n",
            "Epoch [5/10], Step [104/250], Loss: 4.2157\n",
            "Epoch [5/10], Step [105/250], Loss: 4.0017\n",
            "Epoch [5/10], Step [106/250], Loss: 4.0838\n",
            "Epoch [5/10], Step [107/250], Loss: 3.7512\n",
            "Epoch [5/10], Step [108/250], Loss: 3.6195\n",
            "Epoch [5/10], Step [109/250], Loss: 3.7976\n",
            "Epoch [5/10], Step [110/250], Loss: 3.7908\n",
            "Epoch [5/10], Step [111/250], Loss: 3.7839\n",
            "Epoch [5/10], Step [112/250], Loss: 4.4618\n",
            "Epoch [5/10], Step [113/250], Loss: 4.0148\n",
            "Epoch [5/10], Step [114/250], Loss: 3.8172\n",
            "Epoch [5/10], Step [115/250], Loss: 4.1307\n",
            "Epoch [5/10], Step [116/250], Loss: 3.9566\n",
            "Epoch [5/10], Step [117/250], Loss: 5.9578\n",
            "Epoch [5/10], Step [118/250], Loss: 4.0328\n",
            "Epoch [5/10], Step [119/250], Loss: 3.9853\n",
            "Epoch [5/10], Step [120/250], Loss: 3.9712\n",
            "Epoch [5/10], Step [121/250], Loss: 5.4640\n",
            "Epoch [5/10], Step [122/250], Loss: 3.8425\n",
            "Epoch [5/10], Step [123/250], Loss: 3.8573\n",
            "Epoch [5/10], Step [124/250], Loss: 4.1248\n",
            "Epoch [5/10], Step [125/250], Loss: 3.7121\n",
            "Epoch [5/10], Step [126/250], Loss: 3.8880\n",
            "Epoch [5/10], Step [127/250], Loss: 5.1658\n",
            "Epoch [5/10], Step [128/250], Loss: 4.0045\n",
            "Epoch [5/10], Step [129/250], Loss: 3.8880\n",
            "Epoch [5/10], Step [130/250], Loss: 3.9138\n",
            "Epoch [5/10], Step [131/250], Loss: 4.1636\n",
            "Epoch [5/10], Step [132/250], Loss: 4.2052\n",
            "Epoch [5/10], Step [133/250], Loss: 3.8371\n",
            "Epoch [5/10], Step [134/250], Loss: 3.3152\n",
            "Epoch [5/10], Step [135/250], Loss: 3.8868\n",
            "Epoch [5/10], Step [136/250], Loss: 4.5233\n",
            "Epoch [5/10], Step [137/250], Loss: 4.4249\n",
            "Epoch [5/10], Step [138/250], Loss: 5.4278\n",
            "Epoch [5/10], Step [139/250], Loss: 4.0779\n",
            "Epoch [5/10], Step [140/250], Loss: 4.1894\n",
            "Epoch [5/10], Step [141/250], Loss: 4.2903\n",
            "Epoch [5/10], Step [142/250], Loss: 3.9418\n",
            "Epoch [5/10], Step [143/250], Loss: 3.9557\n",
            "Epoch [5/10], Step [144/250], Loss: 3.6457\n",
            "Epoch [5/10], Step [145/250], Loss: 3.9171\n",
            "Epoch [5/10], Step [146/250], Loss: 4.4423\n",
            "Epoch [5/10], Step [147/250], Loss: 5.4754\n",
            "Epoch [5/10], Step [148/250], Loss: 4.1499\n",
            "Epoch [5/10], Step [149/250], Loss: 4.3618\n",
            "Epoch [5/10], Step [150/250], Loss: 4.6391\n",
            "Epoch [5/10], Step [151/250], Loss: 3.8526\n",
            "Epoch [5/10], Step [152/250], Loss: 3.7836\n",
            "Epoch [5/10], Step [153/250], Loss: 3.8682\n",
            "Epoch [5/10], Step [154/250], Loss: 4.0639\n",
            "Epoch [5/10], Step [155/250], Loss: 3.7611\n",
            "Epoch [5/10], Step [156/250], Loss: 4.0570\n",
            "Epoch [5/10], Step [157/250], Loss: 3.9719\n",
            "Epoch [5/10], Step [158/250], Loss: 3.8911\n",
            "Epoch [5/10], Step [159/250], Loss: 3.7592\n",
            "Epoch [5/10], Step [160/250], Loss: 4.0724\n",
            "Epoch [5/10], Step [161/250], Loss: 3.9142\n",
            "Epoch [5/10], Step [162/250], Loss: 3.9814\n",
            "Epoch [5/10], Step [163/250], Loss: 4.9273\n",
            "Epoch [5/10], Step [164/250], Loss: 5.9305\n",
            "Epoch [5/10], Step [165/250], Loss: 3.8299\n",
            "Epoch [5/10], Step [166/250], Loss: 4.4751\n",
            "Epoch [5/10], Step [167/250], Loss: 3.2797\n",
            "Epoch [5/10], Step [168/250], Loss: 4.1461\n",
            "Epoch [5/10], Step [169/250], Loss: 4.2077\n",
            "Epoch [5/10], Step [170/250], Loss: 3.9949\n",
            "Epoch [5/10], Step [171/250], Loss: 4.1111\n",
            "Epoch [5/10], Step [172/250], Loss: 3.9816\n",
            "Epoch [5/10], Step [173/250], Loss: 4.0843\n",
            "Epoch [5/10], Step [174/250], Loss: 4.1519\n",
            "Epoch [5/10], Step [175/250], Loss: 3.7125\n",
            "Epoch [5/10], Step [176/250], Loss: 3.9042\n",
            "Epoch [5/10], Step [177/250], Loss: 3.8795\n",
            "Epoch [5/10], Step [178/250], Loss: 3.9166\n",
            "Epoch [5/10], Step [179/250], Loss: 4.2268\n",
            "Epoch [5/10], Step [180/250], Loss: 3.8743\n",
            "Epoch [5/10], Step [181/250], Loss: 4.7488\n",
            "Epoch [5/10], Step [182/250], Loss: 3.9046\n",
            "Epoch [5/10], Step [183/250], Loss: 3.9687\n",
            "Epoch [5/10], Step [184/250], Loss: 4.0786\n",
            "Epoch [5/10], Step [185/250], Loss: 4.3098\n",
            "Epoch [5/10], Step [186/250], Loss: 3.9191\n",
            "Epoch [5/10], Step [187/250], Loss: 4.3566\n",
            "Epoch [5/10], Step [188/250], Loss: 4.2100\n",
            "Epoch [5/10], Step [189/250], Loss: 3.8200\n",
            "Epoch [5/10], Step [190/250], Loss: 3.8691\n",
            "Epoch [5/10], Step [191/250], Loss: 4.0764\n",
            "Epoch [5/10], Step [192/250], Loss: 4.0190\n",
            "Epoch [5/10], Step [193/250], Loss: 4.0779\n",
            "Epoch [5/10], Step [194/250], Loss: 4.0641\n",
            "Epoch [5/10], Step [195/250], Loss: 3.9632\n",
            "Epoch [5/10], Step [196/250], Loss: 4.0241\n",
            "Epoch [5/10], Step [197/250], Loss: 4.0080\n",
            "Epoch [5/10], Step [198/250], Loss: 4.9159\n",
            "Epoch [5/10], Step [199/250], Loss: 4.0369\n",
            "Epoch [5/10], Step [200/250], Loss: 3.8422\n",
            "Epoch [5/10], Step [201/250], Loss: 5.9574\n",
            "Epoch [5/10], Step [202/250], Loss: 6.8339\n",
            "Epoch [5/10], Step [203/250], Loss: 4.0483\n",
            "Epoch [5/10], Step [204/250], Loss: 3.8704\n",
            "Epoch [5/10], Step [205/250], Loss: 4.0074\n",
            "Epoch [5/10], Step [206/250], Loss: 3.9217\n",
            "Epoch [5/10], Step [207/250], Loss: 3.5984\n",
            "Epoch [5/10], Step [208/250], Loss: 3.8363\n",
            "Epoch [5/10], Step [209/250], Loss: 3.4993\n",
            "Epoch [5/10], Step [210/250], Loss: 3.9726\n",
            "Epoch [5/10], Step [211/250], Loss: 3.8715\n",
            "Epoch [5/10], Step [212/250], Loss: 4.1305\n",
            "Epoch [5/10], Step [213/250], Loss: 4.3271\n",
            "Epoch [5/10], Step [214/250], Loss: 3.8163\n",
            "Epoch [5/10], Step [215/250], Loss: 4.0494\n",
            "Epoch [5/10], Step [216/250], Loss: 4.0473\n",
            "Epoch [5/10], Step [217/250], Loss: 3.5027\n",
            "Epoch [5/10], Step [218/250], Loss: 3.9523\n",
            "Epoch [5/10], Step [219/250], Loss: 3.7936\n",
            "Epoch [5/10], Step [220/250], Loss: 4.1302\n",
            "Epoch [5/10], Step [221/250], Loss: 3.9381\n",
            "Epoch [5/10], Step [222/250], Loss: 3.8402\n",
            "Epoch [5/10], Step [223/250], Loss: 4.9711\n",
            "Epoch [5/10], Step [224/250], Loss: 3.8870\n",
            "Epoch [5/10], Step [225/250], Loss: 3.9540\n",
            "Epoch [5/10], Step [226/250], Loss: 3.7062\n",
            "Epoch [5/10], Step [227/250], Loss: 4.3344\n",
            "Epoch [5/10], Step [228/250], Loss: 5.0631\n",
            "Epoch [5/10], Step [229/250], Loss: 4.4901\n",
            "Epoch [5/10], Step [230/250], Loss: 4.0483\n",
            "Epoch [5/10], Step [231/250], Loss: 4.0599\n",
            "Epoch [5/10], Step [232/250], Loss: 4.0361\n",
            "Epoch [5/10], Step [233/250], Loss: 4.1062\n",
            "Epoch [5/10], Step [234/250], Loss: 3.8875\n",
            "Epoch [5/10], Step [235/250], Loss: 3.8537\n",
            "Epoch [5/10], Step [236/250], Loss: 4.3993\n",
            "Epoch [5/10], Step [237/250], Loss: 4.1525\n",
            "Epoch [5/10], Step [238/250], Loss: 4.5951\n",
            "Epoch [5/10], Step [239/250], Loss: 4.0358\n",
            "Epoch [5/10], Step [240/250], Loss: 4.0392\n",
            "Epoch [5/10], Step [241/250], Loss: 4.1346\n",
            "Epoch [5/10], Step [242/250], Loss: 4.0480\n",
            "Epoch [5/10], Step [243/250], Loss: 6.5100\n",
            "Epoch [5/10], Step [244/250], Loss: 4.1119\n",
            "Epoch [5/10], Step [245/250], Loss: 3.8674\n",
            "Epoch [5/10], Step [246/250], Loss: 3.9514\n",
            "Epoch [5/10], Step [247/250], Loss: 4.1094\n",
            "Epoch [5/10], Step [248/250], Loss: 3.9551\n",
            "Epoch [5/10], Step [249/250], Loss: 4.0802\n",
            "Epoch [5/10], Step [250/250], Loss: 4.0248\n",
            "Epoch 5: Avg Loss = 4.1379 Current Loss = 4.0248\n",
            "Epoch [6/10], Step [1/250], Loss: 4.0754\n",
            "Epoch [6/10], Step [2/250], Loss: 4.0110\n",
            "Epoch [6/10], Step [3/250], Loss: 4.0477\n",
            "Epoch [6/10], Step [4/250], Loss: 3.9389\n",
            "Epoch [6/10], Step [5/250], Loss: 3.7465\n",
            "Epoch [6/10], Step [6/250], Loss: 3.9422\n",
            "Epoch [6/10], Step [7/250], Loss: 3.9159\n",
            "Epoch [6/10], Step [8/250], Loss: 4.1423\n",
            "Epoch [6/10], Step [9/250], Loss: 4.0822\n",
            "Epoch [6/10], Step [10/250], Loss: 4.3080\n",
            "Epoch [6/10], Step [11/250], Loss: 3.8784\n",
            "Epoch [6/10], Step [12/250], Loss: 3.9797\n",
            "Epoch [6/10], Step [13/250], Loss: 3.9460\n",
            "Epoch [6/10], Step [14/250], Loss: 4.0256\n",
            "Epoch [6/10], Step [15/250], Loss: 3.9567\n",
            "Epoch [6/10], Step [16/250], Loss: 3.7526\n",
            "Epoch [6/10], Step [17/250], Loss: 3.8998\n",
            "Epoch [6/10], Step [18/250], Loss: 3.8561\n",
            "Epoch [6/10], Step [19/250], Loss: 4.0593\n",
            "Epoch [6/10], Step [20/250], Loss: 4.1033\n",
            "Epoch [6/10], Step [21/250], Loss: 4.0074\n",
            "Epoch [6/10], Step [22/250], Loss: 7.3943\n",
            "Epoch [6/10], Step [23/250], Loss: 4.2869\n",
            "Epoch [6/10], Step [24/250], Loss: 3.9644\n",
            "Epoch [6/10], Step [25/250], Loss: 4.0010\n",
            "Epoch [6/10], Step [26/250], Loss: 4.1514\n",
            "Epoch [6/10], Step [27/250], Loss: 4.3018\n",
            "Epoch [6/10], Step [28/250], Loss: 3.9460\n",
            "Epoch [6/10], Step [29/250], Loss: 3.8779\n",
            "Epoch [6/10], Step [30/250], Loss: 3.8859\n",
            "Epoch [6/10], Step [31/250], Loss: 3.9528\n",
            "Epoch [6/10], Step [32/250], Loss: 4.0275\n",
            "Epoch [6/10], Step [33/250], Loss: 3.7700\n",
            "Epoch [6/10], Step [34/250], Loss: 3.9088\n",
            "Epoch [6/10], Step [35/250], Loss: 3.8721\n",
            "Epoch [6/10], Step [36/250], Loss: 4.4535\n",
            "Epoch [6/10], Step [37/250], Loss: 4.0075\n",
            "Epoch [6/10], Step [38/250], Loss: 4.8014\n",
            "Epoch [6/10], Step [39/250], Loss: 3.5360\n",
            "Epoch [6/10], Step [40/250], Loss: 3.8284\n",
            "Epoch [6/10], Step [41/250], Loss: 4.5233\n",
            "Epoch [6/10], Step [42/250], Loss: 3.9435\n",
            "Epoch [6/10], Step [43/250], Loss: 4.0645\n",
            "Epoch [6/10], Step [44/250], Loss: 4.9204\n",
            "Epoch [6/10], Step [45/250], Loss: 4.8009\n",
            "Epoch [6/10], Step [46/250], Loss: 4.1255\n",
            "Epoch [6/10], Step [47/250], Loss: 3.8191\n",
            "Epoch [6/10], Step [48/250], Loss: 3.6476\n",
            "Epoch [6/10], Step [49/250], Loss: 3.9771\n",
            "Epoch [6/10], Step [50/250], Loss: 4.1484\n",
            "Epoch [6/10], Step [51/250], Loss: 3.9305\n",
            "Epoch [6/10], Step [52/250], Loss: 4.0110\n",
            "Epoch [6/10], Step [53/250], Loss: 3.8598\n",
            "Epoch [6/10], Step [54/250], Loss: 4.5995\n",
            "Epoch [6/10], Step [55/250], Loss: 4.1706\n",
            "Epoch [6/10], Step [56/250], Loss: 3.7891\n",
            "Epoch [6/10], Step [57/250], Loss: 4.1940\n",
            "Epoch [6/10], Step [58/250], Loss: 3.9486\n",
            "Epoch [6/10], Step [59/250], Loss: 4.0632\n",
            "Epoch [6/10], Step [60/250], Loss: 3.7212\n",
            "Epoch [6/10], Step [61/250], Loss: 4.7921\n",
            "Epoch [6/10], Step [62/250], Loss: 4.1838\n",
            "Epoch [6/10], Step [63/250], Loss: 3.7235\n",
            "Epoch [6/10], Step [64/250], Loss: 4.0738\n",
            "Epoch [6/10], Step [65/250], Loss: 4.0744\n",
            "Epoch [6/10], Step [66/250], Loss: 3.8427\n",
            "Epoch [6/10], Step [67/250], Loss: 3.9413\n",
            "Epoch [6/10], Step [68/250], Loss: 3.8145\n",
            "Epoch [6/10], Step [69/250], Loss: 3.7373\n",
            "Epoch [6/10], Step [70/250], Loss: 3.6963\n",
            "Epoch [6/10], Step [71/250], Loss: 3.6830\n",
            "Epoch [6/10], Step [72/250], Loss: 4.5400\n",
            "Epoch [6/10], Step [73/250], Loss: 3.7521\n",
            "Epoch [6/10], Step [74/250], Loss: 3.9661\n",
            "Epoch [6/10], Step [75/250], Loss: 3.8899\n",
            "Epoch [6/10], Step [76/250], Loss: 3.6607\n",
            "Epoch [6/10], Step [77/250], Loss: 4.4794\n",
            "Epoch [6/10], Step [78/250], Loss: 7.1703\n",
            "Epoch [6/10], Step [79/250], Loss: 3.3012\n",
            "Epoch [6/10], Step [80/250], Loss: 5.5467\n",
            "Epoch [6/10], Step [81/250], Loss: 3.8423\n",
            "Epoch [6/10], Step [82/250], Loss: 4.0789\n",
            "Epoch [6/10], Step [83/250], Loss: 4.0840\n",
            "Epoch [6/10], Step [84/250], Loss: 3.9276\n",
            "Epoch [6/10], Step [85/250], Loss: 3.8322\n",
            "Epoch [6/10], Step [86/250], Loss: 3.7877\n",
            "Epoch [6/10], Step [87/250], Loss: 3.8282\n",
            "Epoch [6/10], Step [88/250], Loss: 3.6623\n",
            "Epoch [6/10], Step [89/250], Loss: 4.1371\n",
            "Epoch [6/10], Step [90/250], Loss: 5.8532\n",
            "Epoch [6/10], Step [91/250], Loss: 7.8738\n",
            "Epoch [6/10], Step [92/250], Loss: 4.2465\n",
            "Epoch [6/10], Step [93/250], Loss: 3.7251\n",
            "Epoch [6/10], Step [94/250], Loss: 3.7312\n",
            "Epoch [6/10], Step [95/250], Loss: 3.8837\n",
            "Epoch [6/10], Step [96/250], Loss: 3.7903\n",
            "Epoch [6/10], Step [97/250], Loss: 3.8647\n",
            "Epoch [6/10], Step [98/250], Loss: 3.9494\n",
            "Epoch [6/10], Step [99/250], Loss: 3.4996\n",
            "Epoch [6/10], Step [100/250], Loss: 3.9445\n",
            "Epoch [6/10], Step [101/250], Loss: 3.7061\n",
            "Epoch [6/10], Step [102/250], Loss: 4.0258\n",
            "Epoch [6/10], Step [103/250], Loss: 3.7272\n",
            "Epoch [6/10], Step [104/250], Loss: 4.4625\n",
            "Epoch [6/10], Step [105/250], Loss: 4.1604\n",
            "Epoch [6/10], Step [106/250], Loss: 4.8278\n",
            "Epoch [6/10], Step [107/250], Loss: 5.0298\n",
            "Epoch [6/10], Step [108/250], Loss: 3.7684\n",
            "Epoch [6/10], Step [109/250], Loss: 3.7566\n",
            "Epoch [6/10], Step [110/250], Loss: 3.6770\n",
            "Epoch [6/10], Step [111/250], Loss: 3.5974\n",
            "Epoch [6/10], Step [112/250], Loss: 4.1859\n",
            "Epoch [6/10], Step [113/250], Loss: 3.9220\n",
            "Epoch [6/10], Step [114/250], Loss: 3.8836\n",
            "Epoch [6/10], Step [115/250], Loss: 3.7561\n",
            "Epoch [6/10], Step [116/250], Loss: 3.8128\n",
            "Epoch [6/10], Step [117/250], Loss: 5.5274\n",
            "Epoch [6/10], Step [118/250], Loss: 4.2630\n",
            "Epoch [6/10], Step [119/250], Loss: 4.3787\n",
            "Epoch [6/10], Step [120/250], Loss: 3.9628\n",
            "Epoch [6/10], Step [121/250], Loss: 4.0402\n",
            "Epoch [6/10], Step [122/250], Loss: 4.2644\n",
            "Epoch [6/10], Step [123/250], Loss: 3.9105\n",
            "Epoch [6/10], Step [124/250], Loss: 4.2094\n",
            "Epoch [6/10], Step [125/250], Loss: 3.9142\n",
            "Epoch [6/10], Step [126/250], Loss: 3.9533\n",
            "Epoch [6/10], Step [127/250], Loss: 4.3395\n",
            "Epoch [6/10], Step [128/250], Loss: 6.8179\n",
            "Epoch [6/10], Step [129/250], Loss: 4.1044\n",
            "Epoch [6/10], Step [130/250], Loss: 4.2249\n",
            "Epoch [6/10], Step [131/250], Loss: 3.3697\n",
            "Epoch [6/10], Step [132/250], Loss: 3.9552\n",
            "Epoch [6/10], Step [133/250], Loss: 5.1420\n",
            "Epoch [6/10], Step [134/250], Loss: 4.1129\n",
            "Epoch [6/10], Step [135/250], Loss: 3.9642\n",
            "Epoch [6/10], Step [136/250], Loss: 3.8775\n",
            "Epoch [6/10], Step [137/250], Loss: 3.9361\n",
            "Epoch [6/10], Step [138/250], Loss: 4.0410\n",
            "Epoch [6/10], Step [139/250], Loss: 3.9717\n",
            "Epoch [6/10], Step [140/250], Loss: 3.6915\n",
            "Epoch [6/10], Step [141/250], Loss: 4.4694\n",
            "Epoch [6/10], Step [142/250], Loss: 4.7527\n",
            "Epoch [6/10], Step [143/250], Loss: 4.0876\n",
            "Epoch [6/10], Step [144/250], Loss: 4.0258\n",
            "Epoch [6/10], Step [145/250], Loss: 4.2142\n",
            "Epoch [6/10], Step [146/250], Loss: 4.1283\n",
            "Epoch [6/10], Step [147/250], Loss: 3.8895\n",
            "Epoch [6/10], Step [148/250], Loss: 3.8230\n",
            "Epoch [6/10], Step [149/250], Loss: 3.9209\n",
            "Epoch [6/10], Step [150/250], Loss: 3.8735\n",
            "Epoch [6/10], Step [151/250], Loss: 4.1527\n",
            "Epoch [6/10], Step [152/250], Loss: 4.0689\n",
            "Epoch [6/10], Step [153/250], Loss: 8.0104\n",
            "Epoch [6/10], Step [154/250], Loss: 4.4182\n",
            "Epoch [6/10], Step [155/250], Loss: 3.8640\n",
            "Epoch [6/10], Step [156/250], Loss: 4.9928\n",
            "Epoch [6/10], Step [157/250], Loss: 3.6180\n",
            "Epoch [6/10], Step [158/250], Loss: 3.8810\n",
            "Epoch [6/10], Step [159/250], Loss: 4.0972\n",
            "Epoch [6/10], Step [160/250], Loss: 4.0208\n",
            "Epoch [6/10], Step [161/250], Loss: 3.6021\n",
            "Epoch [6/10], Step [162/250], Loss: 3.7771\n",
            "Epoch [6/10], Step [163/250], Loss: 3.7657\n",
            "Epoch [6/10], Step [164/250], Loss: 3.8468\n",
            "Epoch [6/10], Step [165/250], Loss: 4.6956\n",
            "Epoch [6/10], Step [166/250], Loss: 4.1890\n",
            "Epoch [6/10], Step [167/250], Loss: 3.9347\n",
            "Epoch [6/10], Step [168/250], Loss: 3.7123\n",
            "Epoch [6/10], Step [169/250], Loss: 4.0945\n",
            "Epoch [6/10], Step [170/250], Loss: 4.0153\n",
            "Epoch [6/10], Step [171/250], Loss: 4.0290\n",
            "Epoch [6/10], Step [172/250], Loss: 4.4925\n",
            "Epoch [6/10], Step [173/250], Loss: 4.0490\n",
            "Epoch [6/10], Step [174/250], Loss: 4.0793\n",
            "Epoch [6/10], Step [175/250], Loss: 3.6901\n",
            "Epoch [6/10], Step [176/250], Loss: 3.8970\n",
            "Epoch [6/10], Step [177/250], Loss: 3.5924\n",
            "Epoch [6/10], Step [178/250], Loss: 3.1739\n",
            "Epoch [6/10], Step [179/250], Loss: 4.1120\n",
            "Epoch [6/10], Step [180/250], Loss: 4.2771\n",
            "Epoch [6/10], Step [181/250], Loss: 3.8471\n",
            "Epoch [6/10], Step [182/250], Loss: 4.2759\n",
            "Epoch [6/10], Step [183/250], Loss: 4.2855\n",
            "Epoch [6/10], Step [184/250], Loss: 3.8991\n",
            "Epoch [6/10], Step [185/250], Loss: 3.5665\n",
            "Epoch [6/10], Step [186/250], Loss: 3.8462\n",
            "Epoch [6/10], Step [187/250], Loss: 3.8062\n",
            "Epoch [6/10], Step [188/250], Loss: 3.6137\n",
            "Epoch [6/10], Step [189/250], Loss: 3.8911\n",
            "Epoch [6/10], Step [190/250], Loss: 4.0099\n",
            "Epoch [6/10], Step [191/250], Loss: 3.8235\n",
            "Epoch [6/10], Step [192/250], Loss: 3.9255\n",
            "Epoch [6/10], Step [193/250], Loss: 3.8893\n",
            "Epoch [6/10], Step [194/250], Loss: 3.7658\n",
            "Epoch [6/10], Step [195/250], Loss: 3.9538\n",
            "Epoch [6/10], Step [196/250], Loss: 3.6728\n",
            "Epoch [6/10], Step [197/250], Loss: 4.1651\n",
            "Epoch [6/10], Step [198/250], Loss: 3.8627\n",
            "Epoch [6/10], Step [199/250], Loss: 4.0004\n",
            "Epoch [6/10], Step [200/250], Loss: 4.3213\n",
            "Epoch [6/10], Step [201/250], Loss: 4.1673\n",
            "Epoch [6/10], Step [202/250], Loss: 4.2862\n",
            "Epoch [6/10], Step [203/250], Loss: 4.5101\n",
            "Epoch [6/10], Step [204/250], Loss: 4.1854\n",
            "Epoch [6/10], Step [205/250], Loss: 3.9559\n",
            "Epoch [6/10], Step [206/250], Loss: 4.1755\n",
            "Epoch [6/10], Step [207/250], Loss: 4.0074\n",
            "Epoch [6/10], Step [208/250], Loss: 3.8882\n",
            "Epoch [6/10], Step [209/250], Loss: 4.0409\n",
            "Epoch [6/10], Step [210/250], Loss: 7.1960\n",
            "Epoch [6/10], Step [211/250], Loss: 3.8409\n",
            "Epoch [6/10], Step [212/250], Loss: 4.3682\n",
            "Epoch [6/10], Step [213/250], Loss: 3.8763\n",
            "Epoch [6/10], Step [214/250], Loss: 3.9036\n",
            "Epoch [6/10], Step [215/250], Loss: 3.8712\n",
            "Epoch [6/10], Step [216/250], Loss: 3.7988\n",
            "Epoch [6/10], Step [217/250], Loss: 4.0105\n",
            "Epoch [6/10], Step [218/250], Loss: 3.8292\n",
            "Epoch [6/10], Step [219/250], Loss: 3.8674\n",
            "Epoch [6/10], Step [220/250], Loss: 4.4384\n",
            "Epoch [6/10], Step [221/250], Loss: 3.8278\n",
            "Epoch [6/10], Step [222/250], Loss: 3.8588\n",
            "Epoch [6/10], Step [223/250], Loss: 3.7525\n",
            "Epoch [6/10], Step [224/250], Loss: 4.0042\n",
            "Epoch [6/10], Step [225/250], Loss: 3.8973\n",
            "Epoch [6/10], Step [226/250], Loss: 4.0074\n",
            "Epoch [6/10], Step [227/250], Loss: 4.0514\n",
            "Epoch [6/10], Step [228/250], Loss: 3.8392\n",
            "Epoch [6/10], Step [229/250], Loss: 3.9965\n",
            "Epoch [6/10], Step [230/250], Loss: 4.0320\n",
            "Epoch [6/10], Step [231/250], Loss: 3.8841\n",
            "Epoch [6/10], Step [232/250], Loss: 3.8809\n",
            "Epoch [6/10], Step [233/250], Loss: 5.6183\n",
            "Epoch [6/10], Step [234/250], Loss: 4.1228\n",
            "Epoch [6/10], Step [235/250], Loss: 3.8832\n",
            "Epoch [6/10], Step [236/250], Loss: 4.8734\n",
            "Epoch [6/10], Step [237/250], Loss: 4.0521\n",
            "Epoch [6/10], Step [238/250], Loss: 4.1726\n",
            "Epoch [6/10], Step [239/250], Loss: 4.1120\n",
            "Epoch [6/10], Step [240/250], Loss: 3.8637\n",
            "Epoch [6/10], Step [241/250], Loss: 4.0292\n",
            "Epoch [6/10], Step [242/250], Loss: 3.9056\n",
            "Epoch [6/10], Step [243/250], Loss: 4.8774\n",
            "Epoch [6/10], Step [244/250], Loss: 4.0021\n",
            "Epoch [6/10], Step [245/250], Loss: 4.1405\n",
            "Epoch [6/10], Step [246/250], Loss: 3.9893\n",
            "Epoch [6/10], Step [247/250], Loss: 3.8772\n",
            "Epoch [6/10], Step [248/250], Loss: 4.0961\n",
            "Epoch [6/10], Step [249/250], Loss: 3.9926\n",
            "Epoch [6/10], Step [250/250], Loss: 3.6340\n",
            "Epoch 6: Avg Loss = 4.1193 Current Loss = 3.6340\n",
            "Epoch [7/10], Step [1/250], Loss: 3.8853\n",
            "Epoch [7/10], Step [2/250], Loss: 4.0980\n",
            "Epoch [7/10], Step [3/250], Loss: 3.8654\n",
            "Epoch [7/10], Step [4/250], Loss: 3.8891\n",
            "Epoch [7/10], Step [5/250], Loss: 3.8200\n",
            "Epoch [7/10], Step [6/250], Loss: 4.3379\n",
            "Epoch [7/10], Step [7/250], Loss: 3.7905\n",
            "Epoch [7/10], Step [8/250], Loss: 3.7691\n",
            "Epoch [7/10], Step [9/250], Loss: 3.5152\n",
            "Epoch [7/10], Step [10/250], Loss: 3.8866\n",
            "Epoch [7/10], Step [11/250], Loss: 3.8366\n",
            "Epoch [7/10], Step [12/250], Loss: 4.5600\n",
            "Epoch [7/10], Step [13/250], Loss: 6.5607\n",
            "Epoch [7/10], Step [14/250], Loss: 4.0735\n",
            "Epoch [7/10], Step [15/250], Loss: 4.3186\n",
            "Epoch [7/10], Step [16/250], Loss: 3.1590\n",
            "Epoch [7/10], Step [17/250], Loss: 4.0144\n",
            "Epoch [7/10], Step [18/250], Loss: 3.5103\n",
            "Epoch [7/10], Step [19/250], Loss: 4.8069\n",
            "Epoch [7/10], Step [20/250], Loss: 3.8891\n",
            "Epoch [7/10], Step [21/250], Loss: 4.3038\n",
            "Epoch [7/10], Step [22/250], Loss: 4.3154\n",
            "Epoch [7/10], Step [23/250], Loss: 4.7789\n",
            "Epoch [7/10], Step [24/250], Loss: 3.7734\n",
            "Epoch [7/10], Step [25/250], Loss: 3.8698\n",
            "Epoch [7/10], Step [26/250], Loss: 3.8737\n",
            "Epoch [7/10], Step [27/250], Loss: 4.2899\n",
            "Epoch [7/10], Step [28/250], Loss: 3.9107\n",
            "Epoch [7/10], Step [29/250], Loss: 4.1393\n",
            "Epoch [7/10], Step [30/250], Loss: 4.1633\n",
            "Epoch [7/10], Step [31/250], Loss: 4.0810\n",
            "Epoch [7/10], Step [32/250], Loss: 3.9147\n",
            "Epoch [7/10], Step [33/250], Loss: 3.9914\n",
            "Epoch [7/10], Step [34/250], Loss: 3.8830\n",
            "Epoch [7/10], Step [35/250], Loss: 4.4821\n",
            "Epoch [7/10], Step [36/250], Loss: 4.4855\n",
            "Epoch [7/10], Step [37/250], Loss: 4.1419\n",
            "Epoch [7/10], Step [38/250], Loss: 3.5653\n",
            "Epoch [7/10], Step [39/250], Loss: 5.6589\n",
            "Epoch [7/10], Step [40/250], Loss: 4.5726\n",
            "Epoch [7/10], Step [41/250], Loss: 4.0591\n",
            "Epoch [7/10], Step [42/250], Loss: 3.8478\n",
            "Epoch [7/10], Step [43/250], Loss: 4.1069\n",
            "Epoch [7/10], Step [44/250], Loss: 3.9289\n",
            "Epoch [7/10], Step [45/250], Loss: 3.9598\n",
            "Epoch [7/10], Step [46/250], Loss: 4.0114\n",
            "Epoch [7/10], Step [47/250], Loss: 4.9889\n",
            "Epoch [7/10], Step [48/250], Loss: 3.9545\n",
            "Epoch [7/10], Step [49/250], Loss: 4.3061\n",
            "Epoch [7/10], Step [50/250], Loss: 4.2716\n",
            "Epoch [7/10], Step [51/250], Loss: 3.7870\n",
            "Epoch [7/10], Step [52/250], Loss: 3.7321\n",
            "Epoch [7/10], Step [53/250], Loss: 4.0198\n",
            "Epoch [7/10], Step [54/250], Loss: 3.9336\n",
            "Epoch [7/10], Step [55/250], Loss: 3.8405\n",
            "Epoch [7/10], Step [56/250], Loss: 3.9621\n",
            "Epoch [7/10], Step [57/250], Loss: 4.0409\n",
            "Epoch [7/10], Step [58/250], Loss: 4.9180\n",
            "Epoch [7/10], Step [59/250], Loss: 4.0825\n",
            "Epoch [7/10], Step [60/250], Loss: 3.7362\n",
            "Epoch [7/10], Step [61/250], Loss: 4.6454\n",
            "Epoch [7/10], Step [62/250], Loss: 3.7257\n",
            "Epoch [7/10], Step [63/250], Loss: 3.8458\n",
            "Epoch [7/10], Step [64/250], Loss: 3.9321\n",
            "Epoch [7/10], Step [65/250], Loss: 4.0284\n",
            "Epoch [7/10], Step [66/250], Loss: 3.8202\n",
            "Epoch [7/10], Step [67/250], Loss: 3.8518\n",
            "Epoch [7/10], Step [68/250], Loss: 3.8251\n",
            "Epoch [7/10], Step [69/250], Loss: 4.0388\n",
            "Epoch [7/10], Step [70/250], Loss: 4.3542\n",
            "Epoch [7/10], Step [71/250], Loss: 3.6589\n",
            "Epoch [7/10], Step [72/250], Loss: 4.0333\n",
            "Epoch [7/10], Step [73/250], Loss: 4.0811\n",
            "Epoch [7/10], Step [74/250], Loss: 4.2878\n",
            "Epoch [7/10], Step [75/250], Loss: 4.5792\n",
            "Epoch [7/10], Step [76/250], Loss: 3.8049\n",
            "Epoch [7/10], Step [77/250], Loss: 3.9140\n",
            "Epoch [7/10], Step [78/250], Loss: 4.1637\n",
            "Epoch [7/10], Step [79/250], Loss: 4.0204\n",
            "Epoch [7/10], Step [80/250], Loss: 3.9550\n",
            "Epoch [7/10], Step [81/250], Loss: 4.1259\n",
            "Epoch [7/10], Step [82/250], Loss: 5.2879\n",
            "Epoch [7/10], Step [83/250], Loss: 3.9450\n",
            "Epoch [7/10], Step [84/250], Loss: 3.6219\n",
            "Epoch [7/10], Step [85/250], Loss: 4.0162\n",
            "Epoch [7/10], Step [86/250], Loss: 4.1294\n",
            "Epoch [7/10], Step [87/250], Loss: 4.0757\n",
            "Epoch [7/10], Step [88/250], Loss: 4.1055\n",
            "Epoch [7/10], Step [89/250], Loss: 3.7048\n",
            "Epoch [7/10], Step [90/250], Loss: 3.5013\n",
            "Epoch [7/10], Step [91/250], Loss: 4.2031\n",
            "Epoch [7/10], Step [92/250], Loss: 4.5024\n",
            "Epoch [7/10], Step [93/250], Loss: 3.8671\n",
            "Epoch [7/10], Step [94/250], Loss: 4.4588\n",
            "Epoch [7/10], Step [95/250], Loss: 3.9022\n",
            "Epoch [7/10], Step [96/250], Loss: 4.1620\n",
            "Epoch [7/10], Step [97/250], Loss: 3.8485\n",
            "Epoch [7/10], Step [98/250], Loss: 3.8995\n",
            "Epoch [7/10], Step [99/250], Loss: 3.7612\n",
            "Epoch [7/10], Step [100/250], Loss: 3.6722\n",
            "Epoch [7/10], Step [101/250], Loss: 4.1107\n",
            "Epoch [7/10], Step [102/250], Loss: 3.5168\n",
            "Epoch [7/10], Step [103/250], Loss: 3.8712\n",
            "Epoch [7/10], Step [104/250], Loss: 4.3469\n",
            "Epoch [7/10], Step [105/250], Loss: 4.6884\n",
            "Epoch [7/10], Step [106/250], Loss: 4.1917\n",
            "Epoch [7/10], Step [107/250], Loss: 3.8237\n",
            "Epoch [7/10], Step [108/250], Loss: 3.9021\n",
            "Epoch [7/10], Step [109/250], Loss: 3.8863\n",
            "Epoch [7/10], Step [110/250], Loss: 3.7209\n",
            "Epoch [7/10], Step [111/250], Loss: 3.6157\n",
            "Epoch [7/10], Step [112/250], Loss: 4.3590\n",
            "Epoch [7/10], Step [113/250], Loss: 4.2728\n",
            "Epoch [7/10], Step [114/250], Loss: 3.9193\n",
            "Epoch [7/10], Step [115/250], Loss: 4.2037\n",
            "Epoch [7/10], Step [116/250], Loss: 3.8482\n",
            "Epoch [7/10], Step [117/250], Loss: 5.8753\n",
            "Epoch [7/10], Step [118/250], Loss: 4.3134\n",
            "Epoch [7/10], Step [119/250], Loss: 4.2407\n",
            "Epoch [7/10], Step [120/250], Loss: 3.8284\n",
            "Epoch [7/10], Step [121/250], Loss: 4.0493\n",
            "Epoch [7/10], Step [122/250], Loss: 3.8384\n",
            "Epoch [7/10], Step [123/250], Loss: 5.1777\n",
            "Epoch [7/10], Step [124/250], Loss: 3.8732\n",
            "Epoch [7/10], Step [125/250], Loss: 4.3425\n",
            "Epoch [7/10], Step [126/250], Loss: 3.7713\n",
            "Epoch [7/10], Step [127/250], Loss: 3.7939\n",
            "Epoch [7/10], Step [128/250], Loss: 4.6640\n",
            "Epoch [7/10], Step [129/250], Loss: 3.8370\n",
            "Epoch [7/10], Step [130/250], Loss: 3.7497\n",
            "Epoch [7/10], Step [131/250], Loss: 3.9524\n",
            "Epoch [7/10], Step [132/250], Loss: 3.9756\n",
            "Epoch [7/10], Step [133/250], Loss: 3.9017\n",
            "Epoch [7/10], Step [134/250], Loss: 3.7669\n",
            "Epoch [7/10], Step [135/250], Loss: 3.7846\n",
            "Epoch [7/10], Step [136/250], Loss: 4.1960\n",
            "Epoch [7/10], Step [137/250], Loss: 3.6100\n",
            "Epoch [7/10], Step [138/250], Loss: 4.2839\n",
            "Epoch [7/10], Step [139/250], Loss: 3.9125\n",
            "Epoch [7/10], Step [140/250], Loss: 4.0168\n",
            "Epoch [7/10], Step [141/250], Loss: 4.1430\n",
            "Epoch [7/10], Step [142/250], Loss: 5.1092\n",
            "Epoch [7/10], Step [143/250], Loss: 3.8887\n",
            "Epoch [7/10], Step [144/250], Loss: 3.8418\n",
            "Epoch [7/10], Step [145/250], Loss: 3.9188\n",
            "Epoch [7/10], Step [146/250], Loss: 4.0303\n",
            "Epoch [7/10], Step [147/250], Loss: 3.8085\n",
            "Epoch [7/10], Step [148/250], Loss: 5.4015\n",
            "Epoch [7/10], Step [149/250], Loss: 2.6099\n",
            "Epoch [7/10], Step [150/250], Loss: 3.9411\n",
            "Epoch [7/10], Step [151/250], Loss: 3.9477\n",
            "Epoch [7/10], Step [152/250], Loss: 4.3823\n",
            "Epoch [7/10], Step [153/250], Loss: 3.9120\n",
            "Epoch [7/10], Step [154/250], Loss: 4.0748\n",
            "Epoch [7/10], Step [155/250], Loss: 5.0607\n",
            "Epoch [7/10], Step [156/250], Loss: 4.0052\n",
            "Epoch [7/10], Step [157/250], Loss: 5.1232\n",
            "Epoch [7/10], Step [158/250], Loss: 3.4840\n",
            "Epoch [7/10], Step [159/250], Loss: 4.2969\n",
            "Epoch [7/10], Step [160/250], Loss: 5.4914\n",
            "Epoch [7/10], Step [161/250], Loss: 3.6973\n",
            "Epoch [7/10], Step [162/250], Loss: 3.8992\n",
            "Epoch [7/10], Step [163/250], Loss: 4.4520\n",
            "Epoch [7/10], Step [164/250], Loss: 4.2561\n",
            "Epoch [7/10], Step [165/250], Loss: 4.4323\n",
            "Epoch [7/10], Step [166/250], Loss: 3.7168\n",
            "Epoch [7/10], Step [167/250], Loss: 3.9425\n",
            "Epoch [7/10], Step [168/250], Loss: 4.3739\n",
            "Epoch [7/10], Step [169/250], Loss: 4.0908\n",
            "Epoch [7/10], Step [170/250], Loss: 5.0403\n",
            "Epoch [7/10], Step [171/250], Loss: 3.6671\n",
            "Epoch [7/10], Step [172/250], Loss: 3.8032\n",
            "Epoch [7/10], Step [173/250], Loss: 3.9037\n",
            "Epoch [7/10], Step [174/250], Loss: 3.9131\n",
            "Epoch [7/10], Step [175/250], Loss: 3.8697\n",
            "Epoch [7/10], Step [176/250], Loss: 3.7260\n",
            "Epoch [7/10], Step [177/250], Loss: 3.6671\n",
            "Epoch [7/10], Step [178/250], Loss: 4.0692\n",
            "Epoch [7/10], Step [179/250], Loss: 3.6093\n",
            "Epoch [7/10], Step [180/250], Loss: 3.7399\n",
            "Epoch [7/10], Step [181/250], Loss: 4.1385\n",
            "Epoch [7/10], Step [182/250], Loss: 5.8580\n",
            "Epoch [7/10], Step [183/250], Loss: 4.0061\n",
            "Epoch [7/10], Step [184/250], Loss: 3.5751\n",
            "Epoch [7/10], Step [185/250], Loss: 3.9306\n",
            "Epoch [7/10], Step [186/250], Loss: 3.4799\n",
            "Epoch [7/10], Step [187/250], Loss: 3.7102\n",
            "Epoch [7/10], Step [188/250], Loss: 3.9307\n",
            "Epoch [7/10], Step [189/250], Loss: 4.0727\n",
            "Epoch [7/10], Step [190/250], Loss: 3.7923\n",
            "Epoch [7/10], Step [191/250], Loss: 4.0971\n",
            "Epoch [7/10], Step [192/250], Loss: 3.7056\n",
            "Epoch [7/10], Step [193/250], Loss: 4.5276\n",
            "Epoch [7/10], Step [194/250], Loss: 4.0004\n",
            "Epoch [7/10], Step [195/250], Loss: 3.9197\n",
            "Epoch [7/10], Step [196/250], Loss: 3.9543\n",
            "Epoch [7/10], Step [197/250], Loss: 3.5639\n",
            "Epoch [7/10], Step [198/250], Loss: 3.9836\n",
            "Epoch [7/10], Step [199/250], Loss: 4.0359\n",
            "Epoch [7/10], Step [200/250], Loss: 3.9702\n",
            "Epoch [7/10], Step [201/250], Loss: 3.7667\n",
            "Epoch [7/10], Step [202/250], Loss: 4.5513\n",
            "Epoch [7/10], Step [203/250], Loss: 4.5362\n",
            "Epoch [7/10], Step [204/250], Loss: 4.0998\n",
            "Epoch [7/10], Step [205/250], Loss: 3.7838\n",
            "Epoch [7/10], Step [206/250], Loss: 3.9070\n",
            "Epoch [7/10], Step [207/250], Loss: 3.5997\n",
            "Epoch [7/10], Step [208/250], Loss: 4.0149\n",
            "Epoch [7/10], Step [209/250], Loss: 3.8842\n",
            "Epoch [7/10], Step [210/250], Loss: 4.6713\n",
            "Epoch [7/10], Step [211/250], Loss: 3.7813\n",
            "Epoch [7/10], Step [212/250], Loss: 3.7630\n",
            "Epoch [7/10], Step [213/250], Loss: 3.7672\n",
            "Epoch [7/10], Step [214/250], Loss: 4.0695\n",
            "Epoch [7/10], Step [215/250], Loss: 3.6874\n",
            "Epoch [7/10], Step [216/250], Loss: 3.6801\n",
            "Epoch [7/10], Step [217/250], Loss: 4.4532\n",
            "Epoch [7/10], Step [218/250], Loss: 3.7020\n",
            "Epoch [7/10], Step [219/250], Loss: 3.7606\n",
            "Epoch [7/10], Step [220/250], Loss: 3.9689\n",
            "Epoch [7/10], Step [221/250], Loss: 3.6963\n",
            "Epoch [7/10], Step [222/250], Loss: 5.2367\n",
            "Epoch [7/10], Step [223/250], Loss: 3.9734\n",
            "Epoch [7/10], Step [224/250], Loss: 3.7396\n",
            "Epoch [7/10], Step [225/250], Loss: 4.5567\n",
            "Epoch [7/10], Step [226/250], Loss: 4.1305\n",
            "Epoch [7/10], Step [227/250], Loss: 3.8043\n",
            "Epoch [7/10], Step [228/250], Loss: 3.8110\n",
            "Epoch [7/10], Step [229/250], Loss: 3.6297\n",
            "Epoch [7/10], Step [230/250], Loss: 3.8322\n",
            "Epoch [7/10], Step [231/250], Loss: 4.0596\n",
            "Epoch [7/10], Step [232/250], Loss: 4.0724\n",
            "Epoch [7/10], Step [233/250], Loss: 4.1011\n",
            "Epoch [7/10], Step [234/250], Loss: 3.9450\n",
            "Epoch [7/10], Step [235/250], Loss: 3.5123\n",
            "Epoch [7/10], Step [236/250], Loss: 4.5702\n",
            "Epoch [7/10], Step [237/250], Loss: 4.1215\n",
            "Epoch [7/10], Step [238/250], Loss: 4.2475\n",
            "Epoch [7/10], Step [239/250], Loss: 4.1059\n",
            "Epoch [7/10], Step [240/250], Loss: 3.8017\n",
            "Epoch [7/10], Step [241/250], Loss: 3.6922\n",
            "Epoch [7/10], Step [242/250], Loss: 3.6290\n",
            "Epoch [7/10], Step [243/250], Loss: 3.7772\n",
            "Epoch [7/10], Step [244/250], Loss: 3.8836\n",
            "Epoch [7/10], Step [245/250], Loss: 3.8864\n",
            "Epoch [7/10], Step [246/250], Loss: 4.1390\n",
            "Epoch [7/10], Step [247/250], Loss: 3.5163\n",
            "Epoch [7/10], Step [248/250], Loss: 5.3926\n",
            "Epoch [7/10], Step [249/250], Loss: 4.1650\n",
            "Epoch [7/10], Step [250/250], Loss: 4.1194\n",
            "Epoch 7: Avg Loss = 4.0674 Current Loss = 4.1194\n",
            "Epoch [8/10], Step [1/250], Loss: 4.0732\n",
            "Epoch [8/10], Step [2/250], Loss: 4.0591\n",
            "Epoch [8/10], Step [3/250], Loss: 3.8455\n",
            "Epoch [8/10], Step [4/250], Loss: 4.0891\n",
            "Epoch [8/10], Step [5/250], Loss: 3.8525\n",
            "Epoch [8/10], Step [6/250], Loss: 3.8238\n",
            "Epoch [8/10], Step [7/250], Loss: 4.7041\n",
            "Epoch [8/10], Step [8/250], Loss: 4.8948\n",
            "Epoch [8/10], Step [9/250], Loss: 3.8372\n",
            "Epoch [8/10], Step [10/250], Loss: 4.4882\n",
            "Epoch [8/10], Step [11/250], Loss: 3.7673\n",
            "Epoch [8/10], Step [12/250], Loss: 3.3922\n",
            "Epoch [8/10], Step [13/250], Loss: 3.8280\n",
            "Epoch [8/10], Step [14/250], Loss: 3.6915\n",
            "Epoch [8/10], Step [15/250], Loss: 3.9145\n",
            "Epoch [8/10], Step [16/250], Loss: 3.8120\n",
            "Epoch [8/10], Step [17/250], Loss: 4.2708\n",
            "Epoch [8/10], Step [18/250], Loss: 3.7157\n",
            "Epoch [8/10], Step [19/250], Loss: 4.2291\n",
            "Epoch [8/10], Step [20/250], Loss: 3.9540\n",
            "Epoch [8/10], Step [21/250], Loss: 3.8535\n",
            "Epoch [8/10], Step [22/250], Loss: 3.8957\n",
            "Epoch [8/10], Step [23/250], Loss: 4.5780\n",
            "Epoch [8/10], Step [24/250], Loss: 3.3156\n",
            "Epoch [8/10], Step [25/250], Loss: 4.1505\n",
            "Epoch [8/10], Step [26/250], Loss: 4.0304\n",
            "Epoch [8/10], Step [27/250], Loss: 4.1860\n",
            "Epoch [8/10], Step [28/250], Loss: 4.0775\n",
            "Epoch [8/10], Step [29/250], Loss: 3.8048\n",
            "Epoch [8/10], Step [30/250], Loss: 4.0080\n",
            "Epoch [8/10], Step [31/250], Loss: 3.8399\n",
            "Epoch [8/10], Step [32/250], Loss: 3.7585\n",
            "Epoch [8/10], Step [33/250], Loss: 4.3394\n",
            "Epoch [8/10], Step [34/250], Loss: 3.7556\n",
            "Epoch [8/10], Step [35/250], Loss: 3.9010\n",
            "Epoch [8/10], Step [36/250], Loss: 3.9335\n",
            "Epoch [8/10], Step [37/250], Loss: 3.8429\n",
            "Epoch [8/10], Step [38/250], Loss: 4.6345\n",
            "Epoch [8/10], Step [39/250], Loss: 4.1469\n",
            "Epoch [8/10], Step [40/250], Loss: 3.9251\n",
            "Epoch [8/10], Step [41/250], Loss: 3.9843\n",
            "Epoch [8/10], Step [42/250], Loss: 3.9343\n",
            "Epoch [8/10], Step [43/250], Loss: 3.9196\n",
            "Epoch [8/10], Step [44/250], Loss: 3.7943\n",
            "Epoch [8/10], Step [45/250], Loss: 3.8439\n",
            "Epoch [8/10], Step [46/250], Loss: 7.0185\n",
            "Epoch [8/10], Step [47/250], Loss: 3.6443\n",
            "Epoch [8/10], Step [48/250], Loss: 5.2889\n",
            "Epoch [8/10], Step [49/250], Loss: 4.0798\n",
            "Epoch [8/10], Step [50/250], Loss: 4.0710\n",
            "Epoch [8/10], Step [51/250], Loss: 4.0767\n",
            "Epoch [8/10], Step [52/250], Loss: 3.6761\n",
            "Epoch [8/10], Step [53/250], Loss: 3.9952\n",
            "Epoch [8/10], Step [54/250], Loss: 4.0341\n",
            "Epoch [8/10], Step [55/250], Loss: 5.8024\n",
            "Epoch [8/10], Step [56/250], Loss: 3.8531\n",
            "Epoch [8/10], Step [57/250], Loss: 3.9205\n",
            "Epoch [8/10], Step [58/250], Loss: 4.1520\n",
            "Epoch [8/10], Step [59/250], Loss: 3.9304\n",
            "Epoch [8/10], Step [60/250], Loss: 3.8764\n",
            "Epoch [8/10], Step [61/250], Loss: 3.8843\n",
            "Epoch [8/10], Step [62/250], Loss: 3.8193\n",
            "Epoch [8/10], Step [63/250], Loss: 3.8535\n",
            "Epoch [8/10], Step [64/250], Loss: 4.3048\n",
            "Epoch [8/10], Step [65/250], Loss: 3.7113\n",
            "Epoch [8/10], Step [66/250], Loss: 3.8650\n",
            "Epoch [8/10], Step [67/250], Loss: 3.8642\n",
            "Epoch [8/10], Step [68/250], Loss: 3.8377\n",
            "Epoch [8/10], Step [69/250], Loss: 3.6697\n",
            "Epoch [8/10], Step [70/250], Loss: 4.7516\n",
            "Epoch [8/10], Step [71/250], Loss: 3.9287\n",
            "Epoch [8/10], Step [72/250], Loss: 3.8617\n",
            "Epoch [8/10], Step [73/250], Loss: 3.5792\n",
            "Epoch [8/10], Step [74/250], Loss: 3.9951\n",
            "Epoch [8/10], Step [75/250], Loss: 3.6378\n",
            "Epoch [8/10], Step [76/250], Loss: 3.8310\n",
            "Epoch [8/10], Step [77/250], Loss: 3.8192\n",
            "Epoch [8/10], Step [78/250], Loss: 6.2789\n",
            "Epoch [8/10], Step [79/250], Loss: 3.8744\n",
            "Epoch [8/10], Step [80/250], Loss: 3.8900\n",
            "Epoch [8/10], Step [81/250], Loss: 4.0162\n",
            "Epoch [8/10], Step [82/250], Loss: 3.9825\n",
            "Epoch [8/10], Step [83/250], Loss: 4.0847\n",
            "Epoch [8/10], Step [84/250], Loss: 4.2028\n",
            "Epoch [8/10], Step [85/250], Loss: 4.5573\n",
            "Epoch [8/10], Step [86/250], Loss: 3.8176\n",
            "Epoch [8/10], Step [87/250], Loss: 3.9254\n",
            "Epoch [8/10], Step [88/250], Loss: 3.9981\n",
            "Epoch [8/10], Step [89/250], Loss: 3.6792\n",
            "Epoch [8/10], Step [90/250], Loss: 4.1774\n",
            "Epoch [8/10], Step [91/250], Loss: 4.0201\n",
            "Epoch [8/10], Step [92/250], Loss: 3.9808\n",
            "Epoch [8/10], Step [93/250], Loss: 3.7558\n",
            "Epoch [8/10], Step [94/250], Loss: 3.6924\n",
            "Epoch [8/10], Step [95/250], Loss: 3.8535\n",
            "Epoch [8/10], Step [96/250], Loss: 4.3506\n",
            "Epoch [8/10], Step [97/250], Loss: 4.0098\n",
            "Epoch [8/10], Step [98/250], Loss: 3.8550\n",
            "Epoch [8/10], Step [99/250], Loss: 3.8116\n",
            "Epoch [8/10], Step [100/250], Loss: 4.1907\n",
            "Epoch [8/10], Step [101/250], Loss: 3.9306\n",
            "Epoch [8/10], Step [102/250], Loss: 3.9152\n",
            "Epoch [8/10], Step [103/250], Loss: 3.8219\n",
            "Epoch [8/10], Step [104/250], Loss: 4.0643\n",
            "Epoch [8/10], Step [105/250], Loss: 3.7163\n",
            "Epoch [8/10], Step [106/250], Loss: 3.5232\n",
            "Epoch [8/10], Step [107/250], Loss: 3.8159\n",
            "Epoch [8/10], Step [108/250], Loss: 3.8627\n",
            "Epoch [8/10], Step [109/250], Loss: 4.2495\n",
            "Epoch [8/10], Step [110/250], Loss: 4.1157\n",
            "Epoch [8/10], Step [111/250], Loss: 3.8237\n",
            "Epoch [8/10], Step [112/250], Loss: 4.5295\n",
            "Epoch [8/10], Step [113/250], Loss: 4.3892\n",
            "Epoch [8/10], Step [114/250], Loss: 4.2109\n",
            "Epoch [8/10], Step [115/250], Loss: 3.7661\n",
            "Epoch [8/10], Step [116/250], Loss: 4.1923\n",
            "Epoch [8/10], Step [117/250], Loss: 3.7384\n",
            "Epoch [8/10], Step [118/250], Loss: 3.9016\n",
            "Epoch [8/10], Step [119/250], Loss: 3.9661\n",
            "Epoch [8/10], Step [120/250], Loss: 4.0532\n",
            "Epoch [8/10], Step [121/250], Loss: 3.8762\n",
            "Epoch [8/10], Step [122/250], Loss: 3.6279\n",
            "Epoch [8/10], Step [123/250], Loss: 4.1840\n",
            "Epoch [8/10], Step [124/250], Loss: 3.9645\n",
            "Epoch [8/10], Step [125/250], Loss: 3.8925\n",
            "Epoch [8/10], Step [126/250], Loss: 4.6482\n",
            "Epoch [8/10], Step [127/250], Loss: 3.7567\n",
            "Epoch [8/10], Step [128/250], Loss: 4.7274\n",
            "Epoch [8/10], Step [129/250], Loss: 3.6798\n",
            "Epoch [8/10], Step [130/250], Loss: 3.7351\n",
            "Epoch [8/10], Step [131/250], Loss: 3.8640\n",
            "Epoch [8/10], Step [132/250], Loss: 3.7136\n",
            "Epoch [8/10], Step [133/250], Loss: 3.9729\n",
            "Epoch [8/10], Step [134/250], Loss: 4.0929\n",
            "Epoch [8/10], Step [135/250], Loss: 3.9618\n",
            "Epoch [8/10], Step [136/250], Loss: 4.1659\n",
            "Epoch [8/10], Step [137/250], Loss: 4.1445\n",
            "Epoch [8/10], Step [138/250], Loss: 3.9425\n",
            "Epoch [8/10], Step [139/250], Loss: 3.5924\n",
            "Epoch [8/10], Step [140/250], Loss: 4.1186\n",
            "Epoch [8/10], Step [141/250], Loss: 3.8399\n",
            "Epoch [8/10], Step [142/250], Loss: 3.6628\n",
            "Epoch [8/10], Step [143/250], Loss: 3.8144\n",
            "Epoch [8/10], Step [144/250], Loss: 4.1326\n",
            "Epoch [8/10], Step [145/250], Loss: 4.0510\n",
            "Epoch [8/10], Step [146/250], Loss: 3.9268\n",
            "Epoch [8/10], Step [147/250], Loss: 2.5376\n",
            "Epoch [8/10], Step [148/250], Loss: 3.8473\n",
            "Epoch [8/10], Step [149/250], Loss: 4.1114\n",
            "Epoch [8/10], Step [150/250], Loss: 4.6053\n",
            "Epoch [8/10], Step [151/250], Loss: 3.9933\n",
            "Epoch [8/10], Step [152/250], Loss: 3.7161\n",
            "Epoch [8/10], Step [153/250], Loss: 3.7750\n",
            "Epoch [8/10], Step [154/250], Loss: 3.9532\n",
            "Epoch [8/10], Step [155/250], Loss: 3.9222\n",
            "Epoch [8/10], Step [156/250], Loss: 3.7861\n",
            "Epoch [8/10], Step [157/250], Loss: 3.8354\n",
            "Epoch [8/10], Step [158/250], Loss: 3.6571\n",
            "Epoch [8/10], Step [159/250], Loss: 3.9218\n",
            "Epoch [8/10], Step [160/250], Loss: 3.7988\n",
            "Epoch [8/10], Step [161/250], Loss: 4.0524\n",
            "Epoch [8/10], Step [162/250], Loss: 3.9269\n",
            "Epoch [8/10], Step [163/250], Loss: 4.1334\n",
            "Epoch [8/10], Step [164/250], Loss: 4.0458\n",
            "Epoch [8/10], Step [165/250], Loss: 4.9903\n",
            "Epoch [8/10], Step [166/250], Loss: 4.0492\n",
            "Epoch [8/10], Step [167/250], Loss: 3.9959\n",
            "Epoch [8/10], Step [168/250], Loss: 3.9055\n",
            "Epoch [8/10], Step [169/250], Loss: 4.1703\n",
            "Epoch [8/10], Step [170/250], Loss: 3.9733\n",
            "Epoch [8/10], Step [171/250], Loss: 3.8513\n",
            "Epoch [8/10], Step [172/250], Loss: 4.1393\n",
            "Epoch [8/10], Step [173/250], Loss: 3.8326\n",
            "Epoch [8/10], Step [174/250], Loss: 4.0272\n",
            "Epoch [8/10], Step [175/250], Loss: 4.0280\n",
            "Epoch [8/10], Step [176/250], Loss: 3.9991\n",
            "Epoch [8/10], Step [177/250], Loss: 3.9424\n",
            "Epoch [8/10], Step [178/250], Loss: 3.8476\n",
            "Epoch [8/10], Step [179/250], Loss: 4.0118\n",
            "Epoch [8/10], Step [180/250], Loss: 3.6697\n",
            "Epoch [8/10], Step [181/250], Loss: 4.0121\n",
            "Epoch [8/10], Step [182/250], Loss: 4.0613\n",
            "Epoch [8/10], Step [183/250], Loss: 4.0082\n",
            "Epoch [8/10], Step [184/250], Loss: 4.1822\n",
            "Epoch [8/10], Step [185/250], Loss: 5.7011\n",
            "Epoch [8/10], Step [186/250], Loss: 4.1513\n",
            "Epoch [8/10], Step [187/250], Loss: 4.0994\n",
            "Epoch [8/10], Step [188/250], Loss: 3.8582\n",
            "Epoch [8/10], Step [189/250], Loss: 4.1867\n",
            "Epoch [8/10], Step [190/250], Loss: 4.1791\n",
            "Epoch [8/10], Step [191/250], Loss: 4.3406\n",
            "Epoch [8/10], Step [192/250], Loss: 4.0932\n",
            "Epoch [8/10], Step [193/250], Loss: 4.2615\n",
            "Epoch [8/10], Step [194/250], Loss: 4.1339\n",
            "Epoch [8/10], Step [195/250], Loss: 3.8593\n",
            "Epoch [8/10], Step [196/250], Loss: 3.9648\n",
            "Epoch [8/10], Step [197/250], Loss: 3.7723\n",
            "Epoch [8/10], Step [198/250], Loss: 3.9245\n",
            "Epoch [8/10], Step [199/250], Loss: 4.0163\n",
            "Epoch [8/10], Step [200/250], Loss: 3.8458\n",
            "Epoch [8/10], Step [201/250], Loss: 3.7587\n",
            "Epoch [8/10], Step [202/250], Loss: 3.8269\n",
            "Epoch [8/10], Step [203/250], Loss: 3.7936\n",
            "Epoch [8/10], Step [204/250], Loss: 4.6943\n",
            "Epoch [8/10], Step [205/250], Loss: 4.8416\n",
            "Epoch [8/10], Step [206/250], Loss: 3.7761\n",
            "Epoch [8/10], Step [207/250], Loss: 3.8155\n",
            "Epoch [8/10], Step [208/250], Loss: 4.1518\n",
            "Epoch [8/10], Step [209/250], Loss: 3.8567\n",
            "Epoch [8/10], Step [210/250], Loss: 4.2191\n",
            "Epoch [8/10], Step [211/250], Loss: 4.1344\n",
            "Epoch [8/10], Step [212/250], Loss: 3.9167\n",
            "Epoch [8/10], Step [213/250], Loss: 3.3925\n",
            "Epoch [8/10], Step [214/250], Loss: 4.0907\n",
            "Epoch [8/10], Step [215/250], Loss: 3.9965\n",
            "Epoch [8/10], Step [216/250], Loss: 4.1125\n",
            "Epoch [8/10], Step [217/250], Loss: 3.8164\n",
            "Epoch [8/10], Step [218/250], Loss: 3.8810\n",
            "Epoch [8/10], Step [219/250], Loss: 4.1787\n",
            "Epoch [8/10], Step [220/250], Loss: 4.2197\n",
            "Epoch [8/10], Step [221/250], Loss: 4.0026\n",
            "Epoch [8/10], Step [222/250], Loss: 3.8985\n",
            "Epoch [8/10], Step [223/250], Loss: 4.0868\n",
            "Epoch [8/10], Step [224/250], Loss: 5.8406\n",
            "Epoch [8/10], Step [225/250], Loss: 7.8316\n",
            "Epoch [8/10], Step [226/250], Loss: 4.0519\n",
            "Epoch [8/10], Step [227/250], Loss: 3.9263\n",
            "Epoch [8/10], Step [228/250], Loss: 4.0776\n",
            "Epoch [8/10], Step [229/250], Loss: 3.8664\n",
            "Epoch [8/10], Step [230/250], Loss: 3.6266\n",
            "Epoch [8/10], Step [231/250], Loss: 3.9351\n",
            "Epoch [8/10], Step [232/250], Loss: 3.7463\n",
            "Epoch [8/10], Step [233/250], Loss: 3.9310\n",
            "Epoch [8/10], Step [234/250], Loss: 4.0856\n",
            "Epoch [8/10], Step [235/250], Loss: 3.7038\n",
            "Epoch [8/10], Step [236/250], Loss: 3.9069\n",
            "Epoch [8/10], Step [237/250], Loss: 3.8221\n",
            "Epoch [8/10], Step [238/250], Loss: 4.1430\n",
            "Epoch [8/10], Step [239/250], Loss: 4.9820\n",
            "Epoch [8/10], Step [240/250], Loss: 3.8602\n",
            "Epoch [8/10], Step [241/250], Loss: 4.2374\n",
            "Epoch [8/10], Step [242/250], Loss: 3.7844\n",
            "Epoch [8/10], Step [243/250], Loss: 3.8981\n",
            "Epoch [8/10], Step [244/250], Loss: 3.8633\n",
            "Epoch [8/10], Step [245/250], Loss: 3.9021\n",
            "Epoch [8/10], Step [246/250], Loss: 3.8654\n",
            "Epoch [8/10], Step [247/250], Loss: 3.9116\n",
            "Epoch [8/10], Step [248/250], Loss: 3.7636\n",
            "Epoch [8/10], Step [249/250], Loss: 3.9569\n",
            "Epoch [8/10], Step [250/250], Loss: 3.8311\n",
            "Epoch 8: Avg Loss = 4.0381 Current Loss = 3.8311\n",
            "Epoch [9/10], Step [1/250], Loss: 6.2613\n",
            "Epoch [9/10], Step [2/250], Loss: 3.9131\n",
            "Epoch [9/10], Step [3/250], Loss: 3.9250\n",
            "Epoch [9/10], Step [4/250], Loss: 4.8021\n",
            "Epoch [9/10], Step [5/250], Loss: 3.8996\n",
            "Epoch [9/10], Step [6/250], Loss: 3.8688\n",
            "Epoch [9/10], Step [7/250], Loss: 3.7184\n",
            "Epoch [9/10], Step [8/250], Loss: 3.5495\n",
            "Epoch [9/10], Step [9/250], Loss: 3.7219\n",
            "Epoch [9/10], Step [10/250], Loss: 3.6612\n",
            "Epoch [9/10], Step [11/250], Loss: 3.7507\n",
            "Epoch [9/10], Step [12/250], Loss: 3.7528\n",
            "Epoch [9/10], Step [13/250], Loss: 3.7956\n",
            "Epoch [9/10], Step [14/250], Loss: 4.0256\n",
            "Epoch [9/10], Step [15/250], Loss: 4.0149\n",
            "Epoch [9/10], Step [16/250], Loss: 4.2020\n",
            "Epoch [9/10], Step [17/250], Loss: 3.8150\n",
            "Epoch [9/10], Step [18/250], Loss: 3.7543\n",
            "Epoch [9/10], Step [19/250], Loss: 3.7967\n",
            "Epoch [9/10], Step [20/250], Loss: 4.0534\n",
            "Epoch [9/10], Step [21/250], Loss: 6.9514\n",
            "Epoch [9/10], Step [22/250], Loss: 3.6717\n",
            "Epoch [9/10], Step [23/250], Loss: 3.7549\n",
            "Epoch [9/10], Step [24/250], Loss: 3.9652\n",
            "Epoch [9/10], Step [25/250], Loss: 3.8032\n",
            "Epoch [9/10], Step [26/250], Loss: 3.8956\n",
            "Epoch [9/10], Step [27/250], Loss: 3.7041\n",
            "Epoch [9/10], Step [28/250], Loss: 3.7403\n",
            "Epoch [9/10], Step [29/250], Loss: 3.8434\n",
            "Epoch [9/10], Step [30/250], Loss: 3.8102\n",
            "Epoch [9/10], Step [31/250], Loss: 4.6513\n",
            "Epoch [9/10], Step [32/250], Loss: 3.9014\n",
            "Epoch [9/10], Step [33/250], Loss: 4.1672\n",
            "Epoch [9/10], Step [34/250], Loss: 3.9586\n",
            "Epoch [9/10], Step [35/250], Loss: 3.9803\n",
            "Epoch [9/10], Step [36/250], Loss: 3.8720\n",
            "Epoch [9/10], Step [37/250], Loss: 4.2097\n",
            "Epoch [9/10], Step [38/250], Loss: 3.7940\n",
            "Epoch [9/10], Step [39/250], Loss: 4.0271\n",
            "Epoch [9/10], Step [40/250], Loss: 3.9161\n",
            "Epoch [9/10], Step [41/250], Loss: 4.0792\n",
            "Epoch [9/10], Step [42/250], Loss: 3.7969\n",
            "Epoch [9/10], Step [43/250], Loss: 3.8894\n",
            "Epoch [9/10], Step [44/250], Loss: 3.5782\n",
            "Epoch [9/10], Step [45/250], Loss: 3.9666\n",
            "Epoch [9/10], Step [46/250], Loss: 4.0903\n",
            "Epoch [9/10], Step [47/250], Loss: 6.9676\n",
            "Epoch [9/10], Step [48/250], Loss: 3.6948\n",
            "Epoch [9/10], Step [49/250], Loss: 4.0642\n",
            "Epoch [9/10], Step [50/250], Loss: 3.9247\n",
            "Epoch [9/10], Step [51/250], Loss: 3.6829\n",
            "Epoch [9/10], Step [52/250], Loss: 3.6589\n",
            "Epoch [9/10], Step [53/250], Loss: 3.7891\n",
            "Epoch [9/10], Step [54/250], Loss: 3.8733\n",
            "Epoch [9/10], Step [55/250], Loss: 3.8461\n",
            "Epoch [9/10], Step [56/250], Loss: 3.8782\n",
            "Epoch [9/10], Step [57/250], Loss: 3.9336\n",
            "Epoch [9/10], Step [58/250], Loss: 4.2318\n",
            "Epoch [9/10], Step [59/250], Loss: 3.9611\n",
            "Epoch [9/10], Step [60/250], Loss: 3.8226\n",
            "Epoch [9/10], Step [61/250], Loss: 4.4247\n",
            "Epoch [9/10], Step [62/250], Loss: 4.0336\n",
            "Epoch [9/10], Step [63/250], Loss: 5.1243\n",
            "Epoch [9/10], Step [64/250], Loss: 3.9557\n",
            "Epoch [9/10], Step [65/250], Loss: 3.6335\n",
            "Epoch [9/10], Step [66/250], Loss: 4.0965\n",
            "Epoch [9/10], Step [67/250], Loss: 4.0858\n",
            "Epoch [9/10], Step [68/250], Loss: 4.2277\n",
            "Epoch [9/10], Step [69/250], Loss: 4.1991\n",
            "Epoch [9/10], Step [70/250], Loss: 3.9341\n",
            "Epoch [9/10], Step [71/250], Loss: 4.1322\n",
            "Epoch [9/10], Step [72/250], Loss: 5.3300\n",
            "Epoch [9/10], Step [73/250], Loss: 3.9664\n",
            "Epoch [9/10], Step [74/250], Loss: 4.0110\n",
            "Epoch [9/10], Step [75/250], Loss: 5.1057\n",
            "Epoch [9/10], Step [76/250], Loss: 3.7763\n",
            "Epoch [9/10], Step [77/250], Loss: 3.9737\n",
            "Epoch [9/10], Step [78/250], Loss: 4.6285\n",
            "Epoch [9/10], Step [79/250], Loss: 3.9228\n",
            "Epoch [9/10], Step [80/250], Loss: 5.1024\n",
            "Epoch [9/10], Step [81/250], Loss: 3.9862\n",
            "Epoch [9/10], Step [82/250], Loss: 4.3807\n",
            "Epoch [9/10], Step [83/250], Loss: 3.7491\n",
            "Epoch [9/10], Step [84/250], Loss: 3.8869\n",
            "Epoch [9/10], Step [85/250], Loss: 3.8449\n",
            "Epoch [9/10], Step [86/250], Loss: 3.3475\n",
            "Epoch [9/10], Step [87/250], Loss: 3.9260\n",
            "Epoch [9/10], Step [88/250], Loss: 3.8463\n",
            "Epoch [9/10], Step [89/250], Loss: 3.6807\n",
            "Epoch [9/10], Step [90/250], Loss: 3.6224\n",
            "Epoch [9/10], Step [91/250], Loss: 6.2277\n",
            "Epoch [9/10], Step [92/250], Loss: 3.9561\n",
            "Epoch [9/10], Step [93/250], Loss: 3.9126\n",
            "Epoch [9/10], Step [94/250], Loss: 3.8429\n",
            "Epoch [9/10], Step [95/250], Loss: 3.8087\n",
            "Epoch [9/10], Step [96/250], Loss: 5.6550\n",
            "Epoch [9/10], Step [97/250], Loss: 3.8010\n",
            "Epoch [9/10], Step [98/250], Loss: 3.9530\n",
            "Epoch [9/10], Step [99/250], Loss: 3.7103\n",
            "Epoch [9/10], Step [100/250], Loss: 4.8694\n",
            "Epoch [9/10], Step [101/250], Loss: 4.0720\n",
            "Epoch [9/10], Step [102/250], Loss: 3.8799\n",
            "Epoch [9/10], Step [103/250], Loss: 3.4706\n",
            "Epoch [9/10], Step [104/250], Loss: 3.8739\n",
            "Epoch [9/10], Step [105/250], Loss: 4.1635\n",
            "Epoch [9/10], Step [106/250], Loss: 3.7963\n",
            "Epoch [9/10], Step [107/250], Loss: 3.8867\n",
            "Epoch [9/10], Step [108/250], Loss: 4.0122\n",
            "Epoch [9/10], Step [109/250], Loss: 3.8344\n",
            "Epoch [9/10], Step [110/250], Loss: 4.2025\n",
            "Epoch [9/10], Step [111/250], Loss: 4.5018\n",
            "Epoch [9/10], Step [112/250], Loss: 3.8631\n",
            "Epoch [9/10], Step [113/250], Loss: 3.7798\n",
            "Epoch [9/10], Step [114/250], Loss: 4.2546\n",
            "Epoch [9/10], Step [115/250], Loss: 4.1046\n",
            "Epoch [9/10], Step [116/250], Loss: 3.8581\n",
            "Epoch [9/10], Step [117/250], Loss: 4.2843\n",
            "Epoch [9/10], Step [118/250], Loss: 4.0723\n",
            "Epoch [9/10], Step [119/250], Loss: 3.8704\n",
            "Epoch [9/10], Step [120/250], Loss: 4.9393\n",
            "Epoch [9/10], Step [121/250], Loss: 4.4942\n",
            "Epoch [9/10], Step [122/250], Loss: 4.0082\n",
            "Epoch [9/10], Step [123/250], Loss: 3.7605\n",
            "Epoch [9/10], Step [124/250], Loss: 4.2754\n",
            "Epoch [9/10], Step [125/250], Loss: 3.6707\n",
            "Epoch [9/10], Step [126/250], Loss: 3.8813\n",
            "Epoch [9/10], Step [127/250], Loss: 4.1187\n",
            "Epoch [9/10], Step [128/250], Loss: 3.5104\n",
            "Epoch [9/10], Step [129/250], Loss: 3.7187\n",
            "Epoch [9/10], Step [130/250], Loss: 5.1019\n",
            "Epoch [9/10], Step [131/250], Loss: 4.3332\n",
            "Epoch [9/10], Step [132/250], Loss: 4.0244\n",
            "Epoch [9/10], Step [133/250], Loss: 3.8290\n",
            "Epoch [9/10], Step [134/250], Loss: 3.8748\n",
            "Epoch [9/10], Step [135/250], Loss: 3.9866\n",
            "Epoch [9/10], Step [136/250], Loss: 3.5628\n",
            "Epoch [9/10], Step [137/250], Loss: 3.9282\n",
            "Epoch [9/10], Step [138/250], Loss: 4.5650\n",
            "Epoch [9/10], Step [139/250], Loss: 3.9765\n",
            "Epoch [9/10], Step [140/250], Loss: 3.8217\n",
            "Epoch [9/10], Step [141/250], Loss: 3.9839\n",
            "Epoch [9/10], Step [142/250], Loss: 3.8244\n",
            "Epoch [9/10], Step [143/250], Loss: 3.7154\n",
            "Epoch [9/10], Step [144/250], Loss: 3.8321\n",
            "Epoch [9/10], Step [145/250], Loss: 5.8695\n",
            "Epoch [9/10], Step [146/250], Loss: 3.7836\n",
            "Epoch [9/10], Step [147/250], Loss: 4.3682\n",
            "Epoch [9/10], Step [148/250], Loss: 3.4609\n",
            "Epoch [9/10], Step [149/250], Loss: 3.8810\n",
            "Epoch [9/10], Step [150/250], Loss: 3.0454\n",
            "Epoch [9/10], Step [151/250], Loss: 4.2403\n",
            "Epoch [9/10], Step [152/250], Loss: 3.7642\n",
            "Epoch [9/10], Step [153/250], Loss: 3.9566\n",
            "Epoch [9/10], Step [154/250], Loss: 3.8338\n",
            "Epoch [9/10], Step [155/250], Loss: 3.9024\n",
            "Epoch [9/10], Step [156/250], Loss: 3.7174\n",
            "Epoch [9/10], Step [157/250], Loss: 4.0336\n",
            "Epoch [9/10], Step [158/250], Loss: 4.2544\n",
            "Epoch [9/10], Step [159/250], Loss: 4.2007\n",
            "Epoch [9/10], Step [160/250], Loss: 3.8280\n",
            "Epoch [9/10], Step [161/250], Loss: 3.7034\n",
            "Epoch [9/10], Step [162/250], Loss: 3.8199\n",
            "Epoch [9/10], Step [163/250], Loss: 4.6301\n",
            "Epoch [9/10], Step [164/250], Loss: 4.1792\n",
            "Epoch [9/10], Step [165/250], Loss: 4.3603\n",
            "Epoch [9/10], Step [166/250], Loss: 3.9987\n",
            "Epoch [9/10], Step [167/250], Loss: 3.7326\n",
            "Epoch [9/10], Step [168/250], Loss: 4.2826\n",
            "Epoch [9/10], Step [169/250], Loss: 3.7487\n",
            "Epoch [9/10], Step [170/250], Loss: 3.9367\n",
            "Epoch [9/10], Step [171/250], Loss: 3.8187\n",
            "Epoch [9/10], Step [172/250], Loss: 3.8998\n",
            "Epoch [9/10], Step [173/250], Loss: 2.6651\n",
            "Epoch [9/10], Step [174/250], Loss: 4.9756\n",
            "Epoch [9/10], Step [175/250], Loss: 4.0631\n",
            "Epoch [9/10], Step [176/250], Loss: 3.6598\n",
            "Epoch [9/10], Step [177/250], Loss: 4.2316\n",
            "Epoch [9/10], Step [178/250], Loss: 4.1251\n",
            "Epoch [9/10], Step [179/250], Loss: 3.9271\n",
            "Epoch [9/10], Step [180/250], Loss: 4.1130\n",
            "Epoch [9/10], Step [181/250], Loss: 3.7773\n",
            "Epoch [9/10], Step [182/250], Loss: 3.9628\n",
            "Epoch [9/10], Step [183/250], Loss: 4.7218\n",
            "Epoch [9/10], Step [184/250], Loss: 3.6591\n",
            "Epoch [9/10], Step [185/250], Loss: 3.7136\n",
            "Epoch [9/10], Step [186/250], Loss: 4.2387\n",
            "Epoch [9/10], Step [187/250], Loss: 3.5292\n",
            "Epoch [9/10], Step [188/250], Loss: 3.6845\n",
            "Epoch [9/10], Step [189/250], Loss: 3.7121\n",
            "Epoch [9/10], Step [190/250], Loss: 6.1542\n",
            "Epoch [9/10], Step [191/250], Loss: 3.7093\n",
            "Epoch [9/10], Step [192/250], Loss: 3.4713\n",
            "Epoch [9/10], Step [193/250], Loss: 3.6676\n",
            "Epoch [9/10], Step [194/250], Loss: 3.8678\n",
            "Epoch [9/10], Step [195/250], Loss: 3.7306\n",
            "Epoch [9/10], Step [196/250], Loss: 3.8392\n",
            "Epoch [9/10], Step [197/250], Loss: 3.7577\n",
            "Epoch [9/10], Step [198/250], Loss: 3.7332\n",
            "Epoch [9/10], Step [199/250], Loss: 3.9768\n",
            "Epoch [9/10], Step [200/250], Loss: 3.9306\n",
            "Epoch [9/10], Step [201/250], Loss: 3.7601\n",
            "Epoch [9/10], Step [202/250], Loss: 3.8794\n",
            "Epoch [9/10], Step [203/250], Loss: 10.6676\n",
            "Epoch [9/10], Step [204/250], Loss: 3.8956\n",
            "Epoch [9/10], Step [205/250], Loss: 3.8718\n",
            "Epoch [9/10], Step [206/250], Loss: 4.3833\n",
            "Epoch [9/10], Step [207/250], Loss: 3.7325\n",
            "Epoch [9/10], Step [208/250], Loss: 4.4259\n",
            "Epoch [9/10], Step [209/250], Loss: 3.6776\n",
            "Epoch [9/10], Step [210/250], Loss: 4.5680\n",
            "Epoch [9/10], Step [211/250], Loss: 4.0455\n",
            "Epoch [9/10], Step [212/250], Loss: 4.1707\n",
            "Epoch [9/10], Step [213/250], Loss: 4.3439\n",
            "Epoch [9/10], Step [214/250], Loss: 4.4254\n",
            "Epoch [9/10], Step [215/250], Loss: 3.7004\n",
            "Epoch [9/10], Step [216/250], Loss: 3.7689\n",
            "Epoch [9/10], Step [217/250], Loss: 4.1735\n",
            "Epoch [9/10], Step [218/250], Loss: 3.9723\n",
            "Epoch [9/10], Step [219/250], Loss: 3.7243\n",
            "Epoch [9/10], Step [220/250], Loss: 4.2918\n",
            "Epoch [9/10], Step [221/250], Loss: 4.4239\n",
            "Epoch [9/10], Step [222/250], Loss: 3.7361\n",
            "Epoch [9/10], Step [223/250], Loss: 4.4172\n",
            "Epoch [9/10], Step [224/250], Loss: 3.9281\n",
            "Epoch [9/10], Step [225/250], Loss: 5.4187\n",
            "Epoch [9/10], Step [226/250], Loss: 3.9847\n",
            "Epoch [9/10], Step [227/250], Loss: 4.6956\n",
            "Epoch [9/10], Step [228/250], Loss: 3.7550\n",
            "Epoch [9/10], Step [229/250], Loss: 4.1676\n",
            "Epoch [9/10], Step [230/250], Loss: 3.9690\n",
            "Epoch [9/10], Step [231/250], Loss: 3.9497\n",
            "Epoch [9/10], Step [232/250], Loss: 3.8903\n",
            "Epoch [9/10], Step [233/250], Loss: 4.1598\n",
            "Epoch [9/10], Step [234/250], Loss: 3.9424\n",
            "Epoch [9/10], Step [235/250], Loss: 3.9561\n",
            "Epoch [9/10], Step [236/250], Loss: 4.0250\n",
            "Epoch [9/10], Step [237/250], Loss: 4.0927\n",
            "Epoch [9/10], Step [238/250], Loss: 4.0219\n",
            "Epoch [9/10], Step [239/250], Loss: 4.3651\n",
            "Epoch [9/10], Step [240/250], Loss: 3.8678\n",
            "Epoch [9/10], Step [241/250], Loss: 3.9568\n",
            "Epoch [9/10], Step [242/250], Loss: 3.9848\n",
            "Epoch [9/10], Step [243/250], Loss: 3.9080\n",
            "Epoch [9/10], Step [244/250], Loss: 3.9874\n",
            "Epoch [9/10], Step [245/250], Loss: 3.5760\n",
            "Epoch [9/10], Step [246/250], Loss: 3.8816\n",
            "Epoch [9/10], Step [247/250], Loss: 4.0926\n",
            "Epoch [9/10], Step [248/250], Loss: 3.7802\n",
            "Epoch [9/10], Step [249/250], Loss: 3.9881\n",
            "Epoch [9/10], Step [250/250], Loss: 3.8863\n",
            "Epoch 9: Avg Loss = 4.0758 Current Loss = 3.8863\n",
            "Epoch [10/10], Step [1/250], Loss: 3.7110\n",
            "Epoch [10/10], Step [2/250], Loss: 4.0401\n",
            "Epoch [10/10], Step [3/250], Loss: 3.9299\n",
            "Epoch [10/10], Step [4/250], Loss: 4.0998\n",
            "Epoch [10/10], Step [5/250], Loss: 3.6595\n",
            "Epoch [10/10], Step [6/250], Loss: 3.6462\n",
            "Epoch [10/10], Step [7/250], Loss: 3.8046\n",
            "Epoch [10/10], Step [8/250], Loss: 3.4820\n",
            "Epoch [10/10], Step [9/250], Loss: 3.6204\n",
            "Epoch [10/10], Step [10/250], Loss: 3.9970\n",
            "Epoch [10/10], Step [11/250], Loss: 4.2464\n",
            "Epoch [10/10], Step [12/250], Loss: 4.1294\n",
            "Epoch [10/10], Step [13/250], Loss: 3.5834\n",
            "Epoch [10/10], Step [14/250], Loss: 4.7203\n",
            "Epoch [10/10], Step [15/250], Loss: 4.0764\n",
            "Epoch [10/10], Step [16/250], Loss: 4.0900\n",
            "Epoch [10/10], Step [17/250], Loss: 3.9920\n",
            "Epoch [10/10], Step [18/250], Loss: 4.1191\n",
            "Epoch [10/10], Step [19/250], Loss: 3.8498\n",
            "Epoch [10/10], Step [20/250], Loss: 3.7316\n",
            "Epoch [10/10], Step [21/250], Loss: 3.8925\n",
            "Epoch [10/10], Step [22/250], Loss: 3.8406\n",
            "Epoch [10/10], Step [23/250], Loss: 5.3644\n",
            "Epoch [10/10], Step [24/250], Loss: 4.2536\n",
            "Epoch [10/10], Step [25/250], Loss: 4.1073\n",
            "Epoch [10/10], Step [26/250], Loss: 4.6872\n",
            "Epoch [10/10], Step [27/250], Loss: 3.6176\n",
            "Epoch [10/10], Step [28/250], Loss: 3.9330\n",
            "Epoch [10/10], Step [29/250], Loss: 4.1608\n",
            "Epoch [10/10], Step [30/250], Loss: 3.3568\n",
            "Epoch [10/10], Step [31/250], Loss: 4.0059\n",
            "Epoch [10/10], Step [32/250], Loss: 3.7637\n",
            "Epoch [10/10], Step [33/250], Loss: 3.9031\n",
            "Epoch [10/10], Step [34/250], Loss: 3.9027\n",
            "Epoch [10/10], Step [35/250], Loss: 3.9362\n",
            "Epoch [10/10], Step [36/250], Loss: 3.9079\n",
            "Epoch [10/10], Step [37/250], Loss: 3.8789\n",
            "Epoch [10/10], Step [38/250], Loss: 6.3940\n",
            "Epoch [10/10], Step [39/250], Loss: 4.0197\n",
            "Epoch [10/10], Step [40/250], Loss: 3.7950\n",
            "Epoch [10/10], Step [41/250], Loss: 4.1561\n",
            "Epoch [10/10], Step [42/250], Loss: 3.5967\n",
            "Epoch [10/10], Step [43/250], Loss: 3.8460\n",
            "Epoch [10/10], Step [44/250], Loss: 3.8293\n",
            "Epoch [10/10], Step [45/250], Loss: 3.7715\n",
            "Epoch [10/10], Step [46/250], Loss: 3.7877\n",
            "Epoch [10/10], Step [47/250], Loss: 4.1983\n",
            "Epoch [10/10], Step [48/250], Loss: 3.8601\n",
            "Epoch [10/10], Step [49/250], Loss: 3.7634\n",
            "Epoch [10/10], Step [50/250], Loss: 3.8823\n",
            "Epoch [10/10], Step [51/250], Loss: 3.8612\n",
            "Epoch [10/10], Step [52/250], Loss: 3.6203\n",
            "Epoch [10/10], Step [53/250], Loss: 3.9865\n",
            "Epoch [10/10], Step [54/250], Loss: 3.8643\n",
            "Epoch [10/10], Step [55/250], Loss: 3.7852\n",
            "Epoch [10/10], Step [56/250], Loss: 4.2373\n",
            "Epoch [10/10], Step [57/250], Loss: 4.4227\n",
            "Epoch [10/10], Step [58/250], Loss: 3.6937\n",
            "Epoch [10/10], Step [59/250], Loss: 3.8491\n",
            "Epoch [10/10], Step [60/250], Loss: 4.1800\n",
            "Epoch [10/10], Step [61/250], Loss: 3.9096\n",
            "Epoch [10/10], Step [62/250], Loss: 4.3402\n",
            "Epoch [10/10], Step [63/250], Loss: 3.8743\n",
            "Epoch [10/10], Step [64/250], Loss: 3.9739\n",
            "Epoch [10/10], Step [65/250], Loss: 4.9316\n",
            "Epoch [10/10], Step [66/250], Loss: 3.8775\n",
            "Epoch [10/10], Step [67/250], Loss: 3.9701\n",
            "Epoch [10/10], Step [68/250], Loss: 3.8974\n",
            "Epoch [10/10], Step [69/250], Loss: 4.2381\n",
            "Epoch [10/10], Step [70/250], Loss: 5.1178\n",
            "Epoch [10/10], Step [71/250], Loss: 4.1725\n",
            "Epoch [10/10], Step [72/250], Loss: 5.1877\n",
            "Epoch [10/10], Step [73/250], Loss: 3.6395\n",
            "Epoch [10/10], Step [74/250], Loss: 3.8410\n",
            "Epoch [10/10], Step [75/250], Loss: 4.1156\n",
            "Epoch [10/10], Step [76/250], Loss: 4.2131\n",
            "Epoch [10/10], Step [77/250], Loss: 3.7703\n",
            "Epoch [10/10], Step [78/250], Loss: 3.7507\n",
            "Epoch [10/10], Step [79/250], Loss: 3.7872\n",
            "Epoch [10/10], Step [80/250], Loss: 3.7547\n",
            "Epoch [10/10], Step [81/250], Loss: 3.8238\n",
            "Epoch [10/10], Step [82/250], Loss: 3.8258\n",
            "Epoch [10/10], Step [83/250], Loss: 3.8325\n",
            "Epoch [10/10], Step [84/250], Loss: 3.9283\n",
            "Epoch [10/10], Step [85/250], Loss: 3.5868\n",
            "Epoch [10/10], Step [86/250], Loss: 4.2178\n",
            "Epoch [10/10], Step [87/250], Loss: 3.9825\n",
            "Epoch [10/10], Step [88/250], Loss: 3.7954\n",
            "Epoch [10/10], Step [89/250], Loss: 4.3925\n",
            "Epoch [10/10], Step [90/250], Loss: 4.1293\n",
            "Epoch [10/10], Step [91/250], Loss: 3.7961\n",
            "Epoch [10/10], Step [92/250], Loss: 4.2329\n",
            "Epoch [10/10], Step [93/250], Loss: 3.9778\n",
            "Epoch [10/10], Step [94/250], Loss: 4.1152\n",
            "Epoch [10/10], Step [95/250], Loss: 4.3934\n",
            "Epoch [10/10], Step [96/250], Loss: 4.2004\n",
            "Epoch [10/10], Step [97/250], Loss: 4.3637\n",
            "Epoch [10/10], Step [98/250], Loss: 4.1401\n",
            "Epoch [10/10], Step [99/250], Loss: 3.8358\n",
            "Epoch [10/10], Step [100/250], Loss: 4.2221\n",
            "Epoch [10/10], Step [101/250], Loss: 4.1165\n",
            "Epoch [10/10], Step [102/250], Loss: 4.2222\n",
            "Epoch [10/10], Step [103/250], Loss: 4.2003\n",
            "Epoch [10/10], Step [104/250], Loss: 4.1073\n",
            "Epoch [10/10], Step [105/250], Loss: 3.6803\n",
            "Epoch [10/10], Step [106/250], Loss: 4.1628\n",
            "Epoch [10/10], Step [107/250], Loss: 3.8320\n",
            "Epoch [10/10], Step [108/250], Loss: 3.8288\n",
            "Epoch [10/10], Step [109/250], Loss: 4.2333\n",
            "Epoch [10/10], Step [110/250], Loss: 3.9567\n",
            "Epoch [10/10], Step [111/250], Loss: 3.8693\n",
            "Epoch [10/10], Step [112/250], Loss: 4.5836\n",
            "Epoch [10/10], Step [113/250], Loss: 3.7506\n",
            "Epoch [10/10], Step [114/250], Loss: 3.6790\n",
            "Epoch [10/10], Step [115/250], Loss: 3.8863\n",
            "Epoch [10/10], Step [116/250], Loss: 4.8605\n",
            "Epoch [10/10], Step [117/250], Loss: 4.0494\n",
            "Epoch [10/10], Step [118/250], Loss: 3.9434\n",
            "Epoch [10/10], Step [119/250], Loss: 4.1383\n",
            "Epoch [10/10], Step [120/250], Loss: 3.8093\n",
            "Epoch [10/10], Step [121/250], Loss: 3.9238\n",
            "Epoch [10/10], Step [122/250], Loss: 3.9895\n",
            "Epoch [10/10], Step [123/250], Loss: 4.1679\n",
            "Epoch [10/10], Step [124/250], Loss: 4.2007\n",
            "Epoch [10/10], Step [125/250], Loss: 3.7218\n",
            "Epoch [10/10], Step [126/250], Loss: 3.5640\n",
            "Epoch [10/10], Step [127/250], Loss: 3.8589\n",
            "Epoch [10/10], Step [128/250], Loss: 3.9066\n",
            "Epoch [10/10], Step [129/250], Loss: 3.9404\n",
            "Epoch [10/10], Step [130/250], Loss: 4.0788\n",
            "Epoch [10/10], Step [131/250], Loss: 4.5012\n",
            "Epoch [10/10], Step [132/250], Loss: 4.3435\n",
            "Epoch [10/10], Step [133/250], Loss: 3.9732\n",
            "Epoch [10/10], Step [134/250], Loss: 3.9761\n",
            "Epoch [10/10], Step [135/250], Loss: 4.7042\n",
            "Epoch [10/10], Step [136/250], Loss: 3.7941\n",
            "Epoch [10/10], Step [137/250], Loss: 3.8534\n",
            "Epoch [10/10], Step [138/250], Loss: 3.7848\n",
            "Epoch [10/10], Step [139/250], Loss: 4.8949\n",
            "Epoch [10/10], Step [140/250], Loss: 3.8162\n",
            "Epoch [10/10], Step [141/250], Loss: 3.7360\n",
            "Epoch [10/10], Step [142/250], Loss: 3.8614\n",
            "Epoch [10/10], Step [143/250], Loss: 3.7606\n",
            "Epoch [10/10], Step [144/250], Loss: 3.8185\n",
            "Epoch [10/10], Step [145/250], Loss: 5.7907\n",
            "Epoch [10/10], Step [146/250], Loss: 3.8099\n",
            "Epoch [10/10], Step [147/250], Loss: 3.8706\n",
            "Epoch [10/10], Step [148/250], Loss: 5.4258\n",
            "Epoch [10/10], Step [149/250], Loss: 3.6225\n",
            "Epoch [10/10], Step [150/250], Loss: 4.0728\n",
            "Epoch [10/10], Step [151/250], Loss: 6.2964\n",
            "Epoch [10/10], Step [152/250], Loss: 3.7777\n",
            "Epoch [10/10], Step [153/250], Loss: 3.8648\n",
            "Epoch [10/10], Step [154/250], Loss: 2.0773\n",
            "Epoch [10/10], Step [155/250], Loss: 3.9141\n",
            "Epoch [10/10], Step [156/250], Loss: 4.3806\n",
            "Epoch [10/10], Step [157/250], Loss: 3.7704\n",
            "Epoch [10/10], Step [158/250], Loss: 3.9414\n",
            "Epoch [10/10], Step [159/250], Loss: 5.3643\n",
            "Epoch [10/10], Step [160/250], Loss: 3.8844\n",
            "Epoch [10/10], Step [161/250], Loss: 3.9124\n",
            "Epoch [10/10], Step [162/250], Loss: 3.8312\n",
            "Epoch [10/10], Step [163/250], Loss: 4.2475\n",
            "Epoch [10/10], Step [164/250], Loss: 4.5055\n",
            "Epoch [10/10], Step [165/250], Loss: 3.8439\n",
            "Epoch [10/10], Step [166/250], Loss: 3.8007\n",
            "Epoch [10/10], Step [167/250], Loss: 3.9904\n",
            "Epoch [10/10], Step [168/250], Loss: 3.8842\n",
            "Epoch [10/10], Step [169/250], Loss: 3.9041\n",
            "Epoch [10/10], Step [170/250], Loss: 3.5716\n",
            "Epoch [10/10], Step [171/250], Loss: 5.3045\n",
            "Epoch [10/10], Step [172/250], Loss: 3.9179\n",
            "Epoch [10/10], Step [173/250], Loss: 4.0039\n",
            "Epoch [10/10], Step [174/250], Loss: 3.8477\n",
            "Epoch [10/10], Step [175/250], Loss: 4.1533\n",
            "Epoch [10/10], Step [176/250], Loss: 3.9564\n",
            "Epoch [10/10], Step [177/250], Loss: 4.0242\n",
            "Epoch [10/10], Step [178/250], Loss: 3.9707\n",
            "Epoch [10/10], Step [179/250], Loss: 4.0966\n",
            "Epoch [10/10], Step [180/250], Loss: 3.9155\n",
            "Epoch [10/10], Step [181/250], Loss: 4.0778\n",
            "Epoch [10/10], Step [182/250], Loss: 4.1515\n",
            "Epoch [10/10], Step [183/250], Loss: 3.7633\n",
            "Epoch [10/10], Step [184/250], Loss: 3.6902\n",
            "Epoch [10/10], Step [185/250], Loss: 4.0057\n",
            "Epoch [10/10], Step [186/250], Loss: 4.2653\n",
            "Epoch [10/10], Step [187/250], Loss: 3.8993\n",
            "Epoch [10/10], Step [188/250], Loss: 3.6871\n",
            "Epoch [10/10], Step [189/250], Loss: 4.6348\n",
            "Epoch [10/10], Step [190/250], Loss: 4.3385\n",
            "Epoch [10/10], Step [191/250], Loss: 3.9525\n",
            "Epoch [10/10], Step [192/250], Loss: 3.9418\n",
            "Epoch [10/10], Step [193/250], Loss: 3.7812\n",
            "Epoch [10/10], Step [194/250], Loss: 4.3362\n",
            "Epoch [10/10], Step [195/250], Loss: 3.3047\n",
            "Epoch [10/10], Step [196/250], Loss: 3.8086\n",
            "Epoch [10/10], Step [197/250], Loss: 4.2985\n",
            "Epoch [10/10], Step [198/250], Loss: 2.4612\n",
            "Epoch [10/10], Step [199/250], Loss: 3.8879\n",
            "Epoch [10/10], Step [200/250], Loss: 3.8917\n",
            "Epoch [10/10], Step [201/250], Loss: 4.0135\n",
            "Epoch [10/10], Step [202/250], Loss: 3.8137\n",
            "Epoch [10/10], Step [203/250], Loss: 3.8239\n",
            "Epoch [10/10], Step [204/250], Loss: 3.9174\n",
            "Epoch [10/10], Step [205/250], Loss: 3.9771\n",
            "Epoch [10/10], Step [206/250], Loss: 3.9632\n",
            "Epoch [10/10], Step [207/250], Loss: 3.8040\n",
            "Epoch [10/10], Step [208/250], Loss: 4.1637\n",
            "Epoch [10/10], Step [209/250], Loss: 3.9162\n",
            "Epoch [10/10], Step [210/250], Loss: 4.1948\n",
            "Epoch [10/10], Step [211/250], Loss: 3.8945\n",
            "Epoch [10/10], Step [212/250], Loss: 6.8986\n",
            "Epoch [10/10], Step [213/250], Loss: 3.8301\n",
            "Epoch [10/10], Step [214/250], Loss: 4.0402\n",
            "Epoch [10/10], Step [215/250], Loss: 4.0056\n",
            "Epoch [10/10], Step [216/250], Loss: 3.8277\n",
            "Epoch [10/10], Step [217/250], Loss: 4.0858\n",
            "Epoch [10/10], Step [218/250], Loss: 3.9931\n",
            "Epoch [10/10], Step [219/250], Loss: 4.3156\n",
            "Epoch [10/10], Step [220/250], Loss: 3.7170\n",
            "Epoch [10/10], Step [221/250], Loss: 6.2157\n",
            "Epoch [10/10], Step [222/250], Loss: 3.7363\n",
            "Epoch [10/10], Step [223/250], Loss: 3.8854\n",
            "Epoch [10/10], Step [224/250], Loss: 4.3343\n",
            "Epoch [10/10], Step [225/250], Loss: 4.0254\n",
            "Epoch [10/10], Step [226/250], Loss: 4.2265\n",
            "Epoch [10/10], Step [227/250], Loss: 3.9488\n",
            "Epoch [10/10], Step [228/250], Loss: 3.9163\n",
            "Epoch [10/10], Step [229/250], Loss: 4.1859\n",
            "Epoch [10/10], Step [230/250], Loss: 4.0224\n",
            "Epoch [10/10], Step [231/250], Loss: 3.7283\n",
            "Epoch [10/10], Step [232/250], Loss: 5.0483\n",
            "Epoch [10/10], Step [233/250], Loss: 4.7175\n",
            "Epoch [10/10], Step [234/250], Loss: 3.8041\n",
            "Epoch [10/10], Step [235/250], Loss: 3.8859\n",
            "Epoch [10/10], Step [236/250], Loss: 3.7427\n",
            "Epoch [10/10], Step [237/250], Loss: 3.8524\n",
            "Epoch [10/10], Step [238/250], Loss: 3.6309\n",
            "Epoch [10/10], Step [239/250], Loss: 3.9340\n",
            "Epoch [10/10], Step [240/250], Loss: 3.8684\n",
            "Epoch [10/10], Step [241/250], Loss: 4.0363\n",
            "Epoch [10/10], Step [242/250], Loss: 3.4510\n",
            "Epoch [10/10], Step [243/250], Loss: 3.8896\n",
            "Epoch [10/10], Step [244/250], Loss: 3.9912\n",
            "Epoch [10/10], Step [245/250], Loss: 4.2019\n",
            "Epoch [10/10], Step [246/250], Loss: 3.5958\n",
            "Epoch [10/10], Step [247/250], Loss: 5.9043\n",
            "Epoch [10/10], Step [248/250], Loss: 3.9993\n",
            "Epoch [10/10], Step [249/250], Loss: 3.8559\n",
            "Epoch [10/10], Step [250/250], Loss: 3.8067\n",
            "Epoch 10: Avg Loss = 4.0482 Current Loss = 3.8067\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# 학습 루프 (10 Epoch)\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    i = 0\n",
        "    for mfcc, text, text_lengths in dataloader:\n",
        "        i += 1\n",
        "        mfcc = mfcc.to(device)\n",
        "        text = text.to(device)\n",
        "        text_lengths = text_lengths.to(device)\n",
        "\n",
        "        if torch.isnan(mfcc).any() or torch.isinf(mfcc).any():\n",
        "          print(\"NaN/Inf detected in input MFCC! Skipping batch.\")\n",
        "          continue\n",
        "        elif torch.isnan(text).any() or torch.isinf(text).any():\n",
        "          print(\"NaN/Inf detected in input text! Skipping batch.\")\n",
        "          continue\n",
        "        elif torch.isnan(text_lengths).any() or torch.isinf(text_lengths).any():\n",
        "          print(\"NaN/Inf detected in input text_lengths! Skipping batch.\")\n",
        "          continue\n",
        "\n",
        "        # 옵티마이저 Gradient 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 모델에 MFCC 입력하여 출력 얻기 (Forward)\n",
        "        outputs = model(mfcc) # (time, batch, feature)\n",
        "\n",
        "        # 모델 출력 NaN 체크\n",
        "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
        "            print(\"Warning: NaN/Inf detected in model outputs! Skipping batch.\")\n",
        "            continue\n",
        "\n",
        "        # 입력 길이 설정 (모델 출력 길이 기준)\n",
        "        #input_lengths = torch.tensor([mfcc_i.size(1) for mfcc_i in mfcc], dtype=torch.long)\n",
        "        #input_lengths = torch.tensor([mfcc.shape[1]] * mfcc.shape[0], dtype=torch.long).to(device)\n",
        "        input_lengths = torch.tensor([outputs.size(0)] * outputs.size(1), dtype=torch.long).to(device)\n",
        "\n",
        "        # 출력 시퀀스 설정\n",
        "        target_lengths = torch.tensor([len(t[t != -1]) for t in text], dtype=torch.long).to(device)\n",
        "        #target_lengths = text_lengths.to(device)\n",
        "\n",
        "        targets = text[text != -1].to(device) # 패딩(-1) 제거\n",
        "\n",
        "        #print(\"Text: \", text)\n",
        "        # NaN 방지 체크\n",
        "        if(input_lengths < target_lengths).any():\n",
        "          print(f\"Error: Some input_lengths ({input_lengths} are shorter than target_lengths ({target_lengths})!)\")\n",
        "          input_lengths = torch.maximum(input_lengths, target_lengths + 1)\n",
        "\n",
        "        # CTC Loss 계산 (입력: 모델 출력, 타겟: 정답 텍스트, 입력 길이, 출력 길이)\n",
        "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
        "\n",
        "        # CTC Loss NaN 체크\n",
        "        if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
        "            print(\"Warning: NaN/Inf detected in loss computation!\")\n",
        "            continue\n",
        "\n",
        "        # 역전파(Gradient 계산)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping 적용\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print Count\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{10}], Step [{i}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Epoch마다 Loss 출력\n",
        "    print(f\"Epoch {epoch+1}: Avg Loss = {total_loss / len(dataloader):.4f} Current Loss = {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPhj2Z_dwlKq"
      },
      "outputs": [],
      "source": [
        "# 전체 모델 저장\n",
        "torch.save(model, 'drive/MyDrive/ai-dataset/speech_model_full4.pth')\n",
        "torch.save(model.state_dict(), 'drive/MyDrive/ai-dataset/speech_model4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djeBGeO_xiVh"
      },
      "outputs": [],
      "source": [
        "# 학습 진행 상태 저장\n",
        "checkpoint = {\n",
        "    'epoch': epoch,  # 마지막 학습한 에포크\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item()\n",
        "}\n",
        "torch.save(checkpoint, '/content/drive/MyDrive/ai-dataset/speech_checkpoint4.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYXTjCl6xHIC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# 학습 진행 상태 불러오기\n",
        "checkpoint = torch.load('/content/drive/MyDrive/speech_checkpoint.pth')\n",
        "\n",
        "# 모델, 옵티마이저 로드\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# 마지막 에포크 번호 확인\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHwKdicHftCP"
      },
      "source": [
        "#### **2.4 평가**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t75NQDP6_SXA"
      },
      "outputs": [],
      "source": [
        "import Levenshtein\n",
        "\n",
        "def evaluate_model(model, audio_file, text_file, tokenizer, sr=16000):\n",
        "    \"\"\"\n",
        "    학습된 모델을 활용하여 새로운 음성 파일과 텍스트 파일을 비교하여 정확도를 평가하는 함수.\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 음성 인식 모델\n",
        "        audio_file (str): 평가할 음성 파일 경로\n",
        "        text_file (str): 해당 음성의 정답 텍스트 파일 경로\n",
        "        tokenizer: 문자 기반 토크나이저\n",
        "        sr (int): 샘플링 레이트 (기본값 16kHz)\n",
        "\n",
        "    Returns:\n",
        "        dict: {\"predicted_text\": str, \"wer\": float, \"cer\": float}\n",
        "    \"\"\"\n",
        "\n",
        "    # 음성 파일 로드 및 특징 추출\n",
        "    try:\n",
        "        audio = load_pcm(audio_file, sr)  # PCM 파일 로드\n",
        "        mfcc, _, _, _ = extract_features(audio, sr)  # MFCC 추출\n",
        "        mfcc = mfcc.unsqueeze(0).to(next(model.parameters()).device)  # 배치 차원 추가 및 GPU로 이동\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio file {audio_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 모델 추론\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(mfcc)  # (T, N, C) 형태의 logits 출력\n",
        "        output = output.permute(1, 0, 2)  # (N, T, C) 형태로 변환\n",
        "\n",
        "    # CTC 디코딩 (가장 높은 확률의 인덱스를 선택)\n",
        "    predicted_indices = torch.argmax(output, dim=2)  # (N, T) 형태\n",
        "    predicted_tokens = [token.item() for token in predicted_indices[0]]\n",
        "\n",
        "    # 중복 제거 (CTC 특성상 같은 문자가 연속적으로 나타나는 경우가 많음)\n",
        "    def ctc_decode(tokens):\n",
        "        decoded = []\n",
        "        prev_token = None\n",
        "        for token in tokens:\n",
        "            if token != prev_token and token != 0:  # BLANK 토큰(0) 무시\n",
        "                decoded.append(token)\n",
        "            prev_token = token\n",
        "        return decoded\n",
        "\n",
        "    predicted_tokens = ctc_decode(predicted_tokens)\n",
        "    predicted_text = tokenizer.decode(predicted_tokens)  # 토큰을 문자로 변환\n",
        "\n",
        "    # 정답 텍스트 로드 및 토큰 변환\n",
        "    with open(text_file, \"r\", encoding=\"cp949\") as f:\n",
        "        ground_truth_text = f.read().strip()\n",
        "        ground_truth_text = clean_transcript(ground_truth_text)\n",
        "\n",
        "    # WER (Word Error Rate) 및 CER (Character Error Rate) 계산\n",
        "    def calculate_wer(ref, hyp):\n",
        "        ref_words = ref.split()\n",
        "        hyp_words = hyp.split()\n",
        "        return Levenshtein.distance(ref_words, hyp_words) / max(len(ref_words), 1)\n",
        "\n",
        "    def calculate_cer(ref, hyp):\n",
        "        return Levenshtein.distance(ref, hyp) / max(len(ref), 1)\n",
        "\n",
        "    wer = calculate_wer(ground_truth_text, predicted_text)\n",
        "    cer = calculate_cer(ground_truth_text, predicted_text)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(\"\\n📌 Evaluation Result\")\n",
        "    print(f\"🔹 Reference Text  : {ground_truth_text}\")\n",
        "    print(f\"🔹 Predicted Text  : {predicted_text}\")\n",
        "    print(f\"✅ WER: {wer:.4f}, CER: {cer:.4f}\")\n",
        "\n",
        "    return {\"predicted_text\": predicted_text, \"wer\": wer, \"cer\": cer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELiu3cso9Be7",
        "outputId": "c63bb293-504f-4603-f8ed-c5e307dabd54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📌 Evaluation Result\n",
            "🔹 Reference Text  : 아 몬 소리야 그건 또 \n",
            "🔹 Predicted Text  :            !   !! ! ! ! D! ! !  !갑!  갑 갑   갑 갑 갑 !  \n",
            "✅ WER: 3.0000, CER: 3.6154\n",
            "{'predicted_text': '           !   !! ! ! ! D! ! !  !갑!  갑 갑   갑 갑 갑 !  ', 'wer': 3.0, 'cer': 3.6153846153846154}\n"
          ]
        }
      ],
      "source": [
        "#실제 평가\n",
        "\n",
        "result = evaluate_model(model, pcm_files[0], txt_files[0], dataset.tokenizer)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3ZW9xfv_47-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "total_wer = 0\n",
        "total_cer = 0\n",
        "num_samples = 0\n",
        "\n",
        "for pcm_file, txt_file in zip(eval_pcm_files, eval_txt_files):\n",
        "    wer, cer = evaluate_single(model, pcm_file, txt_file, device, dataset.tokenizer) # dataset.tokenizer 추가\n",
        "    total_wer += wer\n",
        "    total_cer += cer\n",
        "    num_samples += 1\n",
        "\n",
        "avg_wer = total_wer / num_samples\n",
        "avg_cer = total_cer / num_samples\n",
        "print(f\"WER: {avg_wer:.4f}, CER: {avg_cer:.4f}\")\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
